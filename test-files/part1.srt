1
00:00:00,000 --> 00:00:01,320
大家好，我是安德魯·梅恩。

1
00:00:00,000 --> 00:00:01,320
Hello, I'm Andrew Mayne

2
00:00:01,460 --> 00:00:03,280
歡迎收聽《開眼》播客。

2
00:00:01,460 --> 00:00:03,280
and this is the Opening Eye podcast.

3
00:00:03,460 --> 00:00:06,620
今天我們要來聊聊 ChatGPT 與教育。

3
00:00:03,460 --> 00:00:06,620
Today, we're going to talk about chat GPT and education.

4
00:00:07,620 --> 00:00:08,900
它會讓人變笨嗎？

4
00:00:07,620 --> 00:00:08,900
Does it cause brain rot?

5
00:00:09,160 --> 00:00:11,080
ChatGPT 只是拿來作弊的工具嗎？

5
00:00:09,160 --> 00:00:11,080
Is chat GPT just a tool for cheating?

6
00:00:11,520 --> 00:00:13,040
我們將邀請莉亞·貝爾斯基來聊聊，

6
00:00:11,520 --> 00:00:13,040
We're going to speak with Leah Belsky,

7
00:00:13,160 --> 00:00:14,760
她是《開眼》的教育主管，

7
00:00:13,160 --> 00:00:14,760
head of education at Opening Eye

8
00:00:14,860 --> 00:00:16,820
還有幾位經常使用的學生。

8
00:00:14,860 --> 00:00:16,820
and a couple of students who are active users.

9
00:00:17,400 --> 00:00:20,740
現在，ChatGPT 已經是全球最大的學習平台。

9
00:00:17,400 --> 00:00:20,740
Chat GPT at this point is now the world's largest learning platform.

10
00:00:20,880 --> 00:00:22,700
它能幫你排除雜訊，

10
00:00:20,880 --> 00:00:22,700
It allows you to kind of cut through the noise

11
00:00:22,700 --> 00:00:24,200
讓你專注做自己真正喜歡的事。

11
00:00:22,700 --> 00:00:24,200
and do things that you actually enjoy.

12
00:00:24,500 --> 00:00:27,320
ChatGPT 將為這位女孩打開世界的大門，

12
00:00:24,500 --> 00:00:27,320
Chat GPT is going to unlock the world for this girl

13
00:00:27,420 --> 00:00:29,700
我也不用再像以前那樣擔心了。

13
00:00:27,420 --> 00:00:29,700
and I was not going to have to be worried in the same way.

14
00:00:30,000 --> 00:00:32,060
說到學習和探索新想法，

14
00:00:30,000 --> 00:00:32,060
When it comes to learning and exploring ideas

15
00:00:32,300 --> 00:00:34,040
我常常會問 ChatGPT 這些問題。

15
00:00:32,300 --> 00:00:34,040
I ask chat GPT a lot of those questions.

16
00:00:35,820 --> 00:00:38,500
跟我聊聊你加入 OpenAI 的經歷吧。

16
00:00:35,820 --> 00:00:38,500
Tell me about your journey to Opening Eye.

17
00:00:39,040 --> 00:00:39,260
你知道嗎，

17
00:00:39,040 --> 00:00:39,260
You know,

18
00:00:39,320 --> 00:00:44,300
我認為 OpenAI 最終關乎使命，也關乎這裡的人。

18
00:00:39,320 --> 00:00:44,300
I think ultimately Open AI is about a mission and about its people.

19
00:00:44,560 --> 00:00:46,360
所以我至少可以跟你分享我的故事。

19
00:00:44,560 --> 00:00:46,360
And so I'll tell you at least my story.

20
00:00:46,720 --> 00:00:51,500
我在教育領域耕耘了十五年後，來到了 OpenAI，

20
00:00:46,720 --> 00:00:51,500
So I came to Open AI after spending 15 years in the education space,

21
00:00:51,560 --> 00:00:53,540
一開始在世界銀行，後來加入 Coursera，

21
00:00:51,560 --> 00:00:53,540
starting at the World Bank and then at Coursera

22
00:00:53,860 --> 00:00:57,640
一直專注於讓全球都能獲得教育的使命。

22
00:00:53,860 --> 00:00:57,640
focused on this mission of making education accessible to the world.

23
00:00:57,640 --> 00:01:01,660
當我接下這份工作時，我們的營運長 Brad，

23
00:00:57,640 --> 00:01:01,660
And when I and I took on this job, Brad, our COO

24
00:01:01,800 --> 00:01:04,140
他把我帶進辦公室，你知道的，

24
00:01:01,800 --> 00:01:04,140
he brought he brought me into the office and, you know

25
00:01:04,160 --> 00:01:05,540
我當時在想，究竟

25
00:01:04,160 --> 00:01:05,540
I was wondering what what the exact

26
00:01:05,540 --> 00:01:07,580
OpenAI 在教育領域的重點會是什麼。

26
00:01:05,540 --> 00:01:07,580
focus of education at Open AI would be.

27
00:01:07,780 --> 00:01:10,220
然後他請我坐下，對我說，Leah，你知道嗎，

27
00:01:07,780 --> 00:01:10,220
And and he sat me down and he said, Leah, you know

28
00:01:10,220 --> 00:01:11,920
我希望你去追尋那個大膽的目標。

28
00:01:10,220 --> 00:01:11,920
I want you to go after the moonshot.

29
00:01:12,320 --> 00:01:13,400
我們每個人都有這個夢想，

29
00:01:12,320 --> 00:01:13,400
We all have this dream

30
00:01:13,400 --> 00:01:16,440
那就是我們能提升人類的潛能。

30
00:01:13,400 --> 00:01:16,440
that that I could improve human potential

31
00:01:16,620 --> 00:01:18,120
它能成為一位有效的家教

31
00:01:16,620 --> 00:01:18,120
that it could be an effective tutor

32
00:01:18,120 --> 00:01:20,420
並且能陪伴人們一生。

32
00:01:18,120 --> 00:01:20,420
and a companion for people throughout their lives.

33
00:01:21,100 --> 00:01:23,580
追逐那個夢想，並確保

33
00:01:21,100 --> 00:01:23,580
Go after that dream and make sure

34
00:01:23,580 --> 00:01:25,340
當我們打造出那個產品時

34
00:01:23,580 --> 00:01:25,340
that once we build that product

35
00:01:25,460 --> 00:01:26,660
全世界每個人都能擁有它。

35
00:01:25,460 --> 00:01:26,660
everyone in the world can have it.

36
00:01:26,660 --> 00:01:29,060
這一直是我們的指引方向。

36
00:01:26,660 --> 00:01:29,060
And that's really been the North Star.

37
00:01:29,380 --> 00:01:30,800
我覺得這很有意義，你知道嗎，

37
00:01:29,380 --> 00:01:30,800
And I thought that was meaningful, you know,

38
00:01:30,840 --> 00:01:32,160
從市場推廣負責人的角度來看，

38
00:01:30,840 --> 00:01:32,160
from the head of go to market,

39
00:01:32,300 --> 00:01:33,980
甚至連營收主管也說，

39
00:01:32,300 --> 00:01:33,980
the head of revenue to say,

40
00:01:34,100 --> 00:01:37,020
追求最具變革性的遠大目標，

40
00:01:34,100 --> 00:01:37,020
go after the biggest transformative moonshot so

41
00:01:37,020 --> 00:01:38,180
讓最高的期望

41
00:01:37,020 --> 00:01:38,180
that the highest and hopes

42
00:01:38,180 --> 00:01:40,980
以及我們對 AI 和教育的夢想都能實現。

42
00:01:38,180 --> 00:01:40,980
and dreams that we all have for AI and education can be realized.

43
00:01:42,140 --> 00:01:44,380
這真的很鼓舞人心，也充滿希望，

43
00:01:42,140 --> 00:01:44,380
That's pretty inspiring and hopeful

44
00:01:44,420 --> 00:01:46,540
希望能讓這項技術盡可能普及的想法。

44
00:01:44,420 --> 00:01:46,540
the idea of trying to make this widespread as possible.

45
00:01:47,460 --> 00:01:52,160
你怎麼看待這對全球的影響呢？

45
00:01:47,460 --> 00:01:52,160
How do you see basically, you know, global impact?

46
00:01:52,300 --> 00:01:53,740
我們要怎麼做，你知道的，

46
00:01:52,300 --> 00:01:53,740
How are we going to, you know

47
00:01:54,060 --> 00:01:55,760
才能看到這件事在全世界產生的影響？

47
00:01:54,060 --> 00:01:55,760
see the effects of this around the world?

48
00:01:56,660 --> 00:01:57,060
嗯，

48
00:01:56,660 --> 00:01:57,060
Well,

49
00:01:57,140 --> 00:01:59,340
首先要說的是，像ChatGPT這樣的平台現在

49
00:01:57,140 --> 00:01:59,340
first thing to say is like Chachapiti at

50
00:01:59,340 --> 00:02:01,700
已經成為全球最大的學習平台。

50
00:01:59,340 --> 00:02:01,700
this point is now the world's largest learning platform.

51
00:02:02,240 --> 00:02:04,060
學習是最主要的用途之一，

51
00:02:02,240 --> 00:02:04,060
Learning is one of the top use

52
00:02:04,060 --> 00:02:06,640
目前平台上有六億名用戶。

52
00:02:04,060 --> 00:02:06,640
cases on the platform at 600 million users.

53
00:02:06,880 --> 00:02:09,440
這代表它已經成為全球學習的首選平台。

53
00:02:06,880 --> 00:02:09,440
That means it is the world's learning destination.

54
00:02:10,200 --> 00:02:12,760
而且這也代表著在正規教育體系之外的學習。

54
00:02:10,200 --> 00:02:12,760
And that means learning outside of the educational system.

55
00:02:13,000 --> 00:02:15,260
我真的認為ChatGPT是學習的新前沿。

55
00:02:13,000 --> 00:02:15,260
I really see Chachapiti as a new frontier for learning.

56
00:02:15,940 --> 00:02:17,880
但我們也發現老師是主要的

56
00:02:15,940 --> 00:02:17,880
But we also see that teachers are major

57
00:02:17,880 --> 00:02:20,420
平台使用者之一，

57
00:02:17,880 --> 00:02:20,420
adopters of the of the of the platform

58
00:02:20,420 --> 00:02:22,740
他們不僅用它來減輕

58
00:02:20,420 --> 00:02:22,740
and they are using that both to get

59
00:02:22,740 --> 00:02:24,620
自己工作的行政負擔，

59
00:02:22,740 --> 00:02:24,620
rid of the administrative burden of their own work

60
00:02:24,680 --> 00:02:27,020
也把它帶進了教室。

60
00:02:24,680 --> 00:02:27,020
but also to to bring it to their classroom.

61
00:02:27,700 --> 00:02:31,100
令人驚訝的是，全球對ChatGPT的需求如此之高。

61
00:02:27,700 --> 00:02:31,100
What's been striking is to see the global demand for Chachapiti.

62
00:02:31,660 --> 00:02:32,500
所以幾個星期前，

62
00:02:31,660 --> 00:02:32,500
So a few weeks ago

63
00:02:32,500 --> 00:02:35,340
我們推出了一個名為「OpenAI for Countries」的計畫，

63
00:02:32,500 --> 00:02:35,340
we launched a program called Open AI for Countries

64
00:02:35,640 --> 00:02:38,160
世界各地的政府部門都主動聯繫我們。

64
00:02:35,640 --> 00:02:38,160
and we've had ministries all over the world reach out.

65
00:02:38,560 --> 00:02:40,040
愛沙尼亞，這個國家，

65
00:02:38,560 --> 00:02:40,040
Estonia, the country of Estonia

66
00:02:40,060 --> 00:02:42,300
其實是最早聯絡我們的國家之一。

66
00:02:40,060 --> 00:02:42,300
was actually one of the first countries that reached out.

67
00:02:42,420 --> 00:02:42,900
愛沙尼亞會這麼做很合理。

67
00:02:42,420 --> 00:02:42,900
Estonia makes sense.

68
00:02:43,000 --> 00:02:43,940
愛沙尼亞會這麼做很合理。

68
00:02:43,000 --> 00:02:43,940
Estonia makes sense.

69
00:02:44,060 --> 00:02:45,300
他們在世界上名列前茅。

69
00:02:44,060 --> 00:02:45,300
They are that they are the top of the world.

70
00:02:45,360 --> 00:02:46,800
他們的PISA成績是全球頂尖之一。

70
00:02:45,360 --> 00:02:46,800
They have some of the best PISA scores.

71
00:02:46,920 --> 00:02:48,620
他們擁有非常優秀的教育體系。

71
00:02:46,920 --> 00:02:48,620
They have an amazing educational system.

72
00:02:49,020 --> 00:02:51,680
但他們是第一個意識到：「哇，

72
00:02:49,020 --> 00:02:51,680
But they were the first to realize, wow

73
00:02:51,680 --> 00:02:54,720
這是讓學生更上一層樓的機會，

73
00:02:51,680 --> 00:02:54,720
this is an opportunity to push students even further

74
00:02:54,720 --> 00:02:56,800
也能進一步賦能我們的教師。」

74
00:02:54,720 --> 00:02:56,800
and to empower our teachers even further.

75
00:02:57,380 --> 00:03:00,240
在愛沙尼亞之後，陸續有越來越多國家加入。

75
00:02:57,380 --> 00:03:00,240
And after Estonia, it's been one country after another.

76
00:03:00,640 --> 00:03:01,600
有趣的是，

76
00:03:00,640 --> 00:03:01,600
And what's interesting is

77
00:03:01,600 --> 00:03:03,320
當各國來找我們的時候，

77
00:03:01,600 --> 00:03:03,320
that when the countries come to us,

78
00:03:04,120 --> 00:03:05,380
你知道，他們會來找我們，

78
00:03:04,120 --> 00:03:05,380
you know, they're coming to us

79
00:03:05,380 --> 00:03:07,300
是因為他們想要在教育體系中導入人工智慧，

79
00:03:05,380 --> 00:03:07,300
because they want to deploy AI

80
00:03:07,300 --> 00:03:09,660
把它作為教育系統的核心基礎建設。

80
00:03:07,300 --> 00:03:09,660
as core infrastructure through their education system.

81
00:03:10,180 --> 00:03:11,480
但他們同時也來找我們，

81
00:03:10,180 --> 00:03:11,480
But they're also coming to us

82
00:03:11,480 --> 00:03:14,560
因為他們正感受到經濟轉型帶來的衝擊。

82
00:03:11,480 --> 00:03:14,560
because they're feeling like the impact of an economic transition.

83
00:03:14,860 --> 00:03:16,900
他們也意識到，如果他們要

83
00:03:14,860 --> 00:03:16,900
And they're realizing that if they are

84
00:03:16,900 --> 00:03:19,640
發展一個以人工智慧為核心的經濟，

84
00:03:16,900 --> 00:03:19,640
going to be developing an AI powered economy

85
00:03:19,800 --> 00:03:21,440
他們就必須培養出

85
00:03:19,800 --> 00:03:21,440
they need to they need to graduate

86
00:03:21,440 --> 00:03:24,180
懂得如何運用人工智慧的勞動力。

86
00:03:21,440 --> 00:03:24,180
a workforce that learns how to use AI.

87
00:03:24,540 --> 00:03:27,020
所以這不只是開設新的人工智慧課程，

87
00:03:24,540 --> 00:03:27,020
And so it's not just about creating new AI courses.

88
00:03:27,400 --> 00:03:28,660
他們還必須確保

88
00:03:27,400 --> 00:03:28,660
They need to make sure

89
00:03:28,660 --> 00:03:31,100
每一位從中學畢業的學生

89
00:03:28,660 --> 00:03:31,100
that every student who leaves their secondary school

90
00:03:31,100 --> 00:03:33,740
都曾在課堂上實際使用過人工智慧。

90
00:03:31,100 --> 00:03:33,740
system has used AI as part of their class.

91
00:03:34,420 --> 00:03:37,300
因此，這些國家同時聚焦於兩個重點。

91
00:03:34,420 --> 00:03:37,300
And so it's that it's that dual focus of these countries.

92
00:03:37,300 --> 00:03:39,660
他們希望在經濟上保持競爭力。

92
00:03:37,300 --> 00:03:39,660
They're looking to be economically competitive.

93
00:03:39,660 --> 00:03:41,660
他們想培養具備 AI 能力的勞動力。

93
00:03:39,660 --> 00:03:41,660
They want to build an AI ready workforce.

94
00:03:41,940 --> 00:03:45,140
他們希望能在新興的 AI 經濟中取得成功。

94
00:03:41,940 --> 00:03:45,140
They want to be successful in a new AI powered economy.

95
00:03:45,360 --> 00:03:47,440
同時也希望提升教育體系。

95
00:03:45,360 --> 00:03:47,440
And they want to improve their education system.

96
00:03:47,920 --> 00:03:51,140
你從教育夥伴和機構那邊聽到什麼反饋？

96
00:03:47,920 --> 00:03:51,140
What are you hearing from educational partners and institutions?

97
00:03:52,040 --> 00:03:53,260
我們收到幾個方面的回應。

97
00:03:52,040 --> 00:03:53,260
So we're hearing a couple of things.

98
00:03:53,560 --> 00:03:55,580
我認為那些已經投入資源的人

98
00:03:53,560 --> 00:03:55,580
I think those that have made the investment

99
00:03:56,240 --> 00:03:58,060
對於他們所取得的成就感到自豪，

99
00:03:56,240 --> 00:03:58,060
feel a certain pride that they have

100
00:03:58,060 --> 00:04:00,600
像是在校園內實現了 AI 的平等使用。

100
00:03:58,060 --> 00:04:00,600
equalized access to AI on their campuses.

101
00:04:00,980 --> 00:04:03,120
他們真的相信 AI

101
00:04:00,980 --> 00:04:03,120
They are they really believe that AI

102
00:04:03,120 --> 00:04:05,680
應該成為校園的核心基礎設施，

102
00:04:03,120 --> 00:04:05,680
should be core infrastructure for the campus

103
00:04:05,680 --> 00:04:07,560
並且應該對所有人開放。

103
00:04:05,680 --> 00:04:07,560
and that it should be open to everyone.

104
00:04:07,800 --> 00:04:09,160
他們也意識到，

104
00:04:07,800 --> 00:04:09,160
They're conscious that, you know

105
00:04:09,180 --> 00:04:11,200
在許多校園導入 AI 之前，

105
00:04:09,180 --> 00:04:11,200
before introducing AI in many campus

106
00:04:11,340 --> 00:04:12,840
你會遇到這樣的情況

106
00:04:11,340 --> 00:04:12,840
you'll find a situation

107
00:04:12,840 --> 00:04:14,620
那些沒有申請經濟補助的人

107
00:04:12,840 --> 00:04:14,620
where those who are not on financial aid

108
00:04:14,620 --> 00:04:16,760
會自費購買最新的模型使用權限。

108
00:04:14,620 --> 00:04:16,760
will be buying access to the latest models.

109
00:04:16,800 --> 00:04:19,940
而缺乏資源的人則無法這麼做。

109
00:04:16,800 --> 00:04:19,940
And those who don't have access to resources will not.

110
00:04:19,940 --> 00:04:23,880
因此，推動平等取得資源是一件值得自豪的事。

110
00:04:19,940 --> 00:04:23,880
So there's a there's a pride in and driving equal access.

111
00:04:24,160 --> 00:04:28,440
我認為這些機構也非常渴望彼此交流合作

111
00:04:24,160 --> 00:04:28,440
I think their institutions are also hungry to engage with each other

112
00:04:28,440 --> 00:04:30,560
並且合作了解目前最受歡迎的五種

112
00:04:28,440 --> 00:04:30,560
and collaborate and understand what are the top five

113
00:04:30,560 --> 00:04:32,900
或十種教師運用這項技術的方式

113
00:04:30,560 --> 00:04:32,900
or ten ways in which faculty are using the tech

114
00:04:32,900 --> 00:04:34,360
並將其應用在教學現場。

114
00:04:32,900 --> 00:04:34,360
and bringing it into the classroom.

115
00:04:35,120 --> 00:04:37,800
不過，我們也收到另一種回饋，指出

115
00:04:35,120 --> 00:04:37,800
The other type of feedback that we're getting, though, is that

116
00:04:38,460 --> 00:04:42,980
學生對於使用學校提供的某些AI工具感到猶豫。

116
00:04:38,460 --> 00:04:42,980
students are hesitant to use some of the school provided AI.

117
00:04:43,320 --> 00:04:47,660
除非大學特別強調告訴學生們

117
00:04:43,320 --> 00:04:47,660
And unless universities make a big point of telling students

118
00:04:47,660 --> 00:04:49,520
我們並不會監控這些工具的使用情形

118
00:04:47,660 --> 00:04:49,520
that we're not monitoring this tech

119
00:04:49,600 --> 00:04:51,180
我們也不會查看你們的對話內容。

119
00:04:49,600 --> 00:04:51,180
we're not looking at your conversations.

120
00:04:52,020 --> 00:04:54,480
否則學生會對使用這些工具感到遲疑。

120
00:04:52,020 --> 00:04:54,480
Students are hesitant to adopt it.

121
00:04:54,940 --> 00:04:55,040
你知道嗎，

121
00:04:54,940 --> 00:04:55,040
You know,

122
00:04:55,080 --> 00:04:58,360
令人注意的是，現在的大學生這一代

122
00:04:55,080 --> 00:04:58,360
what's striking is that the generation of students that is

123
00:04:58,360 --> 00:05:00,920
其實就是所謂的疫情世代，

123
00:04:58,360 --> 00:05:00,920
in universities right now is the covid generation,

124
00:05:01,180 --> 00:05:01,320
對不對？

124
00:05:01,180 --> 00:05:01,320
right?

125
00:05:01,440 --> 00:05:04,420
他們習慣於第一次接觸科技時，

125
00:05:01,440 --> 00:05:04,420
They are used to having some of their first experiences

126
00:05:04,420 --> 00:05:07,900
就是透過 Zoom、全球教室這些方式，

126
00:05:04,420 --> 00:05:07,900
with tech be about like Zoom and global classroom and having

127
00:05:08,560 --> 00:05:10,720
老師不是在大聲斥責，就是在監控他們，

127
00:05:08,560 --> 00:05:10,720
teachers screaming at them or monitoring

128
00:05:10,720 --> 00:05:12,360
還會說他們沒寫作業，

128
00:05:10,720 --> 00:05:12,360
and telling them they're not doing that homework

129
00:05:12,360 --> 00:05:14,700
而他們卻只能在家裡快要抓狂。

129
00:05:12,360 --> 00:05:14,700
while they're all sitting at home going crazy.

130
00:05:14,700 --> 00:05:18,600
所以他們對教育科技會感到猶豫。

130
00:05:14,700 --> 00:05:18,600
And so they're hesitant about educational technology.

131
00:05:18,800 --> 00:05:20,640
而大學也開始意識到，如果

131
00:05:18,800 --> 00:05:20,640
And universities are realizing that if

132
00:05:20,640 --> 00:05:22,540
他們想要和學生有更多互動，

132
00:05:20,640 --> 00:05:22,540
they want to engage with students

133
00:05:22,540 --> 00:05:25,260
並且用 ChatGPT 幫助學生用新方式學習，

133
00:05:22,540 --> 00:05:25,260
and help students learn in new ways with chat GPT

134
00:05:25,320 --> 00:05:27,580
就必須真正建立起和學生的信任。

134
00:05:25,320 --> 00:05:27,580
they need to build an actual trust with students.

135
00:05:27,940 --> 00:05:30,560
對啊，我只是覺得很挫折，

135
00:05:27,940 --> 00:05:30,560
Yeah, I just like I was frustrated

136
00:05:30,560 --> 00:05:32,400
因為那些 AI 偵測器真的很不準。

136
00:05:30,560 --> 00:05:32,400
because like the AI detectors were terrible.

137
00:05:32,620 --> 00:05:34,140
就像有些 AI 偵測工具一樣。

137
00:05:32,620 --> 00:05:34,140
Like there are these AI detect like one.

138
00:05:34,640 --> 00:05:38,780
我可以教別人怎麼寫出會被判定為 AI 生成的文字，

138
00:05:34,640 --> 00:05:38,780
I could show somebody how to write text that would be flagged as AI

139
00:05:38,780 --> 00:05:40,980
也可以教他們怎麼下提示詞來避開偵測。

139
00:05:38,780 --> 00:05:40,980
and also how to prompt a thing to avoid it.

140
00:05:40,980 --> 00:05:43,620
這就造成了這種，你知道的，

140
00:05:40,980 --> 00:05:43,620
And that just created this, you know

141
00:05:43,640 --> 00:05:44,880
一開始就產生的關係。

141
00:05:43,640 --> 00:05:44,880
relationship that started off.

142
00:05:44,980 --> 00:05:47,060
如果你用這種爛工具，你懂的，

142
00:05:44,980 --> 00:05:47,060
If you're using a bad tool like that, you know

143
00:05:47,140 --> 00:05:50,560
你可能會把一兩個沒作弊的學生誤判成作弊。

143
00:05:47,140 --> 00:05:50,560
you take one or two students who didn't cheat and told their cheaters.

144
00:05:50,840 --> 00:05:51,300
就是這樣。

144
00:05:50,840 --> 00:05:51,300
That's just.

145
00:05:51,740 --> 00:05:51,840
對啊。

145
00:05:51,740 --> 00:05:51,840
Yeah.

146
00:05:52,200 --> 00:05:53,500
而且在很多學校裡，

146
00:05:52,200 --> 00:05:53,500
And in many institutions

147
00:05:53,500 --> 00:05:55,640
我覺得我們一開始就走錯方向了。

147
00:05:53,500 --> 00:05:55,640
I would say we got off onto the wrong foot.

148
00:05:55,760 --> 00:05:55,900
沒錯。

148
00:05:55,760 --> 00:05:55,900
Yeah.

149
00:05:55,900 --> 00:05:59,740
在教室裡使用 AI，與其試著明確說清楚，

149
00:05:55,900 --> 00:05:59,740
With AI in the classroom, instead of trying to be explicit

150
00:05:59,740 --> 00:06:01,780
並訂立明確的規範，

150
00:05:59,740 --> 00:06:01,780
and establish clear policies on

151
00:06:01,780 --> 00:06:04,120
學生什麼時候該用科技，什麼時候不該用

151
00:06:01,780 --> 00:06:04,120
when students should use the tech and when they shouldn't

152
00:06:04,340 --> 00:06:06,020
老師們一開始有點避開這件事。

152
00:06:04,340 --> 00:06:06,020
teachers sort of hid from it.

153
00:06:06,300 --> 00:06:09,800
我們一開始只是在管控它的使用，而不是靜下心來

153
00:06:06,300 --> 00:06:09,800
We started with policing its use rather than actually sitting there

154
00:06:09,800 --> 00:06:11,040
思考我們究竟該怎麼

154
00:06:09,800 --> 00:06:11,040
and figuring out how do we actually

155
00:06:11,040 --> 00:06:13,200
重新設計我們評量學生的方式

155
00:06:11,040 --> 00:06:13,200
want to redesign the way we assess students

156
00:06:13,200 --> 00:06:14,500
還有我們布置作業的方式。

156
00:06:13,200 --> 00:06:14,500
and the way we assign homework.

157
00:06:15,980 --> 00:06:16,960
但我覺得我們正在進步。

157
00:06:15,980 --> 00:06:16,960
But I think we're moving.

158
00:06:16,960 --> 00:06:18,340
我們已經跨過那個階段了。

158
00:06:16,960 --> 00:06:18,340
We're moving beyond that point.

159
00:06:18,540 --> 00:06:19,320
是啊，這讓我很受鼓舞。

159
00:06:18,540 --> 00:06:19,320
Yeah, I've been encouraged.

160
00:06:19,420 --> 00:06:21,500
我記得我在這裡的時候，我們看到

160
00:06:19,420 --> 00:06:21,500
And we saw I remember when I was here

161
00:06:21,560 --> 00:06:23,700
有一個學校系統當時有點

161
00:06:21,560 --> 00:06:23,700
we had one school system that was like kind

162
00:06:23,700 --> 00:06:25,660
反射性地說要禁止它。

162
00:06:23,700 --> 00:06:25,660
of a knee jerk reaction said we're banning it.

163
00:06:26,040 --> 00:06:27,460
結果過了幾個月，

163
00:06:26,040 --> 00:06:27,460
And it was like a few months later

164
00:06:27,460 --> 00:06:30,260
裡面有不少老師開始說，欸，不對，

164
00:06:27,460 --> 00:06:30,260
they had enough teachers within there going, hey, no

165
00:06:30,340 --> 00:06:31,520
這其實是個很棒的工具。

165
00:06:30,340 --> 00:06:31,520
this is a really good tool.

166
00:06:31,620 --> 00:06:33,180
我們知道該怎麼教這個。

166
00:06:31,620 --> 00:06:33,180
We know how to teach to this.

167
00:06:33,280 --> 00:06:34,240
他們就說，好啊，

167
00:06:33,280 --> 00:06:34,240
And they said, OK

168
00:06:34,360 --> 00:06:35,780
我們現在要改變這個做法。

168
00:06:34,360 --> 00:06:35,780
we're going to reverse that now.

169
00:06:35,780 --> 00:06:38,680
事情就是這樣，這真的很

169
00:06:35,780 --> 00:06:38,680
And that was it was it was very

170
00:06:39,980 --> 00:06:44,480
很高興看到充滿熱情的老師們接受這個，並說，好啊，

170
00:06:39,980 --> 00:06:44,480
good to see energized teachers embrace this and say, OK

171
00:06:44,700 --> 00:06:46,000
因為他們希望學生能理解，

171
00:06:44,700 --> 00:06:46,000
in wanting their students to understand

172
00:06:46,140 --> 00:06:47,520
他們知道這會成為未來的一部分。

172
00:06:46,140 --> 00:06:47,520
they knew this was going to be part of the future.

173
00:06:47,760 --> 00:06:49,200
現在有了學習模式。

173
00:06:47,760 --> 00:06:49,200
Well, now there's study mode.

174
00:06:49,320 --> 00:06:50,460
你可以稍微聊聊這個嗎？

174
00:06:49,320 --> 00:06:50,460
Could you talk a little about that?

175
00:06:51,040 --> 00:06:53,400
學習模式是我們剛推出的功能，

175
00:06:51,040 --> 00:06:53,400
So study mode is a product we just launched

176
00:06:53,460 --> 00:06:56,840
目的是要真正提升學習成效，

176
00:06:53,460 --> 00:06:56,840
and it is intended to take really improve learning

177
00:06:56,840 --> 00:07:00,220
並將 ChatGPT 從只專注於提供答案的體驗，

177
00:06:56,840 --> 00:07:00,220
and chat GPT and take it from an experience that is just focused

178
00:07:00,220 --> 00:07:04,560
轉變為真正引導學生找到答案。

178
00:07:00,220 --> 00:07:04,560
on giving answers to really guiding a student to get to the answers.

179
00:07:04,560 --> 00:07:06,220
所以在學習模式下，

179
00:07:04,560 --> 00:07:06,220
So study in study mode.

180
00:07:06,580 --> 00:07:08,120
ChatGPT 會用蘇格拉底式提問來回答。

180
00:07:06,580 --> 00:07:08,120
Chachapati answers socratically.

181
00:07:08,420 --> 00:07:12,540
它會根據你的學習程度，量身打造回應。

181
00:07:08,420 --> 00:07:12,540
It personalizes responses to sort of the level of your learning.

182
00:07:12,980 --> 00:07:14,840
它能掌握你正在學習的脈絡。

182
00:07:12,980 --> 00:07:14,840
It understands the context of what you're learning.

183
00:07:15,560 --> 00:07:17,360
它會提出很有深度的追問。

183
00:07:15,560 --> 00:07:17,360
It asks great follow up questions.

184
00:07:17,380 --> 00:07:19,500
它會詢問你是否想針對這個主題進行測驗。

184
00:07:17,380 --> 00:07:19,500
It asks if you want to have a quiz on the topic.

185
00:07:19,680 --> 00:07:21,980
它會鼓勵你更深入探索。

185
00:07:19,680 --> 00:07:21,980
It encourages you to to go deeper.

186
00:07:22,780 --> 00:07:23,320
而最終，

186
00:07:22,780 --> 00:07:23,320
And ultimately,

187
00:07:23,560 --> 00:07:26,760
這是邁向真正學習的第一步，

187
00:07:23,560 --> 00:07:26,760
this is this is a first step to really leaning

188
00:07:26,760 --> 00:07:30,160
讓 ChatGPT 成為你專屬家教的概念。

188
00:07:26,760 --> 00:07:30,160
into this idea of of Chachapati as a as a tutor.

189
00:07:30,300 --> 00:07:31,760
那這個想法是怎麼產生的？

189
00:07:30,300 --> 00:07:31,760
So how did this come about?

190
00:07:31,760 --> 00:07:34,780
有跟教育工作者和家長討論過這件事嗎？

190
00:07:31,760 --> 00:07:34,780
Was there conversations with educators and parents about this?

191
00:07:35,640 --> 00:07:40,660
其實學習模式是我們團隊去印度時誕生的，

191
00:07:35,640 --> 00:07:40,660
So study mode actually came out of a trip that our team took in India,

192
00:07:41,180 --> 00:07:42,520
當時他們有一些發現，

192
00:07:41,180 --> 00:07:42,520
where they had a few

193
00:07:42,520 --> 00:07:43,880
他們意識到幾個重點。

193
00:07:42,520 --> 00:07:43,880
where they realized a few things.

194
00:07:44,000 --> 00:07:46,680
首先，他們發現像印度這樣的地方，

194
00:07:44,000 --> 00:07:46,680
One, they realized that in a place like India

195
00:07:46,940 --> 00:07:48,740
許多家庭花了很大一部分的

195
00:07:46,940 --> 00:07:48,740
families were spending a huge percentage of their

196
00:07:48,740 --> 00:07:52,280
每人平均花在家教和課後輔導上的收入。

196
00:07:48,740 --> 00:07:52,280
per capita income on tutors and after school help.

197
00:07:52,940 --> 00:07:55,580
他們也發現，年輕人有著強烈的意願

197
00:07:52,940 --> 00:07:55,580
They also realized that there was just a tremendous will

198
00:07:55,580 --> 00:07:58,820
和渴望想要更上一層樓。

198
00:07:55,580 --> 00:07:58,820
and desire among young people to get to get to the next level.

199
00:07:59,480 --> 00:08:00,520
所以，你知道的，

199
00:07:59,480 --> 00:08:00,520
And so, you know

200
00:08:00,600 --> 00:08:01,760
我們就這樣開始了這段旅程，

200
00:08:00,600 --> 00:08:01,760
we began this journey of like

201
00:08:01,820 --> 00:08:03,840
我們需要做什麼，才能讓ChatGPT

201
00:08:01,820 --> 00:08:03,840
what would it take to actually get Chachapati

202
00:08:03,840 --> 00:08:07,080
變成比現在更優秀的家教？

202
00:08:03,840 --> 00:08:07,080
and turn Chachapati into an even better tutor than it already is?

203
00:08:07,660 --> 00:08:09,200
所以我們一開始就，

203
00:08:07,660 --> 00:08:09,200
And so we started by, you know

204
00:08:09,200 --> 00:08:12,100
這也是我第一次參與打造AI產品的經驗。

204
00:08:09,200 --> 00:08:12,100
and this was my first my first experience building like an AI product.

205
00:08:12,220 --> 00:08:16,000
我們開始根據學習科學建立一套架構，

205
00:08:12,220 --> 00:08:16,000
So we started actually building a schema informed by learning science

206
00:08:16,000 --> 00:08:18,600
並且參考教學專家的建議，

206
00:08:16,000 --> 00:08:18,600
and informed by pedagogical experts that said

207
00:08:18,800 --> 00:08:20,620
ChatGPT應該如何回應？

207
00:08:18,800 --> 00:08:20,620
how should Chachapati respond?

208
00:08:20,960 --> 00:08:22,060
它不只是單純給你答案，

208
00:08:20,960 --> 00:08:22,060
It's not just going to give answers

209
00:08:22,100 --> 00:08:23,400
而是會真正協助你，

209
00:08:22,100 --> 00:08:23,400
but it's going to actually help you.

210
00:08:23,760 --> 00:08:24,260
讓你真的學會。

210
00:08:23,760 --> 00:08:24,260
You'd learn.

211
00:08:24,760 --> 00:08:27,940
接著我們與全球的專家合作，

211
00:08:24,760 --> 00:08:27,940
And then we work with open with experts around the world to

212
00:08:27,940 --> 00:08:29,540
蒐集我們所謂的黃金範例，

212
00:08:27,940 --> 00:08:29,540
gather what we call golden examples

213
00:08:29,540 --> 00:08:32,000
這些都是Chachapati理想回應的範例。

213
00:08:29,540 --> 00:08:32,000
of how Chachapati would ideally respond.

214
00:08:32,640 --> 00:08:33,820
語氣是否具有鼓勵性？

214
00:08:32,640 --> 00:08:33,820
Is the tone encouraging?

215
00:08:34,160 --> 00:08:35,400
是否能激發好奇心？

215
00:08:34,160 --> 00:08:35,400
Does it encourage curiosity?

216
00:08:36,140 --> 00:08:40,920
回應是否貼合學生的需求程度？

216
00:08:36,140 --> 00:08:40,920
Does it cater a response to the level of a student's need?

217
00:08:41,560 --> 00:08:43,540
透過這樣反覆的過程，

217
00:08:41,560 --> 00:08:43,540
And through that process of sort of going back

218
00:08:43,540 --> 00:08:44,980
來回調整並訓練模型，

218
00:08:43,540 --> 00:08:44,980
and forth and training the model

219
00:08:45,140 --> 00:08:47,480
學習模式就是這樣誕生的。

219
00:08:45,140 --> 00:08:47,480
that's how study mode emerged.

220
00:08:47,900 --> 00:08:48,720
我想說，

220
00:08:47,900 --> 00:08:48,720
And I would say, you know

221
00:08:48,800 --> 00:08:50,720
我們非常期待看到大家的回饋，

221
00:08:48,800 --> 00:08:50,720
we're super excited to see the feedback

222
00:08:50,720 --> 00:08:52,200
來自全球對這項產品的反應，

222
00:08:50,720 --> 00:08:52,200
on this product out in the world

223
00:08:52,260 --> 00:08:54,500
但這同時也只是個開始。

223
00:08:52,260 --> 00:08:54,500
but it's also very much a beginning.

224
00:08:54,800 --> 00:08:55,620
沒錯，這是一個起點。

224
00:08:54,800 --> 00:08:55,620
Yeah, it's a starting point.

225
00:08:55,640 --> 00:08:56,400
這就是起點。

225
00:08:55,640 --> 00:08:56,400
It's a starting point.

226
00:08:56,400 --> 00:08:57,000
你知道嗎，

226
00:08:56,400 --> 00:08:57,000
You know,

227
00:08:57,060 --> 00:09:00,200
其實我們現在才剛開始探索

227
00:08:57,060 --> 00:09:00,200
we've we're really only in the early stages of pushing the way in

228
00:09:00,200 --> 00:09:02,440
讓這個模型能以多種形式回應的可能性。

228
00:09:00,200 --> 00:09:02,440
which the model can respond in multimodal ways.

229
00:09:02,920 --> 00:09:03,020
你知道嗎，

229
00:09:02,920 --> 00:09:03,020
You know,

230
00:09:03,100 --> 00:09:05,740
有一天你可以想像請學習模式提供你

230
00:09:03,100 --> 00:09:05,740
one day you could imagine asking study mode to give you

231
00:09:05,740 --> 00:09:09,660
一些生物作業的範例來幫你解釋有機化學

231
00:09:05,740 --> 00:09:09,660
samples of a sort of a biology assignment to explain organic chemistry

232
00:09:09,660 --> 00:09:13,680
還會跳出互動圖表，或適時提醒你，

232
00:09:09,660 --> 00:09:13,680
and to pop up interactive diagrams or to nudge you,

233
00:09:14,040 --> 00:09:15,140
例如三週後提醒你。

233
00:09:14,040 --> 00:09:15,140
you know, three weeks down the road.

234
00:09:15,240 --> 00:09:16,180
你還記得你曾經跟我說過

234
00:09:15,240 --> 00:09:16,180
You remember you told me

235
00:09:16,180 --> 00:09:19,540
你想要在今年的有機化學考試拿高分。

235
00:09:16,180 --> 00:09:19,540
that you wanted to ace this year's organic chemistry exam.

236
00:09:19,680 --> 00:09:22,100
我們現在要不要再回來討論這個主題？

236
00:09:19,680 --> 00:09:22,100
Can shall we dig back into the topic now?

237
00:09:22,400 --> 00:09:26,260
它可以主動陪伴你，長時間支持你的學習。

237
00:09:22,400 --> 00:09:26,260
It could be proactive and really sort of travel with you over time.

238
00:09:26,400 --> 00:09:27,740
我覺得隨著時間過去，

238
00:09:26,400 --> 00:09:27,740
And I think over time

239
00:09:27,940 --> 00:09:29,780
我們希望學習模式能發展到那個程度。

239
00:09:27,940 --> 00:09:29,780
the hope is to get study mode to that point.

240
00:09:30,140 --> 00:09:30,560
那真的很棒。

240
00:09:30,140 --> 00:09:30,560
That'd be great.

241
00:09:30,640 --> 00:09:32,400
沒錯，我覺得這如果結合間隔重複會很有效。

241
00:09:30,640 --> 00:09:32,400
Yeah, I see that combined with spaced repetition.

242
00:09:32,720 --> 00:09:34,940
而且你可以擁有一個不只是很棒的工具，

242
00:09:32,720 --> 00:09:34,940
And you could have just a really good tool that doesn't just

243
00:09:35,720 --> 00:09:37,100
不僅幫助你通過考試，

243
00:09:35,720 --> 00:09:37,100
help you pass the test

244
00:09:37,100 --> 00:09:38,940
還能讓你長時間記住內容。

244
00:09:37,100 --> 00:09:38,940
but help you remember it for a long time.

245
00:09:39,320 --> 00:09:41,080
我覺得你剛剛提到了一個重點，

245
00:09:39,320 --> 00:09:41,080
And I think you touched about something there

246
00:09:41,240 --> 00:09:45,240
就是在某些家庭裡，

246
00:09:41,240 --> 00:09:45,240
which is that in, you know, certain households

247
00:09:45,480 --> 00:09:47,600
他們有能力請私人家教。

247
00:09:45,480 --> 00:09:47,600
they can afford to have private tutors.

248
00:09:47,780 --> 00:09:48,780
家教課程確實很棒。

248
00:09:47,780 --> 00:09:48,780
Tutoring program's great.

249
00:09:48,960 --> 00:09:50,280
我們討論的是一種

249
00:09:48,960 --> 00:09:50,280
And we talk about kind

250
00:09:50,280 --> 00:09:52,980
教育機會上的差異。

250
00:09:50,280 --> 00:09:52,980
of the differences in educational opportunities.

251
00:09:53,640 --> 00:09:55,380
同一所學校裡可能有一群孩子，

251
00:09:53,640 --> 00:09:55,380
You can have a bunch of kids that go to the same school

252
00:09:55,380 --> 00:09:57,200
但那些能讓孩子

252
00:09:55,380 --> 00:09:57,200
but the parents that are able to have the kid

253
00:09:57,200 --> 00:09:59,460
去上家教課的家長，

253
00:09:57,200 --> 00:09:59,460
go to a tutor are going to be in a situation

254
00:09:59,460 --> 00:10:01,420
孩子通常會有更高的成就機會。

254
00:09:59,460 --> 00:10:01,420
where the child's going to have a higher chance of an outcome.

255
00:10:02,340 --> 00:10:03,820
現在你把這個視為一種平衡機會的工具。

255
00:10:02,340 --> 00:10:03,820
Now you see this as a leveler.

256
00:10:04,360 --> 00:10:05,820
你看，我認為AI最先產生重大影響的地方

256
00:10:04,360 --> 00:10:05,820
Look, I think the first place

257
00:10:05,820 --> 00:10:07,500
就是人工智慧帶來巨大改變的領域

257
00:10:05,820 --> 00:10:07,500
where AI is having a huge impact

258
00:10:07,500 --> 00:10:10,000
在教育上，其實並不是在教室裡。

258
00:10:07,500 --> 00:10:10,000
in education is not even in the classroom.

259
00:10:10,140 --> 00:10:11,360
而是在課堂之外，

259
00:10:10,140 --> 00:10:11,360
It's outside of the classroom

260
00:10:11,360 --> 00:10:15,660
它讓每個人都能獲得類似成人協助的資源，實現機會平等。

260
00:10:11,360 --> 00:10:15,660
where it's equalizing access to what's really like adult support.

261
00:10:15,940 --> 00:10:17,240
世界上有許多學生

261
00:10:15,940 --> 00:10:17,240
There are many students out in the world

262
00:10:17,240 --> 00:10:19,040
無法接觸到優質的老師。

262
00:10:17,240 --> 00:10:19,040
who don't have access to a quality teacher.

263
00:10:19,180 --> 00:10:20,340
他們也沒有補習老師可協助。

263
00:10:19,180 --> 00:10:20,340
They don't have access to tutors.

264
00:10:20,440 --> 00:10:21,700
他們的父母也無法

264
00:10:20,440 --> 00:10:21,700
They don't have access to parents who

265
00:10:21,700 --> 00:10:23,160
抽空坐下來幫他們學習。

265
00:10:21,700 --> 00:10:23,160
are going to sit down and help them.

266
00:10:23,160 --> 00:10:24,580
但現在有了AI，

266
00:10:23,160 --> 00:10:24,580
And now with AI

267
00:10:24,820 --> 00:10:28,420
他們可以擁有一個會鼓勵他們的學習夥伴，

267
00:10:24,820 --> 00:10:28,420
they can have this companion who can encourage them

268
00:10:28,560 --> 00:10:30,840
能針對作業或寫作給予回饋，

268
00:10:28,560 --> 00:10:30,840
who can give feedback on their homework, on their writing

269
00:10:31,160 --> 00:10:33,400
也能協助他們解答難題。

269
00:10:31,160 --> 00:10:33,400
who can help them answer tough questions.

270
00:10:35,100 --> 00:10:39,720
我們成立了一個由ChatGPT學生用戶組成的實驗小組。

270
00:10:35,100 --> 00:10:39,720
We formed a lab of student users of ChatGPT.

271
00:10:39,940 --> 00:10:41,040
它叫做 ChatGPT Lab。

271
00:10:39,940 --> 00:10:41,040
It's called ChatGPT Lab.

272
00:10:41,100 --> 00:10:42,940
我想你會和幾位學生聊聊。

272
00:10:41,100 --> 00:10:42,940
I think you're going to talk to a few of the students.

273
00:10:43,480 --> 00:10:46,700
有一點讓我很有感觸，他們說使用

273
00:10:43,480 --> 00:10:46,700
And one thing that really struck me is they said using

274
00:10:47,380 --> 00:10:49,780
使用 AI 讓他們更有自信，

274
00:10:47,380 --> 00:10:49,780
using AI gave them confidence

275
00:10:51,160 --> 00:10:52,740
甚至到了這樣的程度，

275
00:10:51,160 --> 00:10:52,740
and it got to the place

276
00:10:52,740 --> 00:10:55,420
以前遇到困難或感到沮喪的時候。

276
00:10:52,740 --> 00:10:55,420
where formerly they would feel stuck or they would feel discouraged.

277
00:10:56,060 --> 00:10:58,880
其中一位學生分享了她在課堂上的經歷，

277
00:10:56,060 --> 00:10:58,880
One of the students told the story of being in the classroom

278
00:10:59,580 --> 00:11:01,260
她是一位資工系的學生。

278
00:10:59,580 --> 00:11:01,260
as a computer science student.

279
00:11:02,040 --> 00:11:02,740
而且好幾年來，

279
00:11:02,040 --> 00:11:02,740
And for many years

280
00:11:02,860 --> 00:11:05,240
她在資工課程裡常常卡關，

280
00:11:02,860 --> 00:11:05,240
she would get stuck in her computer science courses

281
00:11:05,240 --> 00:11:06,460
甚至一度想要放棄。

281
00:11:05,240 --> 00:11:06,460
and she started to give up.

282
00:11:06,860 --> 00:11:07,960
她看不懂課本內容。

282
00:11:06,860 --> 00:11:07,960
She couldn't understand the textbooks.

283
00:11:08,400 --> 00:11:11,220
但當她在課外用 ChatGPT 當家教時，

283
00:11:08,400 --> 00:11:11,220
But when she used ChatGPT out of school as a tutor

284
00:11:11,280 --> 00:11:12,520
她開始覺得，啊，

284
00:11:11,280 --> 00:11:12,520
she started to feel like, oh

285
00:11:13,060 --> 00:11:13,960
我可以主動發問。

285
00:11:13,060 --> 00:11:13,960
I can ask questions.

286
00:11:14,100 --> 00:11:15,120
我可以理解這一點。

286
00:11:14,100 --> 00:11:15,120
I can understand this.

287
00:11:15,240 --> 00:11:15,380
比如說，

287
00:11:15,240 --> 00:11:15,380
Like,

288
00:11:15,400 --> 00:11:18,520
我有信心，也許我真的能繼續努力向前。

288
00:11:15,400 --> 00:11:18,520
I have confidence and maybe I can actually continue and move forward.

289
00:11:18,520 --> 00:11:20,940
所以我覺得這同樣重要，你知道的，

289
00:11:18,520 --> 00:11:20,940
And so I think it's as much, you know

290
00:11:21,000 --> 00:11:22,760
如果你想想看，家教在世界上扮演什麼角色，

290
00:11:21,000 --> 00:11:22,760
if you think about what tutors do in the world

291
00:11:22,860 --> 00:11:23,680
他們會給你鼓勵。

291
00:11:22,860 --> 00:11:23,680
they give you encouragement.

292
00:11:23,940 --> 00:11:26,040
他們會讓你產生『我可以做到』的自信感，

292
00:11:23,940 --> 00:11:26,040
They give you a sense of confidence that I can

293
00:11:26,040 --> 00:11:27,480
讓你有動力，也能繼續往前走。

293
00:11:26,040 --> 00:11:27,480
and I want and I can move forward.

294
00:11:27,780 --> 00:11:28,480
然後，當然，

294
00:11:27,780 --> 00:11:28,480
And then, of course

295
00:11:28,520 --> 00:11:31,660
他們會用貼近情境、個人化的方式傳達內容。

295
00:11:28,520 --> 00:11:31,660
they deliver the content in contextual and personal ways.

296
00:11:32,080 --> 00:11:33,940
我認為 ChatGPT 也能做到這些事情。

296
00:11:32,080 --> 00:11:33,940
I think ChatGPT can do all of these things.

297
00:11:34,780 --> 00:11:36,960
那麼談到職場生產力時，

297
00:11:34,780 --> 00:11:36,960
So when it comes to workforce productivity

298
00:11:37,180 --> 00:11:38,820
那在個人層面上是怎麼發揮作用的？

298
00:11:37,180 --> 00:11:38,820
how does that work on a personal scale?

299
00:11:38,980 --> 00:11:40,340
你會給大家什麼建議？

299
00:11:38,980 --> 00:11:40,340
What advice would you give to people?

300
00:11:41,320 --> 00:11:44,800
所有最新的數據都顯示，那些員工——

300
00:11:41,320 --> 00:11:44,800
So all the recent data shows that workers who

301
00:11:44,800 --> 00:11:47,060
在職場中運用人工智慧的人，生產力顯著提升。

301
00:11:44,800 --> 00:11:47,060
use AI in the workforce are incredibly more productive.

302
00:11:47,580 --> 00:11:49,400
這在某些領域尤其明顯，

302
00:11:47,580 --> 00:11:49,400
It's particularly true in fields

303
00:11:49,400 --> 00:11:51,720
像是專業服務業和金融相關領域。

303
00:11:49,400 --> 00:11:51,720
like professional services and financial students.

304
00:11:51,880 --> 00:11:52,920
但其實，

304
00:11:51,880 --> 00:11:52,920
But, you know, really

305
00:11:52,920 --> 00:11:56,700
現在任何畢業離校的學生都必須要

305
00:11:52,920 --> 00:11:56,700
any graduate who who leaves institution today needs to

306
00:11:56,700 --> 00:11:58,600
懂得如何在日常生活中運用人工智慧。

306
00:11:56,700 --> 00:11:58,600
know how to use AI in their daily life.

307
00:11:58,680 --> 00:11:59,440
這不僅體現在

307
00:11:58,680 --> 00:11:59,440
And that will come in both

308
00:11:59,440 --> 00:12:00,480
他們求職時，

308
00:11:59,440 --> 00:12:00,480
when they're applying for jobs

309
00:12:00,500 --> 00:12:02,820
也包括剛開始新工作的時候。

309
00:12:00,500 --> 00:12:02,820
as well as when they start start their new job.

310
00:12:03,220 --> 00:12:04,660
因此，其中一個重要原因是

310
00:12:03,220 --> 00:12:04,660
And so one of the big reasons

311
00:12:04,660 --> 00:12:07,620
學校現在將人工智慧作為核心

311
00:12:04,660 --> 00:12:07,620
why institutions are now deploying AI as core

312
00:12:07,620 --> 00:12:10,400
基礎設施佈建在校園各處，是希望

312
00:12:07,620 --> 00:12:10,400
infrastructure across campus is they want to make

313
00:12:10,400 --> 00:12:13,200
學生畢業時能具備這些職場技能。

313
00:12:10,400 --> 00:12:13,200
sure their students leave with those workforce skills.

314
00:12:13,840 --> 00:12:16,300
數據顯示，有七成

314
00:12:13,840 --> 00:12:16,300
The data shows that seven in 10

315
00:12:16,300 --> 00:12:18,980
雇主更傾向聘用懂得人工智慧的人。

315
00:12:16,300 --> 00:12:18,980
employers would rather hire someone with AI

316
00:12:18,980 --> 00:12:21,100
技能甚至超越那些擁有

316
00:12:18,980 --> 00:12:21,100
skills over someone who had up to

317
00:12:21,100 --> 00:12:23,440
在特定職能領域有十年經驗的人。

317
00:12:21,100 --> 00:12:23,440
10 years of experience in a given function.

318
00:12:23,880 --> 00:12:24,980
我親眼見過這種情況。

318
00:12:23,880 --> 00:12:24,980
I've seen that.

319
00:12:25,080 --> 00:12:26,860
我經營一家公司，專門協助組建

319
00:12:25,080 --> 00:12:26,860
I run a firm where I help put together

320
00:12:26,860 --> 00:12:29,000
團隊，與企業合作或協助他們導入人工智慧。

320
00:12:26,860 --> 00:12:29,000
teams to work with companies or trying to deploy AI.

321
00:12:29,200 --> 00:12:31,420
而我們在尋找合作夥伴時最重視的

321
00:12:29,200 --> 00:12:31,420
And the thing that we look for in

322
00:12:31,420 --> 00:12:34,000
就是他們是否具備人工智慧相關技能。

322
00:12:31,420 --> 00:12:34,000
people we work with is basically AI skills.

323
00:12:34,140 --> 00:12:36,120
例如，有人花了六個月學習如何運用這些技術。

323
00:12:34,140 --> 00:12:36,120
Somebody who spent six months learning how to use this stuff.

324
00:12:36,160 --> 00:12:37,620
我其實不在意他們的LinkedIn

324
00:12:36,160 --> 00:12:37,620
I don't really care what their LinkedIn

325
00:12:37,620 --> 00:12:39,860
或他們的學經歷背景長什麼樣。

325
00:12:37,620 --> 00:12:39,860
or actually their curriculum looked like.

326
00:12:39,920 --> 00:12:40,740
我只想知道

326
00:12:39,920 --> 00:12:40,740
I just want to know

327
00:12:41,220 --> 00:12:42,060
你有實際在用嗎？

327
00:12:41,220 --> 00:12:42,060
have you been using it?

328
00:12:42,120 --> 00:12:42,900
你會操作嗎？

328
00:12:42,120 --> 00:12:42,900
Can you use it?

329
00:12:43,540 --> 00:12:43,940
你聽我說，

329
00:12:43,540 --> 00:12:43,940
Look,

330
00:12:44,000 --> 00:12:45,840
我認為另一種很重要的核心素養

330
00:12:44,000 --> 00:12:45,840
I think the other sort of core literacy

331
00:12:45,840 --> 00:12:48,360
現在變得重要的，就是程式設計。

331
00:12:45,840 --> 00:12:48,360
that's going to become important now is coding.

332
00:12:48,700 --> 00:12:50,820
你知道，曾經有一段時間，

332
00:12:48,700 --> 00:12:50,820
You know, for a while it was, you know

333
00:12:50,820 --> 00:12:53,380
有一陣子我們認為所有學生都必須學習程式設計。

333
00:12:50,820 --> 00:12:53,380
there was a moment where we thought all students need to learn coding.

334
00:12:53,520 --> 00:12:55,440
後來，我覺得大家開始把重點放在工程師身上。

334
00:12:53,520 --> 00:12:55,440
And then I think there was a big focus on engineers.

335
00:12:55,660 --> 00:12:57,240
但現在隨著無程式碼開發的興起，

335
00:12:55,660 --> 00:12:57,240
But now with vibe coding

336
00:12:57,240 --> 00:12:59,860
而且現在有各種讓寫程式變得更簡單的工具，

336
00:12:57,240 --> 00:12:59,860
and now that there are all sorts of tools that make coding easier

337
00:13:00,020 --> 00:13:01,400
我認為我們會進入一個新階段，

337
00:13:00,020 --> 00:13:01,400
I think we're going to get to a place

338
00:13:01,400 --> 00:13:04,460
每個學生不僅要學會一般操作人工智慧，

338
00:13:01,400 --> 00:13:04,460
where every student should not only learn how to use AI generally

339
00:13:04,540 --> 00:13:07,080
還要學會運用人工智慧來創作，

339
00:13:04,540 --> 00:13:07,080
but they should learn to use AI to create,

340
00:13:07,520 --> 00:13:11,680
像是生成圖片、開發應用程式、撰寫程式碼。

340
00:13:07,520 --> 00:13:11,680
to create image, to create create applications, to to write code.

341
00:13:11,680 --> 00:13:15,200
我認為基本的程式設計將會成為一種核心素養。

341
00:13:11,680 --> 00:13:15,200
And I think sort of basic coding is going to become a core literacy.

342
00:13:15,340 --> 00:13:16,400
這一點同樣非常重要。

342
00:13:15,340 --> 00:13:16,400
That's going to be important as well.

343
00:13:16,680 --> 00:13:18,420
是啊，我也常聽到有人說，

343
00:13:16,680 --> 00:13:18,420
Yeah, I get I hear people go like

344
00:13:18,560 --> 00:13:20,180
既然AI會幫你寫程式，為什麼還要學寫程式？

344
00:13:18,560 --> 00:13:20,180
why learn to code because AI codes for you.

345
00:13:20,200 --> 00:13:21,140
但其實，

345
00:13:20,200 --> 00:13:21,140
And it's like, like

346
00:13:21,260 --> 00:13:23,480
如果書裡全都是文字，那我們為什麼還要學閱讀？

346
00:13:21,260 --> 00:13:23,480
why learn to read if books are full of text?

347
00:13:23,480 --> 00:13:24,420
我當下心裡想，才不是這樣

347
00:13:23,480 --> 00:13:24,420
I was like, like, no

348
00:13:24,460 --> 00:13:25,900
這世界還是靠這些東西在運作。

348
00:13:24,460 --> 00:13:25,900
the world still runs on these things.

349
00:13:25,900 --> 00:13:26,820
我想，沒錯

349
00:13:25,900 --> 00:13:26,820
And I think, yeah

350
00:13:26,820 --> 00:13:27,720
這真的很有價值。

350
00:13:26,820 --> 00:13:27,720
it's a very valuable.

351
00:13:28,400 --> 00:13:30,040
你知道，我有個同事

351
00:13:28,400 --> 00:13:30,040
You know, one of my colleagues, you know

352
00:13:30,060 --> 00:13:31,460
他主要做很多金融相關的事情

352
00:13:30,060 --> 00:13:31,460
he does a lot of financial stuff

353
00:13:31,460 --> 00:13:32,660
他還會開發新的計算機

353
00:13:31,460 --> 00:13:32,660
and he's creating new calculators

354
00:13:32,660 --> 00:13:34,500
而且經常在做各種小工具。

354
00:13:32,660 --> 00:13:34,500
and building the little tools for this all the time.

355
00:13:34,640 --> 00:13:36,400
我則是自己做一些工具

355
00:13:34,640 --> 00:13:36,400
I build stuff for myself

356
00:13:36,400 --> 00:13:37,520
用來學習或其他用途。

356
00:13:36,400 --> 00:13:37,520
for learning and whatever.

357
00:13:37,760 --> 00:13:39,260
而且我本來就會寫程式

357
00:13:37,760 --> 00:13:39,260
And, you know, I already knew how to code

358
00:13:39,260 --> 00:13:40,560
但這讓一切變得更簡單。

358
00:13:39,260 --> 00:13:40,560
but it just makes it so much easier.

359
00:13:40,660 --> 00:13:42,320
而且了解程式碼的運作很有效率。

359
00:13:40,660 --> 00:13:42,320
And knowing what the code does is effective.

360
00:13:42,680 --> 00:13:43,120
對，完全同意。

360
00:13:42,680 --> 00:13:43,120
Yeah, absolutely.

361
00:13:43,300 --> 00:13:44,180
所以我的看法是

361
00:13:43,300 --> 00:13:44,180
So I'm of the view

362
00:13:44,180 --> 00:13:47,180
隨著寫程式變得越來越簡單，

362
00:13:44,180 --> 00:13:47,180
that as coding becomes something that's easier,

363
00:13:47,700 --> 00:13:49,580
理解程式碼的能力

363
00:13:47,700 --> 00:13:49,580
the ability to understand code

364
00:13:49,760 --> 00:13:52,400
撰寫和除錯程式碼的能力將會成為一項基本素養。

364
00:13:49,760 --> 00:13:52,400
to create it and debug it is going to become a core literacy.

365
00:13:52,580 --> 00:13:55,260
這一點會變得越來越重要。

365
00:13:52,580 --> 00:13:55,260
That's going to become increasingly more important.

366
00:13:55,980 --> 00:13:57,220
沒錯，我覺得這是很棒的建議。

366
00:13:55,980 --> 00:13:57,220
Yeah, I think that's great advice.

367
00:13:57,480 --> 00:14:00,280
我認為人們花越多時間嘗試使用這些工具，

367
00:13:57,480 --> 00:14:00,280
I think just the more time people spend trying to use these tools,

368
00:14:00,380 --> 00:14:01,620
嘗試各種不同的方式和這些工具。

368
00:14:00,380 --> 00:14:01,620
trying different things, these tools.

369
00:14:02,200 --> 00:14:03,640
我聽過一個很有效的方法，

369
00:14:02,200 --> 00:14:03,640
And one of the things I've heard that works really

370
00:14:03,640 --> 00:14:06,160
就是定期和其他人聚會，

370
00:14:03,640 --> 00:14:06,160
well is just meeting on a regular basis with other people

371
00:14:06,260 --> 00:14:08,800
不管是在職場上，比如每週一次，

371
00:14:06,260 --> 00:14:08,800
whether it's at a workplace, you know, like have once a week

372
00:14:08,800 --> 00:14:09,960
或每個月聚一次，

372
00:14:08,800 --> 00:14:09,960
or once a month to get together

373
00:14:09,960 --> 00:14:13,120
一起討論事情，或讓學生分享他們的學習心得。

373
00:14:09,960 --> 00:14:13,120
and talk about stuff or students sharing what they've learned.

374
00:14:14,760 --> 00:14:17,740
最近有一些聳動的新聞標題在討論

374
00:14:14,760 --> 00:14:17,740
So there's been some sort of sensational headlines talking about

375
00:14:17,740 --> 00:14:20,840
AI在教育領域可能會導致大腦退化。

375
00:14:17,740 --> 00:14:20,840
how AI can cause brain rot when it comes to education.

376
00:14:21,720 --> 00:14:23,160
我明白你的想法

376
00:14:21,720 --> 00:14:23,160
Look, I hear you

377
00:14:23,260 --> 00:14:25,860
這是我每天都會被問到的問題之一。

377
00:14:23,260 --> 00:14:25,860
and it's one of these questions I get asked every day.

378
00:14:26,720 --> 00:14:28,080
人工智慧終究只是一個工具。

378
00:14:26,720 --> 00:14:28,080
AI is ultimately a tool.

379
00:14:28,380 --> 00:14:31,760
而在教育領域，最重要的是這個工具如何被運用。

379
00:14:28,380 --> 00:14:31,760
And what matters most in education space is how that tool is used.

380
00:14:32,500 --> 00:14:33,500
學習本來就需要努力與掙扎。

380
00:14:32,500 --> 00:14:33,500
Learning takes struggle.

381
00:14:33,860 --> 00:14:35,180
需要主動處理資訊。

381
00:14:33,860 --> 00:14:35,180
It takes working with information.

382
00:14:35,360 --> 00:14:36,460
還要消化這些資訊。

382
00:14:35,360 --> 00:14:36,460
It takes processing it.

383
00:14:36,460 --> 00:14:38,920
如果學生把人工智慧當成答案產生器來用，

383
00:14:36,460 --> 00:14:38,920
If students use AI as an answer machine

384
00:14:39,440 --> 00:14:40,200
他們就學不到東西。

384
00:14:39,440 --> 00:14:40,200
they're not going to learn.

385
00:14:40,580 --> 00:14:43,020
我想這就是大家所擔心的。

385
00:14:40,580 --> 00:14:43,020
And so I think that's what people are are worried about.

386
00:14:43,340 --> 00:14:46,180
所以我們的任務之一，就是協助學生

386
00:14:43,340 --> 00:14:46,180
And so part of our journey here is really to help students

387
00:14:46,180 --> 00:14:49,980
以及教育者善用人工智慧，來拓展批判性思維，

387
00:14:46,180 --> 00:14:49,980
and educators use AI in ways that will expand critical thinking

388
00:14:50,080 --> 00:14:51,460
並激發創造力。

388
00:14:50,080 --> 00:14:51,460
that will expand creativity.

389
00:14:52,940 --> 00:14:56,640
你不用去讀艱澀的研究報告也知道，

389
00:14:52,940 --> 00:14:56,640
And you don't need to read complex studies to know

390
00:14:56,640 --> 00:14:59,740
人工智慧的使用方式會帶來正面或負面的影響。

390
00:14:56,640 --> 00:14:59,740
that AI used in certain ways is positive or negative.

391
00:14:59,840 --> 00:15:00,980
我會想到我女兒，對吧？

391
00:14:59,840 --> 00:15:00,980
I think of my daughter, right?

392
00:15:01,040 --> 00:15:02,600
她現在正在學長除法。

392
00:15:01,040 --> 00:15:02,600
She's learning long division right now.

393
00:15:02,740 --> 00:15:04,220
這需要一點掙扎。

393
00:15:02,740 --> 00:15:04,220
It takes a certain amount of struggle.

394
00:15:04,360 --> 00:15:05,440
有時還會掉幾滴眼淚。

394
00:15:04,360 --> 00:15:05,440
There's some tears involved.

395
00:15:05,440 --> 00:15:07,640
如果我把計算機交給她，然後說，

395
00:15:05,440 --> 00:15:07,640
If I were to hand her a calculator and said,

396
00:15:07,720 --> 00:15:08,940
不要自己算除法題，

396
00:15:07,720 --> 00:15:08,940
don't do the division problem

397
00:15:09,020 --> 00:15:10,640
直接丟給 AI 算，

397
00:15:09,020 --> 00:15:10,640
just put it into into AI

398
00:15:10,780 --> 00:15:12,120
你就會學會長除法。

398
00:15:10,780 --> 00:15:12,120
you're going to learn long division.

399
00:15:12,400 --> 00:15:13,500
她其實學不會。

399
00:15:12,400 --> 00:15:13,500
She wouldn't.

400
00:15:13,660 --> 00:15:15,280
但同時，你知道，

400
00:15:13,660 --> 00:15:15,280
But at the same time, you know

401
00:15:15,320 --> 00:15:17,300
等她以後學更高階的數學時，

401
00:15:15,320 --> 00:15:17,300
down the road as she's learning advanced math

402
00:15:17,520 --> 00:15:19,920
我預期到時候我可以給她那台計算機，

402
00:15:17,520 --> 00:15:19,920
I anticipate I can give her that calculator

403
00:15:19,920 --> 00:15:21,880
她就能用它來算數學，

403
00:15:19,920 --> 00:15:21,880
and she'll be able to you do math

404
00:15:21,880 --> 00:15:23,940
而且能做到比原本更高層次。

404
00:15:21,880 --> 00:15:23,940
at a higher level than she otherwise would.

405
00:15:24,660 --> 00:15:26,500
AI 的使用方式也是一樣的道理。

405
00:15:24,660 --> 00:15:26,500
AI uses is no different.

406
00:15:26,700 --> 00:15:29,440
它必須以能夠促進回饋的方式來使用

406
00:15:26,700 --> 00:15:29,440
It needs to be used in a way that drives feedback

407
00:15:29,660 --> 00:15:31,460
讓人們能夠獲得個人化的輔導

407
00:15:29,660 --> 00:15:31,460
that gives people personal tutoring

408
00:15:31,560 --> 00:15:35,300
幫助你用各種不同的方式提問和回答問題。

408
00:15:31,560 --> 00:15:35,300
that helps you ask and answer questions and in different ways.

409
00:15:35,920 --> 00:15:37,340
這就是我們推動學習進步的方法。

409
00:15:35,920 --> 00:15:37,340
That's how we're going to advance learning.

410
00:15:38,000 --> 00:15:38,040
沒錯，

410
00:15:38,000 --> 00:15:38,040
Yeah,

411
00:15:38,060 --> 00:15:40,060
我看到有一項研究剛剛

411
00:15:38,060 --> 00:15:40,060
I read there was one study that just

412
00:15:40,060 --> 00:15:41,640
最近上了新聞，我還特地坐下來

412
00:15:40,060 --> 00:15:41,640
recently made the news and I actually sat down

413
00:15:41,740 --> 00:15:42,660
我讀完後心裡想

413
00:15:41,740 --> 00:15:42,660
I read it and I was saying

414
00:15:42,800 --> 00:15:43,920
如果你只是複製貼上答案

414
00:15:42,800 --> 00:15:43,920
if you copy paste answers

415
00:15:43,960 --> 00:15:44,500
你根本學不到東西

415
00:15:43,960 --> 00:15:44,500
you don't learn

416
00:15:44,660 --> 00:15:45,640
這讓我覺得很驚訝。

416
00:15:44,660 --> 00:15:45,640
which was just like, wow.

417
00:15:46,100 --> 00:15:47,640
這就像如果我正在訓練

417
00:15:46,100 --> 00:15:47,640
It's like if I were if I were training

418
00:15:47,640 --> 00:15:49,720
準備馬拉松，而你卻說，

418
00:15:47,640 --> 00:15:49,720
for a marathon and you said like,

419
00:15:49,980 --> 00:15:50,300
欸，Leah，

419
00:15:49,980 --> 00:15:50,300
hey, Leah

420
00:15:50,360 --> 00:15:51,200
我可以給你一台滑板車嗎？

420
00:15:50,360 --> 00:15:51,200
can I give you a scooter?

421
00:15:51,420 --> 00:15:52,800
而在某些訓練課程中，

421
00:15:51,420 --> 00:15:52,800
And for some of these training runs

422
00:15:52,820 --> 00:15:54,660
你可能會想騎著滑板車沿著路走。

422
00:15:52,820 --> 00:15:54,660
you'd like take a scooter down the road.

423
00:15:55,040 --> 00:15:56,240
這樣並不會讓我變得更健康。

423
00:15:55,040 --> 00:15:56,240
I wouldn't get into more shape.

424
00:15:56,340 --> 00:15:58,000
這跟剛才那個情況很像。

424
00:15:56,340 --> 00:15:58,000
That was very similar to that.

425
00:15:58,340 --> 00:15:58,700
沒錯。

425
00:15:58,340 --> 00:15:58,700
Yeah.

426
00:15:58,800 --> 00:16:00,340
我覺得令人沮喪的是，

426
00:15:58,800 --> 00:16:00,340
And I think the frustrating thing is like

427
00:16:00,380 --> 00:16:03,060
我認為這裡確實有一些很重要的問題需要討論。

427
00:16:00,380 --> 00:16:03,060
I think there really are important questions to be asked about this.

428
00:16:03,060 --> 00:16:05,040
我確實認為，像批判性思考這樣的能力，

428
00:16:03,060 --> 00:16:05,040
I do think that, you know, critical thinking skills

429
00:16:05,140 --> 00:16:07,600
我們必須思考要怎麼培養這些能力，以及如何持續精進。

429
00:16:05,140 --> 00:16:07,600
we have to think about how we develop these and how we pursue that.

430
00:16:07,960 --> 00:16:10,000
每當我再次看到這種情況時，我都會有點沮喪，

430
00:16:07,960 --> 00:16:10,000
I get a little frustrated that when I see I again

431
00:16:10,160 --> 00:16:12,420
我相信那份研究一定有它合理的解釋，

431
00:16:10,160 --> 00:16:12,420
I'm sure there's a real good defense of that study

432
00:16:12,480 --> 00:16:13,620
但當我看到這類研究時，

432
00:16:12,480 --> 00:16:13,620
but I see studies like this is

433
00:16:14,280 --> 00:16:15,560
你其實只是在陳述一個大家都知道的事實。

433
00:16:14,280 --> 00:16:15,560
you're saying an obvious thing.

434
00:16:15,640 --> 00:16:16,620
我們真正需要思考的是，

434
00:16:15,640 --> 00:16:16,620
The thing we need to think about

435
00:16:16,620 --> 00:16:19,200
我們到底該怎麼實際運用這些工具？

435
00:16:16,620 --> 00:16:19,200
is how do you actually use these tools?

436
00:16:19,340 --> 00:16:21,440
你要如何提升自己的批判性思考能力？

436
00:16:19,340 --> 00:16:21,440
How can you improve your critical thinking skills?

437
00:16:21,640 --> 00:16:25,020
我認為這會是一個開放式的問題。

437
00:16:21,640 --> 00:16:25,020
And I think that's going to be, you know, an open end question.

438
00:16:25,120 --> 00:16:26,280
我們得持續不斷地自我提問。

438
00:16:25,120 --> 00:16:26,280
We're going to have to keep asking ourselves.

439
00:16:26,640 --> 00:16:29,340
你看，我們之所以創建 Study Mood，部分原因是因為

439
00:16:26,640 --> 00:16:29,340
Look, part of the reason why we created Study Mood

440
00:16:29,560 --> 00:16:31,760
它基本上是一種為學生量身打造的輔導體驗，

440
00:16:29,560 --> 00:16:31,760
which is essentially a tutoring experience for students

441
00:16:31,760 --> 00:16:34,540
是為了讓學生不必再學習如何

441
00:16:31,760 --> 00:16:34,540
is so students wouldn't have to learn how to

442
00:16:34,540 --> 00:16:36,800
用特定方式對模型下指令來獲得回饋，

442
00:16:34,540 --> 00:16:36,800
prompt the model in ways that would give feedback

443
00:16:36,800 --> 00:16:40,200
或進行測驗，或推動學習。

443
00:16:36,800 --> 00:16:40,200
or quiz or or drive learning.

444
00:16:40,380 --> 00:16:42,400
相反地，這會是一種你可以進入的模式，

444
00:16:40,380 --> 00:16:42,400
Instead, it would be a mode that you would enter

445
00:16:42,400 --> 00:16:44,480
在這個模式下，模型本身會主動引導你，

445
00:16:42,400 --> 00:16:44,480
where the model itself would push you

446
00:16:44,480 --> 00:16:46,540
並帶領你找到答案，

446
00:16:44,480 --> 00:16:46,540
and would guide you towards answers

447
00:16:46,540 --> 00:16:49,460
同時提供個人化的背景資訊與知識架構。

447
00:16:46,540 --> 00:16:49,460
and personalize and give context and scaffold knowledge.

448
00:16:50,060 --> 00:16:53,400
所以我們正努力在 ChatGPT 中打造這樣的學習體驗，

448
00:16:50,060 --> 00:16:53,400
So we're on this journey to create an experience in chat GPT

449
00:16:53,400 --> 00:16:57,660
讓學生不需要懂得特定的使用方式也能上手。

449
00:16:53,400 --> 00:16:57,660
where a student doesn't have to know how to use it in certain ways.

450
00:16:58,140 --> 00:16:58,840
我認為這就是

450
00:16:58,140 --> 00:16:58,840
And I think that's

451
00:16:58,840 --> 00:16:59,960
你會真正看到

451
00:16:58,840 --> 00:16:59,960
where you're going to really see

452
00:17:00,610 --> 00:17:02,220
學習的界線被不斷推進。

452
00:17:00,610 --> 00:17:02,220
the boundaries of learning being advanced.

453
00:17:02,580 --> 00:17:03,440
是啊，我真的很興奮

453
00:17:02,580 --> 00:17:03,440
Yeah, and I'm I'm excited

454
00:17:03,440 --> 00:17:06,220
因為我聽到一些家長分享的親身經歷

454
00:17:03,440 --> 00:17:06,220
because I'm hearing kind of anecdotal stories

455
00:17:06,220 --> 00:17:08,620
來自家長，他們會

455
00:17:06,220 --> 00:17:08,620
from parents where they'll,

456
00:17:08,920 --> 00:17:10,260
在家長陪同下

456
00:17:08,920 --> 00:17:10,260
while supervised

457
00:17:10,380 --> 00:17:13,360
但會讓孩子開始和ChatGPT對話。

457
00:17:10,380 --> 00:17:13,360
but they'll let their kid start a conversation, the chat GPT.

458
00:17:13,500 --> 00:17:15,540
你很快就會發現ChatGPT

458
00:17:13,500 --> 00:17:15,540
And you quickly realize the chat GPT

459
00:17:15,540 --> 00:17:17,660
對於討論青蛙這件事有無限的耐心。

459
00:17:15,540 --> 00:17:17,660
has infinite patience to talk about frogs.

460
00:17:18,120 --> 00:17:19,880
你永遠不會讓它感到厭煩。

460
00:17:18,120 --> 00:17:19,880
You can never exhaust it.

461
00:17:19,920 --> 00:17:21,240
它可以一直和你聊青蛙。

461
00:17:19,920 --> 00:17:21,240
It'll talk about frogs forever.

462
00:17:21,240 --> 00:17:24,580
而那個剛開始對爬蟲類有興趣的小朋友，可能突然間

462
00:17:21,240 --> 00:17:24,580
And that budding herpetologist might all of a sudden, you know

463
00:17:24,580 --> 00:17:26,680
發現自己真的很喜歡聊這個主題

463
00:17:24,580 --> 00:17:26,680
realize that they really like talking about this

464
00:17:26,700 --> 00:17:27,800
即使他們的父母沒興趣。

464
00:17:26,700 --> 00:17:27,800
even though their parents don't.

465
00:17:27,800 --> 00:17:32,380
ChatGPT神奇的地方之一就是它非常有耐心，

465
00:17:27,800 --> 00:17:32,380
One of the things that's magical about chat GPT is it is patient,

466
00:17:32,780 --> 00:17:33,280
無限地。

466
00:17:32,780 --> 00:17:33,280
infinitely.

467
00:17:33,600 --> 00:17:35,200
所以你問的任何問題

467
00:17:33,600 --> 00:17:35,200
So no question you ask

468
00:17:35,320 --> 00:17:37,020
ChatGPT 永遠不會覺得你的問題愚蠢。

468
00:17:35,320 --> 00:17:37,020
chat GPT is is ever stupid.

469
00:17:37,220 --> 00:17:38,240
它總是會回應你。

469
00:17:37,220 --> 00:17:38,240
It will always respond.

470
00:17:38,300 --> 00:17:40,980
它總是會給你誠實且合理的答案。

470
00:17:38,300 --> 00:17:40,980
It will always give you an honest, reasonable answer.

471
00:17:41,640 --> 00:17:42,700
而且你知道，

471
00:17:41,640 --> 00:17:42,700
And to, you know

472
00:17:42,760 --> 00:17:46,600
學習有一半是在於建立自信和獲得學習的動力。

472
00:17:42,760 --> 00:17:46,600
half of learning is about feeling confident, inspired to learn.

473
00:17:46,900 --> 00:17:49,540
我們在 Coursera 常說的一句話是，

473
00:17:46,900 --> 00:17:49,540
One of the things we always said at Coursera is, you know

474
00:17:49,560 --> 00:17:52,020
打造優秀的課程和技術是一回事，

474
00:17:49,560 --> 00:17:52,020
it's one thing to build amazing courses and amazing tech

475
00:17:52,040 --> 00:17:53,560
但要真正讓孩子學會，

475
00:17:52,040 --> 00:17:53,560
but to really get a kid to learn

476
00:17:53,640 --> 00:17:57,120
你必須讓他們覺得自己學得會，也想要學。

476
00:17:53,640 --> 00:17:57,120
you need to have them feel like I can learn and I want to learn.

477
00:17:57,120 --> 00:17:59,400
我認為我在

477
00:17:57,120 --> 00:17:59,400
I think a lot of what I is doing in the

478
00:17:59,400 --> 00:18:01,860
教育領域做的很多事情，都是在給予

478
00:17:59,400 --> 00:18:01,860
education space is is giving

479
00:18:01,860 --> 00:18:03,880
人們建立信心的支撐

479
00:18:01,860 --> 00:18:03,880
people that scaffolding to have confidence

480
00:18:03,880 --> 00:18:06,340
讓他們能夠勇於投入並學習。

480
00:18:03,880 --> 00:18:06,340
that they can dive in and they can learn.

481
00:18:07,100 --> 00:18:08,620
我之所以來到OpenAI，有一個原因是

481
00:18:07,100 --> 00:18:08,620
So one of the reasons I came to open

482
00:18:08,700 --> 00:18:10,440
其實也和我女兒有關。

482
00:18:08,700 --> 00:18:10,440
I was actually also connected to my daughter.

483
00:18:11,040 --> 00:18:13,380
她有閱讀障礙。

483
00:18:11,040 --> 00:18:13,380
She is she is dyslexic.

484
00:18:13,780 --> 00:18:14,700
而且多年來，

484
00:18:13,780 --> 00:18:14,700
And for years,

485
00:18:14,700 --> 00:18:16,440
我常看到她哥哥下樓，

485
00:18:14,700 --> 00:18:16,440
I would watch her brother come down

486
00:18:16,440 --> 00:18:17,780
早上讀報紙，

486
00:18:16,440 --> 00:18:17,780
in the morning and read the newspaper

487
00:18:17,780 --> 00:18:19,780
我則默默地想，

487
00:18:17,780 --> 00:18:19,780
and I would stick to myself and think, you know

488
00:18:19,800 --> 00:18:21,200
這麼聰明的小女孩，

488
00:18:19,800 --> 00:18:21,200
how is this brilliant little girl going

489
00:18:21,200 --> 00:18:23,300
到底要怎麼學會接觸這個世界？

489
00:18:21,200 --> 00:18:23,300
to actually learn how to access the world?

490
00:18:23,360 --> 00:18:25,420
她要怎麼了解時事呢？

490
00:18:23,360 --> 00:18:25,420
How is she going to learn that about current events?

491
00:18:26,000 --> 00:18:27,760
在我加入OpenAI的前一個夏天，

491
00:18:26,000 --> 00:18:27,760
And the summer before I joined OpenAI

492
00:18:27,940 --> 00:18:29,340
我們推出了進階語音模式。

492
00:18:27,940 --> 00:18:29,340
we launched advanced voice mode.

493
00:18:30,540 --> 00:18:33,280
我記得把手機交給她，跟她說：Zoe，

493
00:18:30,540 --> 00:18:33,280
And I remember handing her the phone and I said, Zoe

494
00:18:33,340 --> 00:18:35,040
你要不要試著跟ChatGPT聊聊看？

494
00:18:33,340 --> 00:18:35,040
why don't you try talking with Chachi Petit?

495
00:18:35,180 --> 00:18:36,900
她說：嘿，ChatGPT

495
00:18:35,180 --> 00:18:36,900
And she said, hey, Chachi Petit

496
00:18:37,160 --> 00:18:40,160
我媽媽擔心我沒辦法了解最新時事

496
00:18:37,160 --> 00:18:40,160
my mom is worried that I'm not going to learn about current events

497
00:18:40,160 --> 00:18:42,120
因為我不像我哥哥那樣能讀報紙。

497
00:18:40,160 --> 00:18:42,120
because I can't read the newspaper like my brother.

498
00:18:42,800 --> 00:18:44,200
你可以跟我說說世界上正在發生什麼事嗎？

498
00:18:42,800 --> 00:18:44,200
Can you tell me what's going on in the world?

499
00:18:44,760 --> 00:18:45,880
那一刻真的很觸動人心，

499
00:18:44,760 --> 00:18:45,880
And it was so poignant as

500
00:18:45,880 --> 00:18:47,920
我記得 ChatGPT 回覆說，

500
00:18:45,880 --> 00:18:47,920
I remember Chachi Petit responding and saying,

501
00:18:48,720 --> 00:18:50,060
當然可以，Zoe，

501
00:18:48,720 --> 00:18:50,060
you know, sure, Zoe

502
00:18:50,260 --> 00:18:51,460
你對哪些事情有興趣？

502
00:18:50,260 --> 00:18:51,460
what are you interested in?

503
00:18:51,680 --> 00:18:53,500
你今天想多了解世界上的哪些事？

503
00:18:51,680 --> 00:18:53,500
What do you want to learn about in the world today?

504
00:18:53,500 --> 00:18:54,760
然後，

504
00:18:53,500 --> 00:18:54,760
And from there,

505
00:18:54,900 --> 00:18:57,280
對話就這樣繼續下去，他們聊起了

505
00:18:54,900 --> 00:18:57,280
the conversation went on and they were talking

506
00:18:57,280 --> 00:18:59,440
她對太空和機器人等各種興趣。

506
00:18:57,280 --> 00:18:59,440
about all her interests in space and robots.

507
00:18:59,960 --> 00:19:02,440
就在那一刻我意識到，

507
00:18:59,960 --> 00:19:02,440
And I realized in that moment that, you know

508
00:19:02,520 --> 00:19:05,140
ChatGPT 將會為這個女孩打開世界的大門。

508
00:19:02,520 --> 00:19:05,140
Chachi Petit was going to unlock the world for this girl.

509
00:19:05,280 --> 00:19:07,540
而我也不用再像以前那樣擔心了。

509
00:19:05,280 --> 00:19:07,540
And I was not going to have to be worried in the same way.

510
00:19:09,320 --> 00:19:10,120
這真的很有力量。

510
00:19:09,320 --> 00:19:10,120
That's powerful.

511
00:19:11,540 --> 00:19:15,000
我很期待看到它如何協助解決這類問題，

511
00:19:11,540 --> 00:19:15,000
I'm excited to see how it helps with issues like that,

512
00:19:15,120 --> 00:19:16,620
像是可近性相關的問題。

512
00:19:15,120 --> 00:19:16,620
accessibility issues.

513
00:19:16,940 --> 00:19:18,920
我覺得未來觀察它的發展會很有趣，

513
00:19:16,940 --> 00:19:18,920
And I think that's it's going to be interesting to see

514
00:19:18,920 --> 00:19:20,540
幾年後我們會走到哪一步。

514
00:19:18,920 --> 00:19:20,540
where we are a few years from now.

515
00:19:21,720 --> 00:19:23,200
Leah，非常感謝你和我聊這些。

515
00:19:21,720 --> 00:19:23,200
Leah, thank you so much for talking with me.

516
00:19:23,200 --> 00:19:24,260
這次對談真的很有意思。

516
00:19:23,200 --> 00:19:24,260
This has been very interesting.

517
00:19:24,460 --> 00:19:26,000
我很期待接下來的發展會如何。

517
00:19:24,460 --> 00:19:26,000
I'm excited to see where these things go next.

518
00:19:26,440 --> 00:19:26,680
太棒了！

518
00:19:26,440 --> 00:19:26,680
Awesome.

519
00:19:26,920 --> 00:19:27,600
很高興和你聊天，Andrew。

519
00:19:26,920 --> 00:19:27,600
Great talking to you, Andrew.

520
00:19:29,730 --> 00:19:32,290
我也想多認識你們兩位一點。

520
00:19:29,730 --> 00:19:32,290
I'd also like to know a little bit more about you both, too.

521
00:19:32,670 --> 00:19:35,330
Yabi，你可以跟我說說你現在在念什麼嗎？

521
00:19:32,670 --> 00:19:35,330
So Yabi, could you tell me what you're studying right now?

522
00:19:35,950 --> 00:19:36,210
嗯，

522
00:19:35,950 --> 00:19:36,210
Yeah.

523
00:19:36,450 --> 00:19:38,270
其實我剛完成了

523
00:19:36,450 --> 00:19:38,270
So I actually just finished up

524
00:19:38,270 --> 00:19:40,870
在南加大修完了傳播學的學士學位，

524
00:19:38,270 --> 00:19:40,870
my undergraduate degree in communication at USC

525
00:19:41,170 --> 00:19:43,930
接下來我要開始第一學期，

525
00:19:41,170 --> 00:19:43,930
and then I'm going to enter the first semester

526
00:19:43,930 --> 00:19:46,030
這是我商業分析碩士課程的一部分，

526
00:19:43,930 --> 00:19:46,030
of my master's program in business analytics,

527
00:19:46,130 --> 00:19:46,910
同樣是在南加州大學。

527
00:19:46,130 --> 00:19:46,910
also at USC.

528
00:19:47,190 --> 00:19:47,730
好啊，聽起來不錯。

528
00:19:47,190 --> 00:19:47,730
OK, cool.

529
00:19:47,870 --> 00:19:50,090
那你為什麼會想選這個呢？

529
00:19:47,870 --> 00:19:50,090
So why did you want to choose that?

530
00:19:51,570 --> 00:19:53,230
剛進大學的時候，

530
00:19:51,570 --> 00:19:53,230
Initially, when I came to college

531
00:19:53,230 --> 00:19:54,070
我其實沒有很確定。

531
00:19:53,230 --> 00:19:54,070
I wasn't really sure.

532
00:19:54,070 --> 00:19:56,250
也不太確定自己想讀什麼科目。

532
00:19:54,070 --> 00:19:56,250
Kind of like the subject matter I wanted to study.

533
00:19:56,790 --> 00:20:00,530
所以我花了很多時間修通識課程來探索。

533
00:19:56,790 --> 00:20:00,530
So I took like a lot of time to take GE courses to kind of explore.

534
00:20:01,290 --> 00:20:02,030
然後在這個過程中，

534
00:20:01,290 --> 00:20:02,030
And then through there

535
00:20:02,110 --> 00:20:03,530
我發現我比較喜歡

535
00:20:02,110 --> 00:20:03,530
I think I like the more of

536
00:20:03,530 --> 00:20:05,750
研究和理論中比較偏向人的那一面。

536
00:20:03,530 --> 00:20:05,750
the human component of like research and theory.

537
00:20:06,290 --> 00:20:08,050
我發現傳播學是一個很

537
00:20:06,290 --> 00:20:08,050
And I found that communication was like a really

538
00:20:08,050 --> 00:20:10,130
適合探索這方面的主修。

538
00:20:08,050 --> 00:20:10,130
good major to like kind of explore that through.

539
00:20:10,810 --> 00:20:12,850
然後在整個大學期間，

539
00:20:10,810 --> 00:20:12,850
And then throughout, like, you know, my college career

540
00:20:12,850 --> 00:20:14,990
我開始修了很多統計課。

540
00:20:12,850 --> 00:20:14,990
I started taking a lot of stat classes

541
00:20:15,690 --> 00:20:16,810
然後，你知道嗎

541
00:20:15,690 --> 00:20:16,810
and then, you know

542
00:20:16,850 --> 00:20:18,870
讓我打下了分析思考的基礎。

542
00:20:16,850 --> 00:20:18,870
gave me the basis for like analytical thinking.

543
00:20:18,910 --> 00:20:20,570
而我真的很想把這部分補上。

543
00:20:18,910 --> 00:20:20,570
And I really wanted to kind of add that on.

544
00:20:20,910 --> 00:20:22,510
所以我又選修了資料科學作為副修，

544
00:20:20,910 --> 00:20:22,510
So I added a data science minor

545
00:20:23,130 --> 00:20:24,490
然後就有了那種

545
00:20:23,130 --> 00:20:24,490
and then kind of having that

546
00:20:24,490 --> 00:20:27,270
創意和分析兩種元素結合在一起的感覺。

546
00:20:24,490 --> 00:20:27,270
like creative and analytical component come together.

547
00:20:27,390 --> 00:20:29,310
我想這也讓我走向了商業分析這條路。

547
00:20:27,390 --> 00:20:29,310
I think it led me to business analytics.

548
00:20:29,690 --> 00:20:31,510
所以這真的是個很有趣的組合。

548
00:20:29,690 --> 00:20:31,510
So that's it's a very interesting combination there.

549
00:20:31,610 --> 00:20:32,930
事情會這樣發展真的很酷。

549
00:20:31,610 --> 00:20:32,930
That's pretty cool the way that came about.

550
00:20:33,710 --> 00:20:35,710
那 Alop，跟我們說說你現在在學什麼吧。

550
00:20:33,710 --> 00:20:35,710
So Alop, tell us what you're studying.

551
00:20:36,030 --> 00:20:36,230
嗯，

551
00:20:36,030 --> 00:20:36,230
Yeah,

552
00:20:36,290 --> 00:20:39,610
我現在在柏克萊大學主修電機工程和電腦科學。

552
00:20:36,290 --> 00:20:39,610
I'm studying electrical engineering and computer science at Berkeley.

553
00:20:39,690 --> 00:20:40,610
我即將升上大二。

553
00:20:39,690 --> 00:20:40,610
I'm a rising sophomore.

554
00:20:41,330 --> 00:20:43,070
是什麼吸引你投入這些領域的？

554
00:20:41,330 --> 00:20:43,070
What brought you into those fields?

555
00:20:43,850 --> 00:20:45,290
我想從小到大

555
00:20:43,850 --> 00:20:45,290
I think growing up

556
00:20:45,330 --> 00:20:46,370
我是在灣區長大的

556
00:20:45,330 --> 00:20:46,370
I grew up in the Bay Area

557
00:20:46,490 --> 00:20:49,990
所以我從小就接觸到很多科技產品。

557
00:20:46,490 --> 00:20:49,990
so I've been exposed to a lot of tech around me.

558
00:20:50,090 --> 00:20:51,890
我本身也很喜歡動手實作。

558
00:20:50,090 --> 00:20:51,890
And I'm also like a really hands on guy.

559
00:20:51,890 --> 00:20:55,630
我喜歡擺弄電路，或寫寫程式碼。

559
00:20:51,890 --> 00:20:55,630
I like fidgeting with circuits or playing around with code.

560
00:20:55,670 --> 00:20:58,850
我很喜歡替我發現的問題找出解決方法。

560
00:20:55,670 --> 00:20:58,850
And I really like finding solutions to some issues that I see.

561
00:20:59,310 --> 00:21:01,270
我發現我最喜歡的方法之一

561
00:20:59,310 --> 00:21:01,270
And I found that one of my favorite ways

562
00:21:01,270 --> 00:21:03,750
就是透過寫程式和硬體工程來解決。

562
00:21:01,270 --> 00:21:03,750
to do that was through code and through hardware engineering.

563
00:21:04,010 --> 00:21:06,590
所以我覺得這兩者結合起來很棒。

563
00:21:04,010 --> 00:21:06,590
So I thought this would be a good combination of the two.

564
00:21:06,950 --> 00:21:08,230
你小時候也喜歡自己動手做東西嗎？

564
00:21:06,950 --> 00:21:08,230
Did you like to build things when you were younger?

565
00:21:08,530 --> 00:21:09,570
對，真的很喜歡。

565
00:21:08,530 --> 00:21:09,570
Yeah, yeah, definitely.

566
00:21:09,690 --> 00:21:11,890
像是我做過太陽能車。

566
00:21:09,690 --> 00:21:11,890
Like I build like a solar powered car.

567
00:21:12,310 --> 00:21:12,870
當然啦。

567
00:21:12,310 --> 00:21:12,870
Oh, of course.

568
00:21:13,310 --> 00:21:14,570
對，就是做一台小的。

568
00:21:13,310 --> 00:21:14,570
Yeah, just like a small one.

569
00:21:14,650 --> 00:21:15,990
後來我又做了一台大的。

569
00:21:14,650 --> 00:21:15,990
And then I built a big one later.

570
00:21:16,190 --> 00:21:16,810
喔，沒錯，當然。

570
00:21:16,190 --> 00:21:16,810
Oh, yeah, obviously.

571
00:21:17,190 --> 00:21:18,890
對啊，沒錯，

571
00:21:17,190 --> 00:21:18,890
Yeah, but yeah,

572
00:21:18,930 --> 00:21:20,690
只要是我能親眼看到的東西，

572
00:21:18,930 --> 00:21:20,690
just anything I could just like

573
00:21:20,690 --> 00:21:24,270
真的能親自看到並想像出來。

573
00:21:20,690 --> 00:21:24,270
really see just in person and visualize.

574
00:21:25,090 --> 00:21:26,610
所以你們兩位都很有前瞻性。

574
00:21:25,090 --> 00:21:26,610
So you guys are both pretty forward thinking.

575
00:21:26,730 --> 00:21:31,790
你可以跟我分享你第一次對 AI 有重大領悟的時刻是什麼嗎？

575
00:21:26,730 --> 00:21:31,790
Could you tell me what was kind of your first big aha moment with AI?

576
00:21:32,270 --> 00:21:33,410
是指任何類型的 AI 嗎？

576
00:21:32,270 --> 00:21:33,410
Any kind of AI in general?

577
00:21:33,730 --> 00:21:35,430
我記得那時我高三，

577
00:21:33,730 --> 00:21:35,430
I remember I was a junior in high school

578
00:21:35,430 --> 00:21:38,170
那時候大家都在討論，欸，你知道

578
00:21:35,430 --> 00:21:38,170
and and this kind of buzz of, oh, do you know

579
00:21:38,230 --> 00:21:39,270
你有聽過 ChatGPT 嗎？

579
00:21:38,230 --> 00:21:39,270
have you heard of ChachiBT?

580
00:21:39,530 --> 00:21:40,670
這個話題開始流傳起來。

580
00:21:39,530 --> 00:21:40,670
Kind of started going around.

581
00:21:41,270 --> 00:21:44,610
其實在那之前 AI 早就已經存在了。

581
00:21:41,270 --> 00:21:44,610
And that was like that was like obviously AI had existed before.

582
00:21:45,270 --> 00:21:47,850
我以前也看過 ASIMO 機器人。

582
00:21:45,270 --> 00:21:47,850
I had witnessed like the Osimo robot.

583
00:21:47,990 --> 00:21:48,850
不知道你有沒有聽過那個。

583
00:21:47,990 --> 00:21:48,850
I don't know if you've heard of that.

584
00:21:48,850 --> 00:21:50,030
對，就是那種人形機器人。

584
00:21:48,850 --> 00:21:50,030
But yeah, so humanoids.

585
00:21:50,250 --> 00:21:52,030
但真正讓我印象深刻的是看到一個 AI，

585
00:21:50,250 --> 00:21:52,030
But but really like seeing like an AI that

586
00:21:52,030 --> 00:21:55,650
我真正能夠使用並互動的是 ChatGPT。

586
00:21:52,030 --> 00:21:55,650
I could actually use and interact with was ChachiBT.

587
00:21:55,910 --> 00:21:57,930
我們大家都圍在我的電腦旁邊。

587
00:21:55,910 --> 00:21:57,930
And we're all huddling around my computer.

588
00:21:57,930 --> 00:22:00,150
我剛剛在 OpenAI 註冊了一個帳號。

588
00:21:57,930 --> 00:22:00,150
And I just made an account with OpenAI.

589
00:22:00,670 --> 00:22:02,530
我有一份作業是要討論如何

589
00:22:00,670 --> 00:22:02,530
And I had an assignment on how

590
00:22:02,530 --> 00:22:04,350
寫我的《梅岡城故事》心得報告。

590
00:22:02,530 --> 00:22:04,350
to write my to kill a mockingbird essay.

591
00:22:04,550 --> 00:22:05,410
我當時心想

591
00:22:04,550 --> 00:22:05,410
And I was like

592
00:22:05,590 --> 00:22:07,150
如果它真的什麼都能做到的話

592
00:22:05,590 --> 00:22:07,150
if it can if it can really do anything

593
00:22:07,230 --> 00:22:08,250
那就來看看它會怎麼寫。

593
00:22:07,230 --> 00:22:08,250
let's see how it writes.

594
00:22:08,510 --> 00:22:08,650
對啊。

594
00:22:08,510 --> 00:22:08,650
Right.

595
00:22:08,790 --> 00:22:11,710
結果它馬上幫我寫出一篇完整的《梅岡城故事》作文。

595
00:22:08,790 --> 00:22:11,710
And boom, it wrote me a full to kill a mockingbird essay.

596
00:22:11,770 --> 00:22:12,650
我沒有拿去用

596
00:22:11,770 --> 00:22:12,650
I did not use it

597
00:22:12,670 --> 00:22:14,590
但看到這一幕真的很酷。

597
00:22:12,670 --> 00:22:14,590
but it was definitely cool to see.

598
00:22:14,670 --> 00:22:16,550
這真的是讓我印象深刻的一刻。

598
00:22:14,670 --> 00:22:16,550
And just like a moment that I really remember.

599
00:22:16,550 --> 00:22:18,330
對，我想再多問你一些關於這件事的細節。

599
00:22:16,550 --> 00:22:18,330
Yeah, I want to ask you a little bit more about that.

600
00:22:18,810 --> 00:22:19,770
那你呢？

600
00:22:18,810 --> 00:22:19,770
So how about you?

601
00:22:20,410 --> 00:22:21,450
對我來說，我覺得

601
00:22:20,410 --> 00:22:21,450
I think for me

602
00:22:21,770 --> 00:22:23,250
說真的，我覺得這很有趣，因為

602
00:22:21,770 --> 00:22:23,250
it was honestly funny because I think a

603
00:22:23,250 --> 00:22:25,010
很多人一開始都是用在學術上

603
00:22:23,250 --> 00:22:25,010
lot of people started off with like an academic

604
00:22:25,010 --> 00:22:27,230
或是拿來當作學習用途。

604
00:22:25,010 --> 00:22:27,230
or like study use case for it.

605
00:22:27,270 --> 00:22:29,130
但我記得我偶然發現它的時候

605
00:22:27,270 --> 00:22:29,130
But then I think when I stumbled across from it

606
00:22:29,150 --> 00:22:31,370
我記得那時候我是大二生，

606
00:22:29,150 --> 00:22:31,370
I remember I was like a sophomore,

607
00:22:31,550 --> 00:22:33,810
大二的第一學期。

607
00:22:31,550 --> 00:22:33,810
my first semester of sophomore year in college.

608
00:22:34,090 --> 00:22:37,130
我在社群媒體上看到有人分享說，

608
00:22:34,090 --> 00:22:37,130
And I saw like the social media posts about like how, you know

609
00:22:37,150 --> 00:22:40,610
你可以用 ChatGPT 做一些事情，像是

609
00:22:37,150 --> 00:22:40,610
you kind of use ChachiBT for like certain like, you know

610
00:22:40,650 --> 00:22:41,810
幫我寫故事。

610
00:22:40,650 --> 00:22:41,810
write a story for me.

611
00:22:42,230 --> 00:22:45,630
所以我第一次給它的指令就是寫同人小說，

611
00:22:42,230 --> 00:22:45,630
And so I think my first prompt to it was to like write fanfiction

612
00:22:45,630 --> 00:22:47,250
這真的超好笑。

612
00:22:45,630 --> 00:22:47,250
which is like really funny.

613
00:22:47,350 --> 00:22:49,070
這只是個很隨性的用途。

613
00:22:47,350 --> 00:22:49,070
And like it was just like a random use case.

614
00:22:49,190 --> 00:22:50,370
我還記得我拿給室友看，

614
00:22:49,190 --> 00:22:50,370
I remember showing my roommates

615
00:22:50,370 --> 00:22:53,710
說真的，他們都覺得蠻蠢的。

615
00:22:50,370 --> 00:22:53,710
and they all thought it was pretty stupid, honestly.

616
00:22:54,570 --> 00:22:58,290
所以我覺得那時候大多都是一些很平常、隨機的使用情境。

616
00:22:54,570 --> 00:22:58,290
And so I think it was just like a lot of mundane, random use cases.

617
00:22:58,810 --> 00:23:00,230
然後我記得是到後來

617
00:22:58,810 --> 00:23:00,230
And then I think it was like later

618
00:23:00,230 --> 00:23:02,250
我才真的開始在學業上使用它

618
00:23:00,230 --> 00:23:02,250
on that I actually started using it like academically

619
00:23:02,490 --> 00:23:05,270
特別是當我開始修更多偏重程式設計的課程時。

619
00:23:02,490 --> 00:23:05,270
specifically when I started doing like more coding oriented classes.

620
00:23:05,610 --> 00:23:08,270
但我覺得讓我有頓悟的時刻是

620
00:23:05,610 --> 00:23:08,270
But I think that aha moment for me is the fact

621
00:23:08,270 --> 00:23:10,310
你其實可以把它用在日常瑣事上

621
00:23:08,270 --> 00:23:10,310
that you can use it for like day to day tasks

622
00:23:10,310 --> 00:23:12,410
而不只是用在教育研究上。

622
00:23:10,310 --> 00:23:12,410
and less so for like educational research.

623
00:23:12,630 --> 00:23:14,430
我不能就這樣放過這個話題。

623
00:23:12,630 --> 00:23:14,430
I can't let this drop.

624
00:23:14,430 --> 00:23:16,070
你剛剛提到同人小說。

624
00:23:14,430 --> 00:23:16,070
You mentioned fanfiction.

625
00:23:16,850 --> 00:23:17,610
是哪一種同人小說？

625
00:23:16,850 --> 00:23:17,610
What fanfiction?

626
00:23:20,010 --> 00:23:21,530
我先補充一下背景

626
00:23:20,010 --> 00:23:21,530
So just to give context

627
00:23:21,730 --> 00:23:23,010
我覺得我年輕的時候

627
00:23:21,730 --> 00:23:23,010
I feel like when I was younger

628
00:23:23,070 --> 00:23:23,630
就跟大多數人一樣。

628
00:23:23,070 --> 00:23:23,630
as most people do.

629
00:23:23,630 --> 00:23:24,070
這沒有什麼好評價的。

629
00:23:23,630 --> 00:23:24,070
There's no judgment.

630
00:23:24,290 --> 00:23:24,950
你不用特別了解背景。

630
00:23:24,290 --> 00:23:24,950
You don't have to context.

631
00:23:25,070 --> 00:23:25,950
這裡沒有人會評斷你。

631
00:23:25,070 --> 00:23:25,950
There's no judgment here.

632
00:23:26,010 --> 00:23:26,130
謝謝。

632
00:23:26,010 --> 00:23:26,130
Thank you.

633
00:23:26,230 --> 00:23:27,310
這感覺很像 ChatGPT。

633
00:23:26,230 --> 00:23:27,310
This feels like ChachiBT.

634
00:23:27,930 --> 00:23:29,050
但別跟他這麼說。

634
00:23:27,930 --> 00:23:29,050
But don't say that to him.

635
00:23:30,210 --> 00:23:31,510
我懂你的意思。

635
00:23:30,210 --> 00:23:31,510
I see what you're saying.

636
00:23:31,610 --> 00:23:32,270
太棒了。

636
00:23:31,610 --> 00:23:32,270
That's great.

637
00:23:34,790 --> 00:23:35,770
嗯，

637
00:23:34,790 --> 00:23:35,770
But so yeah,

638
00:23:35,790 --> 00:23:37,710
我覺得有很多不同類型的

638
00:23:35,790 --> 00:23:37,710
so I think there's like a lot of different kind

639
00:23:37,710 --> 00:23:40,250
很誇張的同人小說正在流行。

639
00:23:37,710 --> 00:23:40,250
of like outlandish like fanfictions like kind of going on.

640
00:23:40,350 --> 00:23:42,390
我覺得其中一個讓我印象深刻的社群

640
00:23:40,350 --> 00:23:42,390
And I think one of the communities that I thought

641
00:23:42,390 --> 00:23:45,290
就是史瑞克同人小說社群，真的蠻有趣的。

641
00:23:42,390 --> 00:23:45,290
was like pretty funny was like the Shrek fanfiction community.

642
00:23:48,070 --> 00:23:48,430
不好意思。

642
00:23:48,070 --> 00:23:48,430
Sorry.

643
00:23:48,950 --> 00:23:49,430
真的很抱歉。

643
00:23:48,950 --> 00:23:49,430
I'm sorry.

644
00:23:49,510 --> 00:23:51,710
我剛剛以為你說的是史瑞克同人小說社群。

644
00:23:49,510 --> 00:23:51,710
I thought you said the Shrek fanfiction community.

645
00:23:51,830 --> 00:23:52,690
你可以再說一次嗎？

645
00:23:51,830 --> 00:23:52,690
Could you say this again?

646
00:23:53,290 --> 00:23:55,250
沒錯，就是史瑞克同人社群。

646
00:23:53,290 --> 00:23:55,250
Yes, the Shrek fanfiction community.

647
00:23:55,330 --> 00:23:56,610
好，所以你真的有說過那句話。

647
00:23:55,330 --> 00:23:56,610
Okay, so you did say that.

648
00:23:57,250 --> 00:23:57,430
對，

648
00:23:57,250 --> 00:23:57,430
Yeah,

649
00:23:57,530 --> 00:23:59,430
所以我也不太確定為什麼那時會這樣想，

649
00:23:57,530 --> 00:23:59,430
and so I think I don't know why that was like

650
00:23:59,430 --> 00:24:02,210
那就是我腦海中第一個想要引導的套路。

650
00:23:59,430 --> 00:24:02,210
the top of mind kind of trope that I wanted to prompt.

651
00:24:02,810 --> 00:24:04,310
而且那時候真的還很早期。

651
00:24:02,810 --> 00:24:04,310
And back then it was like super early.

652
00:24:04,430 --> 00:24:06,090
所以產出的內容品質也沒那麼好。

652
00:24:04,430 --> 00:24:06,090
So it wasn't like the outputs weren't as good.

653
00:24:06,310 --> 00:24:06,490
沒錯。

653
00:24:06,310 --> 00:24:06,490
Right.

654
00:24:06,670 --> 00:24:08,810
但我覺得這絕對有機會，

654
00:24:06,670 --> 00:24:08,810
But I feel like this could definitely, you know

655
00:24:08,870 --> 00:24:11,630
成為那些大家常常寫同人小說的平台之一。

655
00:24:08,870 --> 00:24:11,630
be on one of those platforms that they write fanfiction all the time.

656
00:24:12,330 --> 00:24:15,110
這很有趣，因為其中一部分原因是，

656
00:24:12,330 --> 00:24:15,110
It's interesting because like part of it is, you know

657
00:24:15,170 --> 00:24:20,710
同人創作的價值之一，就是讓粉絲能夠參與其中，

657
00:24:15,170 --> 00:24:20,710
part of the value of fanfiction is it's a way for fans to participate

658
00:24:20,710 --> 00:24:22,870
而且他們可以創作出很酷的東西，

658
00:24:20,710 --> 00:24:22,870
and they can produce something that's cool

659
00:24:22,870 --> 00:24:24,110
這些東西可能會吸引你，然後

659
00:24:22,870 --> 00:24:24,110
that can kind of appeal to you and

660
00:24:24,110 --> 00:24:25,810
你拿給朋友看，他們會一臉驚訝地說：什麼？

660
00:24:24,110 --> 00:24:25,810
you showed your friends and they were like, what?

661
00:24:26,450 --> 00:24:27,650
他們會覺得，這很蠢。

661
00:24:26,450 --> 00:24:27,650
They're like, this is stupid.

662
00:24:28,290 --> 00:24:29,970
但對你來說，這卻成就了一個很棒的時刻。

662
00:24:28,290 --> 00:24:29,970
But for you, it created a neat moment.

663
00:24:30,090 --> 00:24:31,530
我覺得這是我們很常忽略的事情。

663
00:24:30,090 --> 00:24:31,530
I think that's a lot of things we forget.

664
00:24:31,590 --> 00:24:33,210
像我自己會寫作，而且我很熱愛寫作。

664
00:24:31,590 --> 00:24:33,210
Like I write and I love to write.

665
00:24:33,390 --> 00:24:35,430
對我來說，寫作的體驗本身就是重點。

665
00:24:33,390 --> 00:24:35,430
Part of the writing experience for me though is the writing.

666
00:24:35,530 --> 00:24:36,210
有人會說，喔，

666
00:24:35,530 --> 00:24:36,210
I have people like, oh

667
00:24:36,410 --> 00:24:37,530
你在Open Air工作過。

667
00:24:36,410 --> 00:24:37,530
you worked at Open Air.

668
00:24:37,610 --> 00:24:38,530
你可以直接用ChatGPT就好啦。

668
00:24:37,610 --> 00:24:38,530
You can just chat GPT.

669
00:24:38,650 --> 00:24:40,190
這就像是在問某人，

669
00:24:38,650 --> 00:24:40,190
It's like that's like asking somebody

670
00:24:40,190 --> 00:24:41,430
為什麼要學彈電吉他，

670
00:24:40,190 --> 00:24:41,430
to learn to play the electric guitar

671
00:24:41,430 --> 00:24:43,050
還是你只是想一直聽電吉他音樂，

671
00:24:41,430 --> 00:24:43,050
or are you just going to have, you know,

672
00:24:43,610 --> 00:24:45,030
而不是真的去彈電吉他，

672
00:24:43,610 --> 00:24:45,030
play electric guitar music all the

673
00:24:45,030 --> 00:24:46,350
而不是親自去彈吉他？

673
00:24:45,030 --> 00:24:46,350
time instead of actually playing the guitar?

674
00:24:46,510 --> 00:24:50,030
不是啊，我學寫作就是為了親自去創作、去做這些事情。

674
00:24:46,510 --> 00:24:50,030
Like no, I learned to write to write to do these things.

675
00:24:50,150 --> 00:24:51,210
我覺得這一點常常被忽略。

675
00:24:50,150 --> 00:24:51,210
I think that kind of gets overlooked

676
00:24:51,490 --> 00:24:54,070
但在那個時候，你就可以說：欸，那這種情況怎麼辦？

676
00:24:51,490 --> 00:24:54,070
but that where you can say, hey, what about this scenario?

677
00:24:54,290 --> 00:24:57,050
而《史瑞克》的同人小說大概是最單純又可愛的東西了。

677
00:24:54,290 --> 00:24:57,050
And Shrek fanfiction is like the most sweet and innocent thing.

678
00:24:57,150 --> 00:24:58,690
我想我可能有聽過。

678
00:24:57,150 --> 00:24:58,690
I think I possibly could have heard.

679
00:25:00,650 --> 00:25:03,010
那你覺得你的教授們是怎麼調適的？

679
00:25:00,650 --> 00:25:03,010
So how would you say your professors have been adapting?

680
00:25:03,710 --> 00:25:06,430
我覺得就實際作業來說，

680
00:25:03,710 --> 00:25:06,430
I think in terms of like actual assignments work

681
00:25:06,690 --> 00:25:08,890
我覺得我現在的感受

681
00:25:06,690 --> 00:25:08,890
I feel like the way I'm feeling about

682
00:25:08,890 --> 00:25:10,490
跟我以前的感覺很像，

682
00:25:08,890 --> 00:25:10,490
it is like similar to like how I felt

683
00:25:10,490 --> 00:25:12,830
就像我們小學時期那樣，

683
00:25:10,490 --> 00:25:12,830
when like when we were younger in like elementary school

684
00:25:12,910 --> 00:25:15,210
我們正在經歷從

684
00:25:12,910 --> 00:25:15,210
we're transitioning between like doing

685
00:25:15,210 --> 00:25:17,030
手算除法轉換到使用

685
00:25:15,210 --> 00:25:17,030
like hand division to like using

686
00:25:17,030 --> 00:25:19,230
計算機來處理更複雜的題目，

686
00:25:17,030 --> 00:25:19,230
calculators to like take on like bigger tasks,

687
00:25:19,250 --> 00:25:19,750
就像你說的那樣。

687
00:25:19,250 --> 00:25:19,750
like you said.

688
00:25:20,110 --> 00:25:22,230
所以我覺得現在就是那種過渡期，

688
00:25:20,110 --> 00:25:22,230
So I feel like it's like that kind of transitional period

689
00:25:22,230 --> 00:25:24,590
我覺得很多時候，

689
00:25:22,230 --> 00:25:24,590
where it's like I think a lot of

690
00:25:24,590 --> 00:25:27,950
自動化的任務變得比較少了。

690
00:25:24,590 --> 00:25:27,950
a lot less like like automation like automotive tasks.

691
00:25:28,090 --> 00:25:30,130
我覺得在我的溝通課程裡

691
00:25:28,090 --> 00:25:30,130
So like I feel like in my communication courses

692
00:25:30,130 --> 00:25:32,710
我發現像是那種定義式的問題變少了

692
00:25:30,130 --> 00:25:32,710
like I'm seeing a lot less like at like as

693
00:25:32,710 --> 00:25:36,490
這種針對術語下定義的問題越來越少，反而更多是

693
00:25:32,710 --> 00:25:36,490
defined this like term kind of question and more about like

694
00:25:36,550 --> 00:25:37,830
要怎麼實際應用這個術語？

694
00:25:36,550 --> 00:25:37,830
how do you apply this term?

695
00:25:37,910 --> 00:25:39,710
這個在更大的脈絡下代表什麼意思？

695
00:25:37,910 --> 00:25:39,710
What does this mean in like a bigger context?

696
00:25:39,710 --> 00:25:42,770
所以我覺得課程更強調意義

696
00:25:39,710 --> 00:25:42,770
So I feel like there's a lot more focus on like meaning

697
00:25:42,770 --> 00:25:45,490
還有背後的意圖，而不是

697
00:25:42,770 --> 00:25:45,490
and like intentionality as opposed to

698
00:25:45,490 --> 00:25:48,030
只是基本的理解而已

698
00:25:45,490 --> 00:25:48,030
kind of more like basic understanding.

699
00:25:49,050 --> 00:25:50,150
而且我覺得，你知道的

699
00:25:49,050 --> 00:25:50,150
And I think, you know

700
00:25:50,210 --> 00:25:52,810
很多考試的形式其實

700
00:25:50,210 --> 00:25:52,810
a lot of tests taking forms are actually like

701
00:25:52,810 --> 00:25:54,830
比我原本想像的還要開放

701
00:25:52,810 --> 00:25:54,830
more open format than I thought they would be.

702
00:25:55,350 --> 00:25:56,570
也因為這樣

702
00:25:55,350 --> 00:25:56,570
And because of that

703
00:25:56,650 --> 00:26:00,270
題目的方向也變成更常問那種，你知道的

703
00:25:56,650 --> 00:26:00,270
the questions have also shifted to ask more of like, you know

704
00:26:00,310 --> 00:26:02,370
可以延伸到更大意義的問題

704
00:26:00,310 --> 00:26:02,370
questions that are like, you know,

705
00:26:02,390 --> 00:26:04,950
而不是只停留在表面的理解

705
00:26:02,390 --> 00:26:04,950
could be extrapolated to like bigger meanings as opposed

706
00:26:04,950 --> 00:26:07,290
就像是那種典型的「請定義這個詞」的題目。

706
00:26:04,950 --> 00:26:07,290
to kind of like the typical like define this term.

707
00:26:07,510 --> 00:26:09,530
所以我覺得這就是我觀察到的改變。

707
00:26:07,510 --> 00:26:09,530
So I feel like that's how I've seen a change.

708
00:26:09,710 --> 00:26:15,510
對，我其實有注意到在我的資工課程裡，有些教授

708
00:26:09,710 --> 00:26:15,510
Yeah, I've actually noticed that in in my CS classes, some professors

709
00:26:16,110 --> 00:26:18,330
很明顯他們不希望我們使用

709
00:26:16,110 --> 00:26:18,330
obviously they want us to not use

710
00:26:18,330 --> 00:26:20,430
AI 來完成這些簡單的作業

710
00:26:18,330 --> 00:26:20,430
it for these like simple homework assignments

711
00:26:20,430 --> 00:26:23,110
這些作業只是用來檢查

711
00:26:20,430 --> 00:26:23,110
that are just like check for check

712
00:26:23,110 --> 00:26:24,590
你是否真的記得那些概念。

712
00:26:23,110 --> 00:26:24,590
to see if you actually remember the concept.

713
00:26:25,110 --> 00:26:26,730
但對於比較大型的專案

713
00:26:25,110 --> 00:26:26,730
But for bigger projects

714
00:26:27,610 --> 00:26:29,570
我們其實有兩種選擇。

714
00:26:27,610 --> 00:26:29,570
we actually got two tracks.

715
00:26:29,650 --> 00:26:31,550
一種是可以用 AI 的

715
00:26:29,650 --> 00:26:31,550
We could take one is with using AI

716
00:26:31,550 --> 00:26:33,090
另一種則是不用 AI 的

716
00:26:31,550 --> 00:26:33,090
and one is without using AI

717
00:26:33,090 --> 00:26:33,990
而且

717
00:26:33,090 --> 00:26:33,990
and

718
00:26:33,990 --> 00:26:36,470
我覺得教授願意這樣做真的很有趣，因為他算是

718
00:26:33,990 --> 00:26:36,470
I thought it was really interesting that the professor is kind of

719
00:26:37,030 --> 00:26:40,730
接受這個新概念並且去適應它。

719
00:26:37,030 --> 00:26:40,730
you this this new concept and and adapting to it.

720
00:26:40,850 --> 00:26:42,330
而且教授說如果用 AI 的話

720
00:26:40,850 --> 00:26:42,330
And the professor said with AI

721
00:26:42,710 --> 00:26:44,930
我們要給你一個稍微更有挑戰性的作業

721
00:26:42,710 --> 00:26:44,930
we're going to give you a bit of a harder assignment

722
00:26:44,930 --> 00:26:46,450
而且我們會再多給你一點壓力

722
00:26:44,930 --> 00:26:46,450
and we're going to push you more

723
00:26:46,450 --> 00:26:49,290
你還需要寫一篇心得，說說 AI 帶給你的啟發

723
00:26:46,450 --> 00:26:49,290
and you're going to have to write a reflection on what AI gave you so

724
00:26:49,290 --> 00:26:52,790
這樣你也能掌握 AI 想傳達給你的概念

724
00:26:49,290 --> 00:26:52,790
that you also get the concepts that you would that AI would tell you.

725
00:26:53,290 --> 00:26:56,690
但如果不用 AI，你就會做比較傳統的專案

725
00:26:53,290 --> 00:26:56,690
But with the non AI you would kind of do a more traditional project

726
00:26:56,690 --> 00:26:58,750
但如果用 AI，你可以把專案做得更深入、更有突破

726
00:26:56,690 --> 00:26:58,750
but with AI you could like push it much further.

727
00:26:59,270 --> 00:27:00,350
所以這真的很棒

727
00:26:59,270 --> 00:27:00,350
So it was great to

728
00:27:00,350 --> 00:27:03,130
教授讓你們可以在這兩種方式中選擇

728
00:27:00,350 --> 00:27:03,130
that the professor was allowing you to choose between the two.

729
00:27:03,470 --> 00:27:04,130
你最後選了哪一個？

729
00:27:03,470 --> 00:27:04,130
Which did you choose?

730
00:27:04,130 --> 00:27:05,350
我選了不用 AI 的那個

730
00:27:04,130 --> 00:27:05,350
I chose non AI.

731
00:27:05,730 --> 00:27:07,270
好，嗯，對啊

731
00:27:05,730 --> 00:27:07,270
Okay, but um, yeah

732
00:27:07,310 --> 00:27:08,850
我知道很多人都選了用 AI 的那個

732
00:27:07,310 --> 00:27:08,850
I know a lot of people who did choose AI.

733
00:27:09,270 --> 00:27:10,250
你為什麼會選不用 AI 的？

733
00:27:09,270 --> 00:27:10,250
Why did you choose non AI?

734
00:27:10,810 --> 00:27:13,130
我覺得對我來說

734
00:27:10,810 --> 00:27:13,130
I think that for me

735
00:27:13,630 --> 00:27:21,370
我本身不是那種技術很強的人

735
00:27:13,630 --> 00:27:21,370
I I wasn't the I wasn't the most like technically

736
00:27:21,370 --> 00:27:24,350
像是與產品概念相契合的聲音

736
00:27:21,370 --> 00:27:24,350
like sound with the concept that the product was about

737
00:27:24,350 --> 00:27:28,210
我記得我自己有幾個想加入的功能

737
00:27:24,350 --> 00:27:28,210
and I remember that I had a few features that I

738
00:27:28,210 --> 00:27:31,430
自己想在這種小遊戲裡實現

738
00:27:28,210 --> 00:27:31,430
myself wanted to implement in this in this kind of mini game

739
00:27:31,830 --> 00:27:35,690
但有了 AI，你可以請它幫你發想點子

739
00:27:31,830 --> 00:27:35,690
but with AI it was like you can ask it to brainstorm

740
00:27:35,690 --> 00:27:36,790
還能協助你產生功能想法

740
00:27:35,690 --> 00:27:36,790
and help you get features

741
00:27:36,790 --> 00:27:38,390
你也可以用 AI 來實現這些功能

741
00:27:36,790 --> 00:27:38,390
and you can implement those with AI

742
00:27:38,610 --> 00:27:40,530
但我其實已經知道自己想做什麼

742
00:27:38,610 --> 00:27:40,530
but I kind of knew what I wanted to do

743
00:27:40,530 --> 00:27:44,270
一開始就已經有自己的想法了

743
00:27:40,530 --> 00:27:44,270
from the get-go and because I already had my ideas.

744
00:27:44,410 --> 00:27:45,910
我很常用 AI 來發想點子

744
00:27:44,410 --> 00:27:45,910
I use AI a lot for brainstorming

745
00:27:46,010 --> 00:27:47,790
但因為我早就有想法了，所以我就覺得

745
00:27:46,010 --> 00:27:47,790
but because I already had my ideas I was like

746
00:27:48,190 --> 00:27:50,390
那我乾脆專注

746
00:27:48,190 --> 00:27:50,390
oh I might as well just really just like focus

747
00:27:50,390 --> 00:27:52,510
在實現我已經想到的點子上

747
00:27:50,390 --> 00:27:52,510
on implementing the ideas that I had already come up with.

748
00:27:53,410 --> 00:27:54,490
這種情況很常見

748
00:27:53,410 --> 00:27:54,490
That's a it's a thing

749
00:27:54,490 --> 00:27:55,790
經常會遇到，因為我覺得有些

749
00:27:54,490 --> 00:27:55,790
comes up a lot because I think some

750
00:27:55,790 --> 00:27:57,770
有些教授希望學生學好基礎知識

750
00:27:55,790 --> 00:27:57,770
professors they want the students to learn the fundamentals

751
00:27:57,770 --> 00:27:59,470
這很重要，因為如果你學會了 Python，

751
00:27:57,770 --> 00:27:59,470
which is important because if you learn Python

752
00:27:59,570 --> 00:28:00,910
你真的需要學會 Python。

752
00:27:59,570 --> 00:28:00,910
you really need to learn Python

753
00:28:00,910 --> 00:28:03,670
我認為這在未來幾十年都會很有幫助。

753
00:28:00,910 --> 00:28:03,670
and I think it's going to be helpful for even for decades to come.

754
00:28:03,810 --> 00:28:05,690
我們需要有懂得這些運作原理的人。

754
00:28:03,810 --> 00:28:05,690
We need to have people who understand how these things work

755
00:28:05,870 --> 00:28:08,330
但我有看過一些課程，

755
00:28:05,870 --> 00:28:08,330
but I've seen some programs

756
00:28:08,330 --> 00:28:09,510
他們甚至沒有教學生如何

756
00:28:08,330 --> 00:28:09,510
where they're not even teaching them how

757
00:28:09,510 --> 00:28:11,190
利用 AI 來產生程式碼，

757
00:28:09,510 --> 00:28:11,190
to use AI for like code generation

758
00:28:11,370 --> 00:28:14,410
在我看來，這根本就是教學失職，你懂吧？

758
00:28:11,370 --> 00:28:14,410
which to me sounds like malpractice, you know

759
00:28:14,450 --> 00:28:16,550
當然，這只是我個人的看法。

759
00:28:14,450 --> 00:28:16,550
which again just that's my take on that.

760
00:28:16,670 --> 00:28:17,930
但我覺得這個想法很不錯，

760
00:28:16,670 --> 00:28:17,930
But I think that I like the idea

761
00:28:17,930 --> 00:28:20,130
就是有不同的課程可以選擇，

761
00:28:17,930 --> 00:28:20,130
that there's a different courses

762
00:28:20,130 --> 00:28:22,130
還有如果你打算使用

762
00:28:20,130 --> 00:28:22,130
and the idea that if you're going to use

763
00:28:22,130 --> 00:28:23,990
AI，那很好，你就必須接受更大的挑戰。

763
00:28:22,130 --> 00:28:23,990
AI then great you have to do bigger challenges

764
00:28:23,990 --> 00:28:25,650
也有人問過我這個問題。

764
00:28:23,990 --> 00:28:25,650
and I've had people ask me about this.

765
00:28:25,710 --> 00:28:27,130
如果大家都能用 ChatGPT，那我們該怎麼辦？

765
00:28:25,710 --> 00:28:27,130
What do we do people can use chat GPT?

766
00:28:27,130 --> 00:28:29,030
我會讓學生做更大型的專題計畫

766
00:28:27,130 --> 00:28:29,030
I'm like have students do bigger projects

767
00:28:29,210 --> 00:28:30,390
你知道，以前他們只會做讀書報告

767
00:28:29,210 --> 00:28:30,390
you know where they would have done a book report

768
00:28:30,390 --> 00:28:31,750
現在我會讓他們拍攝音樂影片，

768
00:28:30,390 --> 00:28:31,750
before have I make a music video,

769
00:28:31,990 --> 00:28:34,010
就是讓他們做更有規模的作品，做更多不同的嘗試。

769
00:28:31,990 --> 00:28:34,010
you know, just make bigger things and do more.

770
00:28:34,690 --> 00:28:36,150
我覺得這些擔憂是有道理的

770
00:28:34,690 --> 00:28:36,150
I think they're valid concerns

771
00:28:36,150 --> 00:28:37,530
當大家面對某些事物時，

771
00:28:36,150 --> 00:28:37,530
when people are looking at something that's

772
00:28:37,530 --> 00:28:38,930
這些事物會改變我們的工作方式，

772
00:28:37,530 --> 00:28:38,930
going to change the way that we work

773
00:28:38,930 --> 00:28:40,030
大家都在試著了解最後會有什麼結果。

773
00:28:38,930 --> 00:28:40,030
and trying to figure out the outcome.

774
00:28:40,030 --> 00:28:42,150
所以我能理解大家的猶豫，

774
00:28:40,030 --> 00:28:42,150
And so I understand some of the hesitation there

775
00:28:42,150 --> 00:28:44,810
我也認為批判性思考能力非常重要，

775
00:28:42,150 --> 00:28:44,810
and I do think that critical thinking skills are important

776
00:28:44,810 --> 00:28:46,970
我覺得這項技術也正努力去配合這一點，

776
00:28:44,810 --> 00:28:46,970
and I think this technology is trying to adapt to that

777
00:28:46,970 --> 00:28:48,610
因為如果它只是另一種

777
00:28:46,970 --> 00:28:48,610
because if it's just another thing that

778
00:28:48,610 --> 00:28:50,170
讓大家只會複製貼上的工具，

778
00:28:48,610 --> 00:28:50,170
people copy paste from them like yeah

779
00:28:50,210 --> 00:28:51,030
那其實學不到東西。

779
00:28:50,210 --> 00:28:51,030
you're not going to learn.

780
00:28:51,550 --> 00:28:54,750
所以要睜大眼睛，關注新功能，深入研究 ChatGPT。

780
00:28:51,550 --> 00:28:54,750
So opening eyes rolling out a new feature study the chat GPT

781
00:28:55,170 --> 00:28:57,230
這似乎是一種非常有趣的做法。

781
00:28:55,170 --> 00:28:57,230
which seems to be kind of very interesting approach with that.

782
00:28:57,330 --> 00:28:58,210
你們有試過這個嗎？

782
00:28:57,330 --> 00:28:58,210
Have you guys tried this?

783
00:28:58,770 --> 00:29:00,450
那你們會怎麼形容它？

783
00:28:58,770 --> 00:29:00,450
Yeah, what's your how would you describe it?

784
00:29:01,530 --> 00:29:05,670
我會說學習模式其實就是一種對話。

784
00:29:01,530 --> 00:29:05,670
I would say that study mode is is a chat.

785
00:29:05,830 --> 00:29:09,190
它很有挑戰性，會讓使用者

785
00:29:05,830 --> 00:29:09,190
It's challenging and it forces forces the user

786
00:29:09,190 --> 00:29:11,590
真的去挑戰自己對各種內容的理解。

786
00:29:09,190 --> 00:29:11,590
to actually challenge themselves with any sort of content.

787
00:29:12,130 --> 00:29:14,050
像我自己在嘗試時，

787
00:29:12,130 --> 00:29:14,050
So I myself when I tried it

788
00:29:14,110 --> 00:29:16,250
我請它教我人工智慧相關的知識，

788
00:29:14,110 --> 00:29:16,250
I I asked it to teach me about AI

789
00:29:16,250 --> 00:29:19,970
我也用一般聊天模式試過，

789
00:29:16,250 --> 00:29:19,970
and I also tried this with a regular chat mode

790
00:29:19,970 --> 00:29:22,470
它只是給我一大堆關於人工智慧的清單，

790
00:29:19,970 --> 00:29:22,470
and it just gave me a big list of oh AI

791
00:29:22,470 --> 00:29:25,010
說有這種學習方式、那種學習方式，

791
00:29:22,470 --> 00:29:25,010
is this there's this type of learning this type of learning

792
00:29:25,250 --> 00:29:27,830
但當我用學習模式時，

792
00:29:25,250 --> 00:29:27,830
and when I asked study mode.

793
00:29:28,630 --> 00:29:30,810
它其實沒有直接回答我的問題，

793
00:29:28,630 --> 00:29:30,810
It actually just didn't even answer my question

794
00:29:30,810 --> 00:29:34,210
反而丟給我三個問題，想了解我真正想要的是什麼，

794
00:29:30,810 --> 00:29:34,210
and gave me three questions to understand what I really wanted

795
00:29:34,210 --> 00:29:36,550
還問我有沒有特定想了解的主題？

795
00:29:34,210 --> 00:29:36,550
and it said do you have a specific topic that you

796
00:29:36,550 --> 00:29:39,010
我真的很想深入了解你對這個有多少認識？

796
00:29:36,550 --> 00:29:39,010
really want to get into how much do you know about this?

797
00:29:39,130 --> 00:29:40,070
你現在正在做什麼？

797
00:29:39,130 --> 00:29:40,070
What are you doing right now?

798
00:29:40,570 --> 00:29:45,050
然後我把話題聚焦到一個特定主題，

798
00:29:40,570 --> 00:29:45,050
And I got it to a specific topic which is

799
00:29:45,050 --> 00:29:48,510
就是微調，它真的一項一項地詳細說明，

799
00:29:45,050 --> 00:29:48,510
fine-tuning and it really broke it down one by one

800
00:29:48,510 --> 00:29:51,510
我覺得學習模式最重要的一點是，

800
00:29:48,510 --> 00:29:51,510
and I think the biggest thing about study mode is

801
00:29:51,510 --> 00:29:53,010
它不會只是單純假設，

801
00:29:51,510 --> 00:29:53,010
that it doesn't just assume

802
00:29:53,010 --> 00:29:56,350
你已經記住了，然後幾分鐘後就跳過。

802
00:29:53,010 --> 00:29:56,350
that you remember it and move on maybe after a few minutes.

803
00:29:56,350 --> 00:29:57,790
它會回頭提醒你，喔，

803
00:29:56,350 --> 00:29:57,790
It'll go back and say, oh

804
00:29:57,930 --> 00:29:59,970
這裡有個小小的自我檢查，確認一下你是否理解。

804
00:29:57,930 --> 00:29:59,970
here's a little mental check a little sanity check.
