1
00:00:00,000 --> 00:00:01,620
檢查一下，你還記得這個嗎？

1
00:00:00,000 --> 00:00:01,620
Check Do you remember this?

2
00:00:01,620 --> 00:00:03,340
你還得再回答一次。

2
00:00:01,620 --> 00:00:03,340
And you'll have to answer that again.

3
00:00:03,340 --> 00:00:04,660
因為在我們的大腦裡

3
00:00:03,340 --> 00:00:04,660
Because in our brains

4
00:00:04,660 --> 00:00:06,600
真正形成那些連結的就是這個，

4
00:00:04,660 --> 00:00:06,600
that's what's really like forming those connections,

5
00:00:06,600 --> 00:00:07,880
我們的神經連結，

5
00:00:06,600 --> 00:00:07,880
our neural connections

6
00:00:07,880 --> 00:00:10,380
實際上幫助我們記住這些概念。

6
00:00:07,880 --> 00:00:10,380
and actually helping us remember these concepts.

7
00:00:10,380 --> 00:00:12,340
我認為學習模式真的

7
00:00:10,380 --> 00:00:12,340
And I think that study mode does a really

8
00:00:12,340 --> 00:00:14,440
非常適合你想要真正學會一個概念，

8
00:00:12,340 --> 00:00:14,440
great job if you really want to learn a concept

9
00:00:14,440 --> 00:00:18,400
並且應用該概念，充分理解它的全部潛力。

9
00:00:14,440 --> 00:00:18,400
and apply that concept and understand it to its fullest potential.

10
00:00:18,400 --> 00:00:20,360
這時候你就會用它，

10
00:00:18,400 --> 00:00:20,360
That's when you would use it

11
00:00:20,360 --> 00:00:22,040
而不是只想要一個問題的答案。

11
00:00:20,360 --> 00:00:22,040
when you don't want just a question answer.

12
00:00:22,040 --> 00:00:22,740
是的，

12
00:00:22,040 --> 00:00:22,740
Yeah,

13
00:00:22,740 --> 00:00:25,080
我也做過類似的

13
00:00:22,740 --> 00:00:25,080
I've also kind of done like a similar like

14
00:00:25,080 --> 00:00:28,440
並排比較學習模式和一般模式，

14
00:00:25,080 --> 00:00:28,440
side-by-side comparison with study mode versus like the regular mode

15
00:00:28,440 --> 00:00:29,740
我覺得非常有分析性。

15
00:00:28,440 --> 00:00:29,740
and I think very analytical.

16
00:00:29,740 --> 00:00:30,540
我很喜歡這個。

16
00:00:29,740 --> 00:00:30,540
I love this.

17
00:00:30,540 --> 00:00:34,020
我玩得像是，喔

17
00:00:30,540 --> 00:00:34,020
I played like, Oh

18
00:00:34,020 --> 00:00:34,700
這真的很酷。

18
00:00:34,020 --> 00:00:34,700
this is pretty cool.

19
00:00:34,700 --> 00:00:39,160
我想當時我正在

19
00:00:34,700 --> 00:00:39,160
And I think like when I so I was

20
00:00:39,160 --> 00:00:40,580
我想我當時是在嘗試。

20
00:00:39,160 --> 00:00:40,580
I think I was like trying to.

21
00:00:40,580 --> 00:00:43,900
我在做一個非常小眾主題的研究，

21
00:00:40,580 --> 00:00:43,900
I was doing like a research on a very like niche topic,

22
00:00:43,900 --> 00:00:47,160
關於電子舞曲和狂歡文化的。

22
00:00:43,900 --> 00:00:47,160
kind of about like EDM and rave culture.

23
00:00:47,160 --> 00:00:49,060
還有它在加州的歷史。

23
00:00:47,160 --> 00:00:49,060
And it's like history in like California.

24
00:00:49,060 --> 00:00:49,600
我當時想，

24
00:00:49,060 --> 00:00:49,600
And I was like

25
00:00:49,600 --> 00:00:51,460
我覺得這是一個非常有趣的主題，

25
00:00:49,600 --> 00:00:51,460
I feel like that's like a really interesting topic

26
00:00:51,460 --> 00:00:54,600
可以研究看看，了解一下。

26
00:00:51,460 --> 00:00:54,600
to kind of do research on and see what.

27
00:00:54,600 --> 00:00:59,400
你知道的，看看ChatGPT怎麼說，我記得，嗯

27
00:00:54,600 --> 00:00:59,400
Like, you know, chat GBT has to say about it, and I remember, well

28
00:00:59,400 --> 00:01:03,320
我想我用ChatGPT做研究的方式是，

28
00:00:59,400 --> 00:01:03,320
I guess my usage of like kind of chat GBT for like research is.

29
00:01:03,320 --> 00:01:04,540
我覺得我總是

29
00:01:03,320 --> 00:01:04,540
I feel like I always

30
00:01:04,540 --> 00:01:05,380
心裡一直有個念頭

30
00:01:04,540 --> 00:01:05,380
in the back of my mind

31
00:01:05,380 --> 00:01:06,900
承認它就像一個大型語言模型。

31
00:01:05,380 --> 00:01:06,900
acknowledging that it's like an LLM.

32
00:01:06,900 --> 00:01:10,580
不太適合用於研究，除非

32
00:01:06,900 --> 00:01:10,580
Not very like suited for like research unless

33
00:01:10,580 --> 00:01:12,400
你對它有良好的參數設定。

33
00:01:10,580 --> 00:01:12,400
you kind of have good parameters around it.

34
00:01:12,400 --> 00:01:14,200
所以我通常的做法是

34
00:01:12,400 --> 00:01:14,200
So usually the way I approach it is

35
00:01:14,200 --> 00:01:15,960
先瀏覽我找到的資料來源，

35
00:01:14,200 --> 00:01:15,960
that I go through my sources that I find,

36
00:01:15,960 --> 00:01:17,600
像是透過Google，找研究論文。

36
00:01:15,960 --> 00:01:17,600
like through Google, like research papers.

37
00:01:17,600 --> 00:01:19,220
然後我會把它貼到聊天視窗裡，

37
00:01:17,600 --> 00:01:19,220
And I actually paste that into the chat

38
00:01:19,220 --> 00:01:21,520
並要求它只從那些參數中提取資訊。

38
00:01:19,220 --> 00:01:21,520
and ask it to only draw from those parameters.

39
00:01:21,520 --> 00:01:24,440
因為我想讓內容更具體，不那麼泛泛而談。

39
00:01:21,520 --> 00:01:24,440
Because I want to keep it like, more contained and less like general.

40
00:01:24,440 --> 00:01:26,480
所以我覺得

40
00:01:24,440 --> 00:01:26,480
And so I feel like

41
00:01:26,480 --> 00:01:29,300
當我在做那種研究，像是探討

41
00:01:26,480 --> 00:01:29,300
when I was doing that research into like, kind of looking into like

42
00:01:29,300 --> 00:01:31,200
加州的電子舞曲文化時，我發現。

42
00:01:29,300 --> 00:01:31,200
rave culture in California, I found that.

43
00:01:31,200 --> 00:01:34,260
當我使用一般模式並輸入相關背景時，

43
00:01:31,200 --> 00:01:34,260
Like, when I use the regular mode and put in the context

44
00:01:34,260 --> 00:01:35,900
它確實產生了不錯的結果，

44
00:01:34,260 --> 00:01:35,900
it did produce a good output

45
00:01:35,900 --> 00:01:36,660
但使用穩定模式時

45
00:01:35,900 --> 00:01:36,660
but with steady mode

46
00:01:36,660 --> 00:01:38,540
我其實根本不需要用到那個上下文

46
00:01:36,660 --> 00:01:38,540
I didn't even really need to use that context

47
00:01:38,540 --> 00:01:42,080
因為它把範圍縮小到那種來回對話的參數。

47
00:01:38,540 --> 00:01:42,080
because it narrowed the parameters to that back-and-forth chat.

48
00:01:42,080 --> 00:01:45,400
我覺得我的學習在那裡變得更嚴謹了。

48
00:01:42,080 --> 00:01:45,400
And I feel like my learning was more kind of rigorous through there.

49
00:01:45,400 --> 00:01:47,720
因為我們是在回答問題，

49
00:01:45,400 --> 00:01:47,720
Because it's like we're answering questions instead

50
00:01:47,720 --> 00:01:48,760
而不是被灌輸，

50
00:01:47,720 --> 00:01:48,760
of being like Fed,

51
00:01:48,760 --> 00:01:50,680
就像是一些內容片段，

51
00:01:48,760 --> 00:01:50,680
kind of like pieces of content

52
00:01:50,680 --> 00:01:52,280
是一般模式提供的資訊。

52
00:01:50,680 --> 00:01:52,280
information that the regular mode gave.

53
00:01:52,280 --> 00:01:55,440
所以我覺得那是一種非常有效的學習方法。

53
00:01:52,280 --> 00:01:55,440
So I thought that was like a really effective kind of learning method.

54
00:01:56,440 --> 00:01:58,920
這是否讓你對未來更有信心，

54
00:01:56,440 --> 00:01:58,920
Has this given you more confidence about the

55
00:01:58,920 --> 00:02:00,980
在尋找職位方面，

55
00:01:58,920 --> 00:02:00,980
future as far as you're finding a role

56
00:02:00,980 --> 00:02:04,100
以及能否適應接下來的變化？

56
00:02:00,980 --> 00:02:04,100
and being able to adapt to whatever is going to come next or not?

57
00:02:04,660 --> 00:02:05,680
我覺得，絕對是的，

57
00:02:04,660 --> 00:02:05,680
Definitely, I think,

58
00:02:05,680 --> 00:02:09,260
我認為OpenAI所產出的東西有兩面向。

58
00:02:05,680 --> 00:02:09,260
I think there's two sides to to what OpenAI is producing.

59
00:02:09,260 --> 00:02:11,700
我覺得這個代理模式，

59
00:02:09,260 --> 00:02:11,700
I think with this like agent mode there

60
00:02:11,700 --> 00:02:15,280
有一種超未來感，讓你會想，

60
00:02:11,700 --> 00:02:15,280
there is that like super futuristic look where you think,

61
00:02:15,280 --> 00:02:17,080
喔，像是，可能有些，像是

61
00:02:15,280 --> 00:02:17,080
oh, like, maybe some, like

62
00:02:17,080 --> 00:02:19,220
我以前實習時做過的某些任務

62
00:02:17,080 --> 00:02:19,220
certain tasks that I used to be doing for my internship

63
00:02:19,220 --> 00:02:20,680
他們甚至不會想讓我做。

63
00:02:19,220 --> 00:02:20,680
they won't even want me to do.

64
00:02:20,680 --> 00:02:21,880
但使用學習模式時

64
00:02:20,680 --> 00:02:21,880
But then with study mode

65
00:02:21,880 --> 00:02:25,320
就像是回頭思考，像是

65
00:02:21,880 --> 00:02:25,320
it's really like drawing it back and thinking about, like

66
00:02:25,320 --> 00:02:27,140
你在學校學到的每個概念。

66
00:02:25,320 --> 00:02:27,140
each concept that you're learning in school.

67
00:02:27,140 --> 00:02:28,580
所以對我來說

67
00:02:27,140 --> 00:02:28,580
And so, to me

68
00:02:28,580 --> 00:02:32,340
我認為學習模式是 Chat GPT 中最重要的模式之一

68
00:02:28,580 --> 00:02:32,340
I think study mode is one of the most important modes in in Chat GPT

69
00:02:32,340 --> 00:02:35,060
因為它真的能幫助你學習。

69
00:02:32,340 --> 00:02:35,060
because it'll actually help you learn.

70
00:02:35,060 --> 00:02:36,520
而我覺得人們通常是

70
00:02:35,060 --> 00:02:36,520
Whereas I think people are

71
00:02:36,520 --> 00:02:38,140
當他們嘗試使用 AI 時

71
00:02:36,520 --> 00:02:38,140
when they're trying to use AI

72
00:02:38,140 --> 00:02:39,660
他們只是想直接得到答案。

72
00:02:38,140 --> 00:02:39,660
they're trying to just get the answer.

73
00:02:39,660 --> 00:02:43,360
是的，所以，我

73
00:02:39,660 --> 00:02:43,360
Yeah, so yeah, I'm.

74
00:02:43,360 --> 00:02:44,500
我很期待。

74
00:02:43,360 --> 00:02:44,500
I'm excited.

75
00:02:44,500 --> 00:02:46,340
作為一個終身學習者，每次我們有

75
00:02:44,500 --> 00:02:46,340
As a lifelong learner is every time we have

76
00:02:46,340 --> 00:02:48,320
像這樣的新工具，可以不斷地玩弄它

76
00:02:46,340 --> 00:02:48,320
a new tool like this to continuously play with it

77
00:02:48,320 --> 00:02:49,360
然後看看它會發展到哪裡

77
00:02:48,320 --> 00:02:49,360
and then to see where this is heading

78
00:02:49,360 --> 00:02:53,420
我一直在使用 Chat GPT。

78
00:02:49,360 --> 00:02:53,420
I use Chat GPT all the time.

79
00:02:53,420 --> 00:02:54,900
我在這裡坦承這點

79
00:02:53,420 --> 00:02:54,900
I'm gonna admit that here

80
00:02:54,900 --> 00:02:56,720
我不會花太多時間在社群媒體上

80
00:02:54,900 --> 00:02:56,720
I don't spend a lot of time on social media

81
00:02:56,720 --> 00:02:57,720
但我從來不是那種重度社群

81
00:02:56,720 --> 00:02:57,720
but it's never really big social

82
00:02:57,720 --> 00:02:59,320
媒體使用者，除了看新聞之外。

82
00:02:57,720 --> 00:02:59,320
media person other than like for news.

83
00:02:59,320 --> 00:03:01,020
你的經驗是什麼？

83
00:02:59,320 --> 00:03:01,020
What has been your experience?

84
00:03:01,020 --> 00:03:02,420
你會花時間嗎？

84
00:03:01,020 --> 00:03:02,420
Do you spend?

85
00:03:02,420 --> 00:03:02,900
像是，怎麼花？

85
00:03:02,420 --> 00:03:02,900
Like, how?

86
00:03:02,900 --> 00:03:04,380
平常一天是怎麼過的？

86
00:03:02,900 --> 00:03:04,380
is like, what's a normal day for you?

87
00:03:04,380 --> 00:03:05,540
還有，像是

87
00:03:04,380 --> 00:03:05,540
And then also like

88
00:03:05,540 --> 00:03:08,040
假設你用多少時間在社群媒體，跟 Chat GPT 比較起來？

88
00:03:05,540 --> 00:03:08,040
let's say how much social media are using versus chat?

89
00:03:08,040 --> 00:03:08,480
GPT。

89
00:03:08,040 --> 00:03:08,480
GPT.

90
00:03:08,480 --> 00:03:11,000
對我來說，我想是這樣。

90
00:03:08,480 --> 00:03:11,000
For me, I think so.

91
00:03:11,000 --> 00:03:12,820
其實我不是最近才這樣

91
00:03:11,000 --> 00:03:12,820
I actually not recently

92
00:03:12,820 --> 00:03:14,100
大約是一年前，

92
00:03:12,820 --> 00:03:14,100
about like a year ago,

93
00:03:14,100 --> 00:03:18,220
我決定稍微遠離社群媒體的使用。

93
00:03:14,100 --> 00:03:18,220
kind of decided to take a step back from social media usage.

94
00:03:18,220 --> 00:03:20,040
特別是像抖音這類的，

94
00:03:18,220 --> 00:03:20,040
Just particularly like tick-tock

95
00:03:20,040 --> 00:03:24,700
因為我覺得自己每天都在接收

95
00:03:20,040 --> 00:03:24,700
just cuz I feel like I was just like getting

96
00:03:24,700 --> 00:03:28,500
大量集中在同一個地方的內容，

96
00:03:24,700 --> 00:03:28,500
a lot of content in one place in like a day

97
00:03:28,500 --> 00:03:29,660
以一種容易消化的方式，

97
00:03:28,500 --> 00:03:29,660
like a digestible manner

98
00:03:29,660 --> 00:03:31,700
這也是它方便的地方。

98
00:03:29,660 --> 00:03:31,700
which is like what the convenience of it is.

99
00:03:31,700 --> 00:03:34,640
但我覺得我開始過度依賴這種便利性。

99
00:03:31,700 --> 00:03:34,640
But I feel like I started getting too used to that convenience.

100
00:03:34,640 --> 00:03:36,400
而且，我覺得，

100
00:03:34,640 --> 00:03:36,400
And like, like, I feel like.

101
00:03:36,400 --> 00:03:37,200
我只是，

101
00:03:36,400 --> 00:03:37,200
I was just like,

102
00:03:37,200 --> 00:03:39,580
有點被動地消費內容，卻沒有真正

102
00:03:37,200 --> 00:03:39,580
kind of passively consuming content without really

103
00:03:39,580 --> 00:03:41,960
去研究或查證它。

103
00:03:39,580 --> 00:03:41,960
like researching or fact-checking it as much.

104
00:03:41,960 --> 00:03:42,940
我覺得這樣不太好。

104
00:03:41,960 --> 00:03:42,940
I feel like it wasn't.

105
00:03:42,940 --> 00:03:44,560
這不就是抖音的重點嗎，對啊。

105
00:03:42,940 --> 00:03:44,560
That's the whole point of tick-tock, yeah

106
00:03:44,560 --> 00:03:45,220
這就是重點。

106
00:03:44,560 --> 00:03:45,220
that's the whole point.

107
00:03:45,220 --> 00:03:48,120
但我覺得它讓我變得相當自滿。

107
00:03:45,220 --> 00:03:48,120
But like, I feel like I was like it was making me pretty complacent.

108
00:03:48,120 --> 00:03:49,840
我想，是的，我不喜歡那樣。

108
00:03:48,120 --> 00:03:49,840
I think, yeah, and I didn't like that

109
00:03:49,840 --> 00:03:51,320
所以我決定戒掉它。

109
00:03:49,840 --> 00:03:51,320
so I decided to go off it.

110
00:03:51,320 --> 00:03:52,600
嗯，所以從那個角度來看，

110
00:03:51,320 --> 00:03:52,600
Um, so in that sense

111
00:03:52,600 --> 00:03:55,980
我覺得我較少使用影片類型的內容，

111
00:03:52,600 --> 00:03:55,980
I think less of usage with like video form content

112
00:03:55,980 --> 00:03:59,920
但我喜歡用社群媒體和同儕溝通，

112
00:03:55,980 --> 00:03:59,920
but I do like you social media to like communicate with my peers,

113
00:03:59,920 --> 00:04:00,920
你知道的，

113
00:03:59,920 --> 00:04:00,920
and you know

114
00:04:00,920 --> 00:04:03,540
我想Instagram主要是用來分享照片之類的。

114
00:04:00,920 --> 00:04:03,540
I guess Instagram for like photos and stuff like that.

115
00:04:03,540 --> 00:04:07,460
但我覺得ChatGPT在我生活中變得更重要了。

115
00:04:03,540 --> 00:04:07,460
But I think chat GPT has become more salient in my life.

116
00:04:07,460 --> 00:04:09,480
因為我覺得現在作為學生，

116
00:04:07,460 --> 00:04:09,480
Like, cuz I feel like right now as a student.

117
00:04:09,480 --> 00:04:11,880
我的生活有80%都在學校，

117
00:04:09,480 --> 00:04:11,880
Like, like 80% of my life is like school

118
00:04:11,880 --> 00:04:13,200
甚至連暑假也是，

118
00:04:11,880 --> 00:04:13,200
and then I guess even the summer,

119
00:04:13,200 --> 00:04:15,560
我的生活有另外80%都在工作。

119
00:04:13,200 --> 00:04:15,560
like another 80% of my life is like work.

120
00:04:15,560 --> 00:04:17,280
所以我覺得我確實會使用它。

120
00:04:15,560 --> 00:04:17,280
So I feel like I definitely use it

121
00:04:17,280 --> 00:04:19,380
無論是在那方面，

121
00:04:17,280 --> 00:04:19,380
whether it's like in terms of that,

122
00:04:19,380 --> 00:04:21,180
甚至是日常生活中的活動。

122
00:04:19,380 --> 00:04:21,180
or even like day-to-day like activities.

123
00:04:21,180 --> 00:04:25,220
我覺得我用ChatGPT的頻率遠高於社群媒體。

123
00:04:21,180 --> 00:04:25,220
I feel like much more using chat GPT than I am social media.

124
00:04:25,220 --> 00:04:27,320
是啊，沒錯。

124
00:04:25,220 --> 00:04:27,320
But yeah, yeah.

125
00:04:27,320 --> 00:04:30,080
我認為一個很重要的點是，

125
00:04:27,320 --> 00:04:30,080
I think a big thing is that, like

126
00:04:30,080 --> 00:04:32,460
社群媒體現在試圖成為那種

126
00:04:30,080 --> 00:04:32,460
social media is now trying to be that kind of

127
00:04:32,460 --> 00:04:34,700
一站式的綜合平台，

127
00:04:32,460 --> 00:04:34,700
like a one-stop shop for kind of everything,

128
00:04:34,700 --> 00:04:35,000
對吧？

128
00:04:34,700 --> 00:04:35,000
right?

129
00:04:35,000 --> 00:04:37,480
現在人們會在社群媒體上購物，

129
00:04:35,000 --> 00:04:37,480
Like, people will now shop on social media

130
00:04:37,480 --> 00:04:39,840
也會在社群媒體上獲取新聞，

130
00:04:37,480 --> 00:04:39,840
people will now find their news on social media

131
00:04:39,840 --> 00:04:40,920
看看朋友們在做什麼。

131
00:04:39,840 --> 00:04:40,920
see what their friends are doing.

132
00:04:40,920 --> 00:04:44,220
我覺得，跟你說的類似，這就是關於

132
00:04:40,920 --> 00:04:44,220
And I think, similar to what you were saying, yeah, be about like

133
00:04:44,220 --> 00:04:45,380
你如何使用社群媒體，

133
00:04:44,220 --> 00:04:45,380
how social media you

134
00:04:45,380 --> 00:04:47,320
你開始接觸到各種不同的內容。

134
00:04:45,380 --> 00:04:47,320
you kind of started getting a little bit of everything.

135
00:04:47,320 --> 00:04:50,520
有些人覺得這是好事，但對我來說，

135
00:04:47,320 --> 00:04:50,520
You know, some people can find that as a good thing, but for me

136
00:04:50,520 --> 00:04:52,120
以前我使用社群媒體時，

136
00:04:50,520 --> 00:04:52,120
when I used to go on social media

137
00:04:52,120 --> 00:04:55,040
我發現自己會滑動好幾個小時。

137
00:04:52,120 --> 00:04:55,040
I realized that I was just scrolling for hours.

138
00:04:55,040 --> 00:04:57,160
因為我覺得，喔，像是

138
00:04:55,040 --> 00:04:57,160
Because I realized that I was like, oh, like

139
00:04:57,160 --> 00:04:58,220
我正在獲取一些新聞內容，

139
00:04:57,160 --> 00:04:58,220
I'm getting some news content

140
00:04:58,220 --> 00:04:59,880
我也在這裡學習一些事情。

140
00:04:58,220 --> 00:04:59,880
I'm learning about some things here.

141
00:04:59,880 --> 00:05:01,600
所以我對於

141
00:04:59,880 --> 00:05:01,600
And so I was less actual

142
00:05:01,600 --> 00:05:04,340
自己花在社群媒體上的時間，並沒有真正意識到。

142
00:05:01,600 --> 00:05:04,340
actually conscious about the time I was spending on social media.

143
00:05:04,340 --> 00:05:06,440
我發現那段時間裡，很多內容

143
00:05:04,340 --> 00:05:06,440
And I realized that a lot of that time is

144
00:05:06,440 --> 00:05:08,740
其實是我不想看的。

144
00:05:06,440 --> 00:05:08,740
just a lot of content that I don't want as well.

145
00:05:08,740 --> 00:05:12,600
所以我意識到，對我日常生活來說，

145
00:05:08,740 --> 00:05:12,600
And so I've realized that that, you know, for my day-to-day I

146
00:05:12,600 --> 00:05:14,780
我減少了使用社群媒體的時間，

146
00:05:12,600 --> 00:05:14,780
I've drawn back on my social media usage

147
00:05:14,780 --> 00:05:16,980
在學習和探索想法方面，

147
00:05:14,780 --> 00:05:16,980
and when it comes to learning and exploring ideas.

148
00:05:16,980 --> 00:05:19,000
我會向 Chat GPT 提出很多問題，

148
00:05:16,980 --> 00:05:19,000
I ask Chat GPT a lot of those questions

149
00:05:19,000 --> 00:05:21,340
因為我對自己想要的東西非常明確，

149
00:05:19,000 --> 00:05:21,340
because I'm very specific about what I want

150
00:05:21,340 --> 00:05:23,480
以及我能從 Chat GPT 獲得什麼。

150
00:05:21,340 --> 00:05:23,480
and what I can get out of chat, GPT.

151
00:05:23,480 --> 00:05:24,200
還有社群媒體。

151
00:05:23,480 --> 00:05:24,200
And social media.

152
00:05:24,200 --> 00:05:27,000
我有意識地把它當作休閒時間使用，

152
00:05:24,200 --> 00:05:27,000
I I use consciously is like my leisure time,

153
00:05:27,000 --> 00:05:29,300
就是放鬆一下，享受一些內容。

153
00:05:27,000 --> 00:05:29,300
just like relax and just enjoy some content.

154
00:05:29,300 --> 00:05:30,640
而且，我不會

154
00:05:29,300 --> 00:05:30,640
Whereas, like, I don't

155
00:05:30,640 --> 00:05:32,940
喜歡把所有東西都混在同一個應用程式裡。

155
00:05:30,640 --> 00:05:32,940
I don't like to kind of mix everything into one app.

156
00:05:32,940 --> 00:05:35,680
對我來說，有這麼一刻，

156
00:05:32,940 --> 00:05:35,680
For me, there was this moment

157
00:05:35,680 --> 00:05:37,860
那真的是在深入研究時發生的，

157
00:05:35,680 --> 00:05:37,860
and it really happened with deep research

158
00:05:37,860 --> 00:05:39,720
我得到的資訊品質，

158
00:05:37,860 --> 00:05:39,720
where the quality of things I was

159
00:05:39,720 --> 00:05:41,760
常常比我在網路上找到的還要好。

159
00:05:39,720 --> 00:05:41,760
getting was often better than I found online.

160
00:05:41,760 --> 00:05:42,860
我發現，

160
00:05:41,760 --> 00:05:42,860
And I found it like

161
00:05:42,860 --> 00:05:44,360
我可以提出一個非常有趣的問題，

161
00:05:42,860 --> 00:05:44,360
I could ask a really interesting question.

162
00:05:44,360 --> 00:05:46,040
可能是關於一些愚蠢的事情，比如說，

162
00:05:44,360 --> 00:05:46,040
It'd be something silly about, like, you know

163
00:05:46,040 --> 00:05:49,100
一群鱷魚屠殺人類之類的，我是說，

163
00:05:46,040 --> 00:05:49,100
a bunch of crocodiles massacring people or something like, I mean

164
00:05:49,100 --> 00:05:50,420
還有一個二戰的故事。

164
00:05:49,100 --> 00:05:50,420
there's a World War Two story.

165
00:05:50,420 --> 00:05:52,000
我會想，這到底是真相是什麼？

165
00:05:50,420 --> 00:05:52,000
I'm like, what's the truth about this?

166
00:05:52,000 --> 00:05:54,420
你會得到一份深入的研究報告，並且回覆得很好。

166
00:05:52,000 --> 00:05:54,420
And you get a deep research report and comes back well.

167
00:05:54,420 --> 00:05:55,040
我當時反應過度了，

167
00:05:54,420 --> 00:05:55,040
I was overblown,

168
00:05:55,040 --> 00:05:58,200
但瀏覽這些資料並說，好的，這很有趣，

168
00:05:55,040 --> 00:05:58,200
but that was fun to sort of look through these things and say, Okay

169
00:05:58,200 --> 00:05:59,320
我想要的是什麼問題？

169
00:05:58,200 --> 00:05:59,320
what are questions I want that?

170
00:05:59,320 --> 00:06:00,560
我希望有相關的文章。

170
00:05:59,320 --> 00:06:00,560
I wish there are articles on that?

171
00:06:00,560 --> 00:06:03,520
你有沒有發現，突然之間，

171
00:06:00,560 --> 00:06:03,520
And and did you find that, too, that all of a sudden?

172
00:06:04,180 --> 00:06:05,140
內容變得更好了。

172
00:06:04,180 --> 00:06:05,140
Content got better.

173
00:06:05,140 --> 00:06:07,620
我記得在高中時，

173
00:06:05,140 --> 00:06:07,620
I remember so in high school

174
00:06:07,620 --> 00:06:09,380
當 ChatGPT 剛推出的時候，

174
00:06:07,620 --> 00:06:09,380
when when Chad GPT first came out

175
00:06:09,380 --> 00:06:12,880
我正在上一門研究課，

175
00:06:09,380 --> 00:06:12,880
I was in like a research like a research class.

176
00:06:12,880 --> 00:06:15,720
基本上我就是研究我想要的主題，

176
00:06:12,880 --> 00:06:15,720
Where I basically just researched on a topic that I wanted to.

177
00:06:15,720 --> 00:06:15,920
然後

177
00:06:15,720 --> 00:06:15,920
And

178
00:06:15,920 --> 00:06:16,300
然後

178
00:06:15,920 --> 00:06:16,300
and

179
00:06:16,300 --> 00:06:17,260
寫了一篇論文。

179
00:06:16,300 --> 00:06:17,260
wrote a paper about it.

180
00:06:17,260 --> 00:06:20,000
一開始，ChatGPT 無法連接網路，

180
00:06:17,260 --> 00:06:20,000
And early on, Chad GPT couldn't access the web

181
00:06:20,000 --> 00:06:21,960
所以我會貼上一個連結，然後想，喔

181
00:06:20,000 --> 00:06:21,960
so I'd paste in a link and be like, Oh

182
00:06:21,960 --> 00:06:22,860
我看不懂那個。

182
00:06:21,960 --> 00:06:22,860
I can't read that.

183
00:06:22,860 --> 00:06:24,600
所以我發現，

183
00:06:22,860 --> 00:06:24,600
And so I found that like

184
00:06:24,600 --> 00:06:26,300
我會像是放入一篇很長的論文，

184
00:06:24,600 --> 00:06:26,300
I would like pace like a big paper in

185
00:06:26,300 --> 00:06:28,140
然後試著用那種方式給它資料，

185
00:06:26,300 --> 00:06:28,140
and and try to give it that way

186
00:06:28,140 --> 00:06:29,460
但結果並不是這樣。

186
00:06:28,140 --> 00:06:29,460
but it wasn't.

187
00:06:29,460 --> 00:06:30,520
它不擅長，像是

187
00:06:29,460 --> 00:06:30,520
It wasn't good at, like

188
00:06:30,520 --> 00:06:31,680
幫我找內容，

188
00:06:30,520 --> 00:06:31,680
finding content for me

189
00:06:31,680 --> 00:06:32,900
所以我真的得自己去做這件事。

189
00:06:31,680 --> 00:06:32,900
so I really had to do that myself.

190
00:06:32,900 --> 00:06:34,360
不過深度研究，

190
00:06:32,900 --> 00:06:34,360
But deep research,

191
00:06:34,360 --> 00:06:38,420
它能讓我找到符合我需求的特定內容。

191
00:06:34,360 --> 00:06:38,420
it could allow me to actually find content specific to what I want.

192
00:06:38,420 --> 00:06:41,860
我通常會告訴它，

192
00:06:38,420 --> 00:06:41,860
And what I usually do is I tell it

193
00:06:41,860 --> 00:06:44,520
只搜尋真正學術性的，

193
00:06:41,860 --> 00:06:44,520
to only only search for really academic,

194
00:06:44,520 --> 00:06:48,640
有實證支持的研究，並且形成論點。

194
00:06:44,520 --> 00:06:48,640
merit-backed, like research and formulate an argument.

195
00:06:48,640 --> 00:06:53,720
對我來說，內容本身其實非常好。

195
00:06:48,640 --> 00:06:53,720
For me, and the content itself is actually very good

196
00:06:53,720 --> 00:06:54,980
我經常引用它。

196
00:06:53,720 --> 00:06:54,980
and I cite it pretty often.

197
00:06:54,980 --> 00:06:56,140
那麼對你來說，Abby

197
00:06:54,980 --> 00:06:56,140
So for you, Abby

198
00:06:56,140 --> 00:06:57,980
你有什麼特別的技巧嗎

198
00:06:56,140 --> 00:06:57,980
do you have any particular tricks you use

199
00:06:57,980 --> 00:06:59,260
或者你會做些什麼來嘗試讓它

199
00:06:57,980 --> 00:06:59,260
or things you do to try to get it

200
00:06:59,260 --> 00:07:00,820
以你想要的方式運作？

200
00:06:59,260 --> 00:07:00,820
to sort of perform the way you want it to?

201
00:07:00,820 --> 00:07:03,400
嗯，我想，首先也是最重要的

201
00:07:00,820 --> 00:07:03,400
Um, I think, yeah, first and foremost

202
00:07:03,400 --> 00:07:04,920
就是縮小研究的範圍

202
00:07:03,400 --> 00:07:04,920
kind of like narrowing the research

203
00:07:04,920 --> 00:07:07,000
我覺得透過提供特定的參數很有幫助

203
00:07:04,920 --> 00:07:07,000
parameters I think is helpful by feeding

204
00:07:07,000 --> 00:07:10,060
讓它從我想要的資料來源生成內容。

204
00:07:07,000 --> 00:07:10,060
it kind of the sources that I want it to kind of generate from.

205
00:07:10,060 --> 00:07:13,160
然後我覺得對我來說

205
00:07:10,060 --> 00:07:13,160
And then I think for me

206
00:07:13,160 --> 00:07:16,160
我也會讓它扮演批判者的角色。

206
00:07:13,160 --> 00:07:16,160
I also prompt it to be a critical actor.

207
00:07:16,160 --> 00:07:18,420
有時候，因為我覺得

207
00:07:16,160 --> 00:07:18,420
Sometimes because I think, like

208
00:07:18,420 --> 00:07:19,400
儘管我很喜歡

208
00:07:18,420 --> 00:07:19,400
as much as I like

209
00:07:19,400 --> 00:07:22,060
我收到的回饋，特別是說

209
00:07:19,400 --> 00:07:22,060
kind of the feedback I receive, especially, say so

210
00:07:22,060 --> 00:07:24,620
比方說我在腦力激盪研究論文的時候，對吧？

210
00:07:22,060 --> 00:07:24,620
say if I'm like, trying to brainstorm, like a research paper, right?

211
00:07:24,620 --> 00:07:25,700
我當時心想

211
00:07:24,620 --> 00:07:25,700
And I'm like

212
00:07:25,700 --> 00:07:28,940
在構思我的論文主旨陳述，並且

212
00:07:25,700 --> 00:07:28,940
formulating like my kind of thesis like statement and having,

213
00:07:28,940 --> 00:07:29,860
整理我的重點。

213
00:07:28,940 --> 00:07:29,860
like, my set points.

214
00:07:29,860 --> 00:07:31,700
我覺得我收到的回饋

214
00:07:29,860 --> 00:07:31,700
And I think the feedback I get,

215
00:07:31,700 --> 00:07:33,960
大多數都是非常正面的。

215
00:07:31,700 --> 00:07:33,960
for the most part is like, overwhelmingly positive.

216
00:07:33,960 --> 00:07:36,900
是的，那是大家對它的第一反應

216
00:07:33,960 --> 00:07:36,900
Yeah, and that's like the first, I mean, reaction to it

217
00:07:36,900 --> 00:07:39,320
我認為這是合理的。

217
00:07:36,900 --> 00:07:39,320
and I think that's that's valid.

218
00:07:39,320 --> 00:07:42,000
但有時我希望能有真正批判性的觀點。

218
00:07:39,320 --> 00:07:42,000
But sometimes I want an actual critical like outlook.

219
00:07:42,000 --> 00:07:43,520
所以我覺得引導它去

219
00:07:42,000 --> 00:07:43,520
So I think prompting it to kind

220
00:07:43,520 --> 00:07:46,180
扮演特定角色非常有幫助。

220
00:07:43,520 --> 00:07:46,180
of embody certain personas is really helpful.

221
00:07:46,180 --> 00:07:49,280
它給出的回應能夠實際上做出一篇

221
00:07:46,180 --> 00:07:49,280
And like the response that it gives to actually make a like,

222
00:07:49,280 --> 00:07:50,740
完整的論文。

222
00:07:49,280 --> 00:07:50,740
a holistic like paper.

223
00:07:50,740 --> 00:07:54,280
然後我覺得把這個和，像是，

223
00:07:50,740 --> 00:07:54,280
And then I think coupling that, like, with, you know

224
00:07:54,280 --> 00:07:55,760
請教授協助

224
00:07:54,280 --> 00:07:55,760
like having your professors help

225
00:07:55,760 --> 00:07:59,340
並且填補某些空白，會很有幫助。

225
00:07:55,760 --> 00:07:59,340
and kind of also filling certain gaps, I think helps.

226
00:07:59,340 --> 00:08:01,140
像是要有一個整體的概覽。

226
00:07:59,340 --> 00:08:01,140
Like, have a holistic kind of overview.

227
00:08:01,140 --> 00:08:02,240
至少當我在寫作時是這樣。

227
00:08:01,140 --> 00:08:02,240
At least when I'm writing people.

228
00:08:02,240 --> 00:08:04,340
所以給我們一些你如何提示的例子，

228
00:08:02,240 --> 00:08:04,340
So give us some example of how you prompt

229
00:08:04,340 --> 00:08:04,980
當你談到

229
00:08:04,340 --> 00:08:04,980
when you talk about

230
00:08:04,980 --> 00:08:05,880
不同的角色時，

230
00:08:04,980 --> 00:08:05,880
like a different personas

231
00:08:05,880 --> 00:08:07,440
你具體會要求它做什麼。

231
00:08:05,880 --> 00:08:07,440
what you ask it to do specifically.

232
00:08:07,440 --> 00:08:10,280
舉例來說，

232
00:08:07,440 --> 00:08:10,280
So, for example,

233
00:08:10,280 --> 00:08:15,220
我想之前我做過一些關於陰謀論的研究，

233
00:08:10,280 --> 00:08:15,220
I think a while back I was doing some research on conspiracy theories

234
00:08:15,220 --> 00:08:18,800
我覺得那是很有趣的研究。

234
00:08:15,220 --> 00:08:18,800
and I think it was it was interesting research.

235
00:08:18,800 --> 00:08:20,840
所以我覺得外面有很多資訊，

235
00:08:18,800 --> 00:08:20,840
So I feel like there's a lot of information out there

236
00:08:20,840 --> 00:08:24,560
所以我認為第一步，肯定是縮小那些範圍。

236
00:08:20,840 --> 00:08:24,560
and so I think first step, definitely narrowing those parameters.

237
00:08:24,560 --> 00:08:27,820
然後我想我在創造一些論點，

237
00:08:24,560 --> 00:08:27,820
And then I think I was creating kind of the arguments that like

238
00:08:27,820 --> 00:08:30,240
接著我請它做的。

238
00:08:27,820 --> 00:08:30,240
followed under there and I asked it.

239
00:08:30,240 --> 00:08:33,820
嗯，我請它實際編造一個陰謀論，

239
00:08:30,240 --> 00:08:33,820
Um, I asked it to actually make up a conspiracy theory

240
00:08:33,820 --> 00:08:36,000
利用我一直在學習的概念，

240
00:08:33,820 --> 00:08:36,000
using kind of the concepts that I've been learning,

241
00:08:36,000 --> 00:08:38,640
所以它編造了一個陰謀論。

241
00:08:36,000 --> 00:08:38,640
and so it made up a conspiracy theory.

242
00:08:38,640 --> 00:08:39,460
那個理論是什麼？

242
00:08:38,640 --> 00:08:39,460
What was the theory?

243
00:08:39,460 --> 00:08:41,660
我想那是一個關於……的理論，

243
00:08:39,460 --> 00:08:41,660
I think it was some theory about like

244
00:08:41,660 --> 00:08:44,880
機場的鏡子怎麼運作，

244
00:08:41,660 --> 00:08:44,880
airport mirrors go on

245
00:08:44,880 --> 00:08:47,760
但我忘了具體細節是什麼，

245
00:08:44,880 --> 00:08:47,760
and I forgot what the exact parameters were

246
00:08:47,760 --> 00:08:49,860
大概是說機場的鏡子是怎麼樣的。

246
00:08:47,760 --> 00:08:49,860
but it was something about how like airport mirrors are.

247
00:08:49,860 --> 00:08:50,200
像是，

247
00:08:49,860 --> 00:08:50,200
Like,

248
00:08:50,200 --> 00:08:53,920
監視你的行動，提醒你要小心。

248
00:08:50,200 --> 00:08:53,920
monitoring like your activity and how like you should be careful.

249
00:08:53,920 --> 00:08:57,140
但它顯然用很優雅的方式表達，

249
00:08:53,920 --> 00:08:57,140
But it obviously worded it in very elegant way

250
00:08:57,140 --> 00:08:59,980
因為它利用了那類研究，來說明，

250
00:08:57,140 --> 00:08:59,980
because it was using kind of that research to kind of like, say, okay

251
00:08:59,980 --> 00:09:01,340
這就是人們相信它的原因。

251
00:08:59,980 --> 00:09:01,340
this is why people believe in it.

252
00:09:01,340 --> 00:09:04,980
所以我請它扮演不同角色，

252
00:09:01,340 --> 00:09:04,980
And so I asked it to take up like certain personas across,

253
00:09:04,980 --> 00:09:08,400
例如政治光譜上的角色，然後問它，

253
00:09:04,980 --> 00:09:08,400
like, for example, the political scale, and ask it.

254
00:09:08,400 --> 00:09:09,980
你知道，這個人會怎麼反應，

254
00:09:08,400 --> 00:09:09,980
Like, you know, how would this person react to it

255
00:09:09,980 --> 00:09:11,320
這個人會怎麼批評它。

255
00:09:09,980 --> 00:09:11,320
how would this person critique it

256
00:09:11,320 --> 00:09:12,780
這個人怎麼會相信這個呢？

256
00:09:11,320 --> 00:09:12,780
how would this person believe in it?

257
00:09:12,780 --> 00:09:15,800
我覺得它給了我一個大致的概覽，

257
00:09:12,780 --> 00:09:15,800
And I think it gave me, like, kind of like an overview,

258
00:09:15,800 --> 00:09:18,800
不只是學習，也幫助我形成論點。

258
00:09:15,800 --> 00:09:18,800
like for my learning, but also like for the like argument forming.

259
00:09:18,800 --> 00:09:20,640
確保我從這個角度思考，

259
00:09:18,800 --> 00:09:20,640
Making sure that I'm thinking it from like,

260
00:09:20,640 --> 00:09:22,020
用批判的視角或其他視角。

260
00:09:20,640 --> 00:09:22,020
this critical lens or this lens.

261
00:09:22,020 --> 00:09:22,840
而不是只做，像是

261
00:09:22,020 --> 00:09:22,840
And not just do, like

262
00:09:22,840 --> 00:09:25,640
我認為你對那個陰謀論所知道的事。

262
00:09:22,840 --> 00:09:25,640
what I think you know about that conspiracy theory.

263
00:09:26,100 --> 00:09:26,760
那真的很棒。

263
00:09:26,100 --> 00:09:26,760
That's really good.

264
00:09:26,760 --> 00:09:31,780
我覺得這是其中一個很容易被忽略的事情，

264
00:09:26,760 --> 00:09:31,780
I think that that's one of the really low key things

265
00:09:31,780 --> 00:09:34,520
我認為大家可以多做的，不只是說，

265
00:09:31,780 --> 00:09:34,520
I think people could probably do more of is not just say

266
00:09:34,520 --> 00:09:37,240
只給一個觀點，而是實際說出，像是，

266
00:09:34,520 --> 00:09:37,240
give it one point of view, but actually tell, like, yeah

267
00:09:37,240 --> 00:09:38,440
給我幾個不同的觀點。

267
00:09:37,240 --> 00:09:38,440
give me a couple different points of view.

268
00:09:38,440 --> 00:09:39,620
那就是你用 GPT-3 的時候。

268
00:09:38,440 --> 00:09:39,620
And that was you back with GPD 3.

269
00:09:39,620 --> 00:09:41,100
我覺得那真的很酷，因為

269
00:09:39,620 --> 00:09:41,100
That was what I thought was really cool as

270
00:09:41,100 --> 00:09:43,440
我可以模擬不同觀點的對話。

270
00:09:41,100 --> 00:09:43,440
I could simulate a conversation from different points of view.

271
00:09:43,440 --> 00:09:46,440
閱讀那些逐字稿很有趣

271
00:09:43,440 --> 00:09:46,440
And it was reading those transcripts was interesting

272
00:09:46,440 --> 00:09:48,980
因為你可以看到模型相當擅長適應這些內容。

272
00:09:46,440 --> 00:09:48,980
because you could see the models pretty good at adapting to that.

273
00:09:48,980 --> 00:09:51,160
我喜歡拿研究論文來說，

273
00:09:48,980 --> 00:09:51,160
And I love taking research papers and saying

274
00:09:51,160 --> 00:09:52,260
給我反方觀點，

274
00:09:51,160 --> 00:09:52,260
give me the counterpoint

275
00:09:52,260 --> 00:09:53,300
給我這個對那個，

275
00:09:52,260 --> 00:09:53,300
give me this to that

276
00:09:53,300 --> 00:09:54,640
我認為作為這樣的工具，

276
00:09:53,300 --> 00:09:54,640
and I think as that kind of tool,

277
00:09:54,640 --> 00:09:55,140
這是很有幫助的。

277
00:09:54,640 --> 00:09:55,140
that's helpful.

278
00:09:55,540 --> 00:09:56,960
你有沒有特別的

278
00:09:55,540 --> 00:09:56,960
Do you have any particular kind of

279
00:09:56,960 --> 00:09:58,620
技巧用於任何角色設定，

279
00:09:56,960 --> 00:09:58,620
tips you use for any kind of personas

280
00:09:58,620 --> 00:10:00,140
或是你會問的某些問題？

280
00:09:58,620 --> 00:10:00,140
or any to some questions you ask?

281
00:10:00,480 --> 00:10:02,420
是的，絕對如此，我同意

281
00:10:00,480 --> 00:10:02,420
Yeah, definitely, I agree with like

282
00:10:02,420 --> 00:10:04,500
它對你非常寬容，

282
00:10:02,420 --> 00:10:04,500
the it's very like, lenient with you

283
00:10:04,500 --> 00:10:05,600
它會像是，喔，

283
00:10:04,500 --> 00:10:05,600
and it'll be like, oh, like

284
00:10:05,600 --> 00:10:06,820
你在這方面做得很好，

284
00:10:05,600 --> 00:10:06,820
you've done a great job on this

285
00:10:06,820 --> 00:10:09,320
我發現你可以在那裡。

285
00:10:06,820 --> 00:10:09,320
and I found that there you can.

286
00:10:09,320 --> 00:10:12,020
你可以為你的聊天設定具體指示

286
00:10:09,320 --> 00:10:12,020
You can put a specific instructions for your chat

287
00:10:12,020 --> 00:10:14,260
所以如果你進入個人化 GPT

287
00:10:12,020 --> 00:10:14,260
so if you go to like Personalized GPT

288
00:10:14,260 --> 00:10:17,020
你可以輸入具體的指示。

288
00:10:14,260 --> 00:10:17,020
you can put specific instructions.

289
00:10:17,020 --> 00:10:19,380
我會說，不要廢話，直接切入重點。

289
00:10:17,020 --> 00:10:19,380
So I say, like, no fluff, like, just get to the point.

290
00:10:19,380 --> 00:10:22,180
每當我問你問題時，請對我坦誠以待。

290
00:10:19,380 --> 00:10:22,180
Be brutally honest with me whenever I

291
00:10:22,180 --> 00:10:23,760
無論是我提問還是提交內容時。

291
00:10:22,180 --> 00:10:23,760
ask you a question or whenever I submit.

292
00:10:23,760 --> 00:10:26,760
還有關於角色設定的部分。

292
00:10:23,760 --> 00:10:26,760
What I have, and to the persona thing as well.

293
00:10:26,760 --> 00:10:26,960
像是，

293
00:10:26,760 --> 00:10:26,960
Like,

294
00:10:26,960 --> 00:10:29,600
我覺得這是一件很棒的事，

294
00:10:26,960 --> 00:10:29,600
I think it's it's a great thing to to

295
00:10:29,600 --> 00:10:32,120
能獲得不同的觀點，而不是只告訴它，

295
00:10:29,600 --> 00:10:32,120
get different perspectives versus just tell it,

296
00:10:32,120 --> 00:10:32,300
喔，

296
00:10:32,120 --> 00:10:32,300
oh

297
00:10:32,300 --> 00:10:33,120
你覺得怎麼樣？

297
00:10:32,300 --> 00:10:33,120
what do you think?

298
00:10:33,120 --> 00:10:34,860
因為那樣它會更像是，

298
00:10:33,120 --> 00:10:34,860
Because then it'll be, oh, like, more like

299
00:10:34,860 --> 00:10:37,220
我會站在你的立場和觀點上。

299
00:10:34,860 --> 00:10:37,220
I'll take your side and your perspective on it.

300
00:10:37,220 --> 00:10:40,140
所以我告訴它，知道嗎，

300
00:10:37,220 --> 00:10:40,140
So I've, I've told it to, you know

301
00:10:40,140 --> 00:10:42,500
扮演頂尖公司的顧問角色

301
00:10:40,140 --> 00:10:42,500
act as like a like a consultant at a top firm

302
00:10:42,500 --> 00:10:44,240
告訴我你是怎麼做到的。

302
00:10:42,500 --> 00:10:44,240
and tell me how you do this.

303
00:10:44,240 --> 00:10:46,440
或者我叫她扮演一位非常有創意的

303
00:10:44,240 --> 00:10:46,440
Or I told her to act as like a super creative

304
00:10:46,440 --> 00:10:48,760
專門研究這個領域的教授

304
00:10:46,440 --> 00:10:48,760
like professor that researches on this

305
00:10:48,760 --> 00:10:50,440
並提出獨特的解決方案。

305
00:10:48,760 --> 00:10:50,440
and come up with unique solutions to this.

306
00:10:50,440 --> 00:10:53,820
所以我覺得我真的在

306
00:10:50,440 --> 00:10:53,820
So I think I'm really putting putting

307
00:10:53,820 --> 00:10:56,660
賦予ChatGPT一個角色或視角。

307
00:10:53,820 --> 00:10:56,660
chat GPT into like a persona or perspective.

308
00:10:56,660 --> 00:11:00,320
而且專業人士會

308
00:10:56,660 --> 00:11:00,320
And a professional will like

309
00:11:00,320 --> 00:11:02,800
讓它能夠表現得更好，這是我注意到的。

309
00:11:00,320 --> 00:11:02,800
empower it to actually do better is what I've noticed.

310
00:11:03,180 --> 00:11:07,700
我覺得你們兩個都是非常投入的學生。

310
00:11:03,180 --> 00:11:07,700
You two are both, I think, probably very well engaged students.

311
00:11:07,700 --> 00:11:10,600
你們對這件事有很多想法。

311
00:11:07,700 --> 00:11:10,600
You, you, you have a lot of thought about this.

312
00:11:10,600 --> 00:11:13,320
你聽過哪些誤解？

312
00:11:10,600 --> 00:11:13,320
What are some misconceptions you hear?

313
00:11:13,320 --> 00:11:15,520
像是我跟人談到在教育中使用ChatGPT，

313
00:11:13,320 --> 00:11:15,520
Like, I talk to people, bring a chat, GPT in education?

314
00:11:15,520 --> 00:11:17,460
大家會說有人用它作弊之類的，然後

314
00:11:15,520 --> 00:11:17,460
People got cheating and this sort of stuff, and again

315
00:11:17,460 --> 00:11:19,880
這裡面確實有值得討論的話題。

315
00:11:17,460 --> 00:11:19,880
there's a conversation to be had for there.

316
00:11:19,880 --> 00:11:21,500
但我覺得有時候我會聽到這樣的假設

316
00:11:19,880 --> 00:11:21,500
But I think sometimes I hear this assumption

317
00:11:21,500 --> 00:11:24,440
認為每個孩子都會自動用這個來作弊。

317
00:11:21,500 --> 00:11:24,440
that every kid's just automatically going to use this to cheat.

318
00:11:25,000 --> 00:11:26,340
你有遇過這種情況嗎？

318
00:11:25,000 --> 00:11:26,340
Have you encountered that?

319
00:11:26,480 --> 00:11:27,760
你覺得，嗯，

319
00:11:26,480 --> 00:11:27,760
Do you feel like, you know

320
00:11:27,760 --> 00:11:30,620
這是你經驗中會發生的事嗎？

320
00:11:27,760 --> 00:11:30,620
was that a in your experience?

321
00:11:31,620 --> 00:11:32,400
這是一個很好的問題，

321
00:11:31,620 --> 00:11:32,400
It is a great question

322
00:11:32,400 --> 00:11:34,500
這個問題需要很多思考。

322
00:11:32,400 --> 00:11:34,500
and there's a lot of thought that goes into it.

323
00:11:34,500 --> 00:11:37,900
但我越想關於原創性，

323
00:11:34,500 --> 00:11:37,900
But the more I think about kind of originality

324
00:11:37,900 --> 00:11:42,060
以及人們如何想要你自己真實且原創的作品。

324
00:11:37,900 --> 00:11:42,060
and how how people want like your own authentic, original work.

325
00:11:42,060 --> 00:11:45,260
因為這樣會幫助你的學習。

325
00:11:42,060 --> 00:11:45,260
Because like, that'll help with your learning.

326
00:11:45,260 --> 00:11:46,320
這點絕對沒錯。

326
00:11:45,260 --> 00:11:46,320
That's that's definitely true.

327
00:11:46,320 --> 00:11:49,560
但我們會到達一個關於 AI 的階段，

327
00:11:46,320 --> 00:11:49,560
But then we're gonna get to a point with AI

328
00:11:49,560 --> 00:11:50,560
也許我們已經到了，

328
00:11:49,560 --> 00:11:50,560
maybe we already have

329
00:11:50,560 --> 00:11:52,800
而且這趨勢會持續下去。

329
00:11:50,560 --> 00:11:52,800
but it'll keep going down the line.

330
00:11:52,800 --> 00:11:57,220
屆時，AI 產出的作品會變成像是，

330
00:11:52,800 --> 00:11:57,220
Where actually, like, AI produced work is going to be like,

331
00:11:57,220 --> 00:11:59,760
比人類好上許多許多。

331
00:11:57,220 --> 00:11:59,760
much, much better than than humans.

332
00:11:59,760 --> 00:12:00,760
到了這種程度，

332
00:11:59,760 --> 00:12:00,760
To the point where

333
00:12:00,760 --> 00:12:04,760
我們為什麼還要輕忽 AI 所能產出的成果？

333
00:12:00,760 --> 00:12:04,760
why are we just like dismissing what what an AI can produce?

334
00:12:04,760 --> 00:12:07,420
我認為很快我們就會達到這個階段，

334
00:12:04,760 --> 00:12:07,420
And I think soon we'll get to the point

335
00:12:07,420 --> 00:12:12,140
像 ChatGPT 這樣的工具會有

335
00:12:07,420 --> 00:12:12,140
where we'll have chat GPT like it'll have a

336
00:12:12,140 --> 00:12:13,540
更大的影響力，比方說，

336
00:12:12,140 --> 00:12:13,540
much bigger impact and let's say,

337
00:12:13,540 --> 00:12:15,040
用來撰寫報告。

337
00:12:13,540 --> 00:12:15,040
like writing some report.

338
00:12:15,040 --> 00:12:17,380
但老師們也會

338
00:12:15,040 --> 00:12:17,380
But then teachers will

339
00:12:17,380 --> 00:12:19,000
以及學生們也必須適應

339
00:12:17,380 --> 00:12:19,000
and students will also have to adapt

340
00:12:19,000 --> 00:12:21,400
如何展現他們的成長。

340
00:12:19,000 --> 00:12:21,400
to how they can show their growth.

341
00:12:21,400 --> 00:12:24,340
同時也要充分利用 AI，

341
00:12:21,400 --> 00:12:24,340
While also like using AI to its fullest

342
00:12:24,340 --> 00:12:28,740
因為它能完成許多人體無法做到的任務。

342
00:12:24,340 --> 00:12:28,740
because it can complete so many great tasks that humans can't.

343
00:12:29,100 --> 00:12:31,820
我覺得這個問題很難回答，

343
00:12:29,100 --> 00:12:31,820
I think it's it's just hard to answer that question

344
00:12:31,820 --> 00:12:33,380
因為我覺得我們正在重新定義。

344
00:12:31,820 --> 00:12:33,380
because I feel like we're redefining.

345
00:12:33,400 --> 00:12:34,820
不，他剛剛說這是個很棒的問題。

345
00:12:33,400 --> 00:12:34,820
No, he just said it was a great question.

346
00:12:35,060 --> 00:12:36,320
他剛剛說這是個好問題。

346
00:12:35,060 --> 00:12:36,320
He just said it was a good question.

347
00:12:36,640 --> 00:12:41,160
我只是想說

347
00:12:36,640 --> 00:12:41,160
I was just saying

348
00:12:41,160 --> 00:12:42,660
我覺得這真的很難

348
00:12:41,160 --> 00:12:42,660
I think it's just it's hard

349
00:12:42,660 --> 00:12:45,380
因為我覺得我們好像在重新定義，像是

349
00:12:42,660 --> 00:12:45,380
because I feel like we're like kind of redefining, we're like.

350
00:12:45,380 --> 00:12:46,300
作弊的意義，

350
00:12:45,380 --> 00:12:46,300
Cheating means,

351
00:12:46,300 --> 00:12:49,020
所以我覺得我甚至不知道作弊到底是什麼意思了。

351
00:12:46,300 --> 00:12:49,020
so I feel like I don't even know what cheating means anymore.

352
00:12:49,020 --> 00:12:52,000
所以我覺得這就是為什麼會

352
00:12:49,020 --> 00:12:52,000
So I feel like that's why it's like,

353
00:12:52,000 --> 00:12:52,480
有點困難。

353
00:12:52,000 --> 00:12:52,480
kind of hard.

354
00:12:52,960 --> 00:12:53,280
是的，不，

354
00:12:52,960 --> 00:12:53,280
Yeah, no,

355
00:12:53,280 --> 00:12:55,600
這是一個合理的問題，也是一個合理的回答。

355
00:12:53,280 --> 00:12:55,600
it's a fair question and a fair response.

356
00:12:56,440 --> 00:12:59,260
你會給高中生什麼建議

356
00:12:56,440 --> 00:12:59,260
What advice would you give to high

357
00:12:59,260 --> 00:13:01,540
關於現在使用這些工具？

357
00:12:59,260 --> 00:13:01,540
school students right now about using these tools?

358
00:13:02,240 --> 00:13:04,280
我不知道，我想我有件事，像是

358
00:13:02,240 --> 00:13:04,280
I don't know something that I think I've, like

359
00:13:04,280 --> 00:13:05,560
很多朋友都在討論的。

359
00:13:04,280 --> 00:13:05,560
a lot of my friends were talking about.

360
00:13:05,560 --> 00:13:08,260
甚至在我來這裡之前，我也，知道嗎

360
00:13:05,560 --> 00:13:08,260
Even like before I came, came here, I was, you know

361
00:13:08,260 --> 00:13:09,520
和大學的朋友們待在一起

361
00:13:08,260 --> 00:13:09,520
staying with my friends from college

362
00:13:09,520 --> 00:13:11,320
我們就在聊著一些事情。

362
00:13:09,520 --> 00:13:11,320
and we were just like talking about.

363
00:13:11,320 --> 00:13:14,220
我在問他們關於 Chat GPT 和人工智慧的經驗

363
00:13:11,320 --> 00:13:14,220
I was asking their experiences about like, Chat GPT and AI

364
00:13:14,220 --> 00:13:16,420
其中一個朋友說

364
00:13:14,220 --> 00:13:16,420
and one of my friends was like

365
00:13:16,420 --> 00:13:19,840
我覺得如果高中時有 Chat GPT 的話

365
00:13:16,420 --> 00:13:19,840
I feel like if I had Chat GPT in high school

366
00:13:19,840 --> 00:13:21,960
我現在不會有今天的成就。

366
00:13:19,840 --> 00:13:21,960
I wouldn't be where I am today.

367
00:13:21,960 --> 00:13:23,400
我問她說，喔

367
00:13:21,960 --> 00:13:23,400
And I asked her, like, Oh

368
00:13:23,400 --> 00:13:25,480
你這話是什麼意思？她說

368
00:13:23,400 --> 00:13:25,480
what do you mean by that? and she said

369
00:13:25,480 --> 00:13:29,180
我覺得我會變得更笨。

369
00:13:25,480 --> 00:13:29,180
I feel like I would just be stupider.

370
00:13:29,180 --> 00:13:31,180
我覺得她這樣說得最貼切

370
00:13:29,180 --> 00:13:31,180
I feel like that's just the best way she put it

371
00:13:31,180 --> 00:13:32,960
所以我就思考了這件事。

371
00:13:31,180 --> 00:13:32,960
and so I thought about that.

372
00:13:32,960 --> 00:13:36,800
我覺得我多少能理解她的觀點。

372
00:13:32,960 --> 00:13:36,800
And I feel like I can somewhat see that point.

373
00:13:36,800 --> 00:13:40,840
但我認為人工智慧的發展方式

373
00:13:36,800 --> 00:13:40,840
But I think the way that, like AI, is developing

374
00:13:40,840 --> 00:13:41,780
至少在人工智慧的使用上

374
00:13:40,840 --> 00:13:41,780
at least AI usage

375
00:13:41,780 --> 00:13:43,420
我覺得我們已經開始注意到了。

375
00:13:41,780 --> 00:13:43,420
I feel like we're noticing.

376
00:13:43,420 --> 00:13:45,900
至少從經驗來看，這有點像是一種轉變

376
00:13:43,420 --> 00:13:45,900
Kind of like a shift, at least, you know, anecdotally

377
00:13:45,900 --> 00:13:47,440
從我所見，是一種轉變，逐漸遠離

377
00:13:45,900 --> 00:13:47,440
from what I see, a shift away from.

378
00:13:47,440 --> 00:13:50,720
不再把它當作作弊的工具，或者說

378
00:13:47,440 --> 00:13:50,720
Kind of using it as like a cheat use case, or, you know

379
00:13:50,720 --> 00:13:54,440
用來走捷徑的用途

379
00:13:50,720 --> 00:13:54,440
a use case to like, you know, take the easier path,

380
00:13:54,440 --> 00:13:57,000
而是更多用來提升

380
00:13:54,440 --> 00:13:57,000
but more so to just like, improve,

381
00:13:57,000 --> 00:13:58,820
你的生產力和學習效率

381
00:13:57,000 --> 00:13:58,820
like your productivity, and like, learning.

382
00:13:58,820 --> 00:14:00,880
我認為，從這個角度來看

382
00:13:58,820 --> 00:14:00,880
And I think, like, through that lens

383
00:14:00,880 --> 00:14:01,920
我覺得那個建議

383
00:14:00,880 --> 00:14:01,920
I think that advice

384
00:14:01,920 --> 00:14:02,900
是我會給出的

384
00:14:01,920 --> 00:14:02,900
like I would give

385
00:14:02,900 --> 00:14:06,680
就是要有內在的動力

385
00:14:02,900 --> 00:14:06,680
is to kind of, like, you know, have like an intrinsic motivation.

386
00:14:06,680 --> 00:14:08,060
這說起來不容易

386
00:14:06,680 --> 00:14:08,060
I mean, that's hard advice

387
00:14:08,060 --> 00:14:09,500
但我覺得你知道的

387
00:14:08,060 --> 00:14:09,500
but I feel like, you know

388
00:14:09,500 --> 00:14:11,860
你必須有學習的動機

388
00:14:09,500 --> 00:14:11,860
you just have like that motivation to learn.

389
00:14:11,860 --> 00:14:15,360
我認為你可以善用 Chat GPT 來幫助你

389
00:14:11,860 --> 00:14:15,360
And I think you can really use Chat GPT to kind of help you

390
00:14:15,360 --> 00:14:17,720
但同時不要把它當成拐杖依賴

390
00:14:15,360 --> 00:14:17,720
but don't use it as a crutch at the same time.

391
00:14:17,720 --> 00:14:20,460
我覺得你必須對自己負責。

391
00:14:17,720 --> 00:14:20,460
I think you just have to hold yourself accountable.

392
00:14:20,460 --> 00:14:23,020
不要只是用它來取得答案，

392
00:14:20,460 --> 00:14:23,020
And kind of not using it to get the answer

393
00:14:23,020 --> 00:14:25,900
而是用來幫助你更好地理解事物。

393
00:14:23,020 --> 00:14:25,900
and more so to help you kind of understand things better.

394
00:14:25,900 --> 00:14:28,320
但我覺得這很難，

394
00:14:25,900 --> 00:14:28,320
But I think that's that's hard

395
00:14:28,320 --> 00:14:29,180
尤其是在你還是高中生的時候。

395
00:14:28,320 --> 00:14:29,180
when you're in high school.

396
00:14:29,180 --> 00:14:32,640
不過我認為這能讓你穿越雜音，

396
00:14:29,180 --> 00:14:32,640
But I think it allows you to kind of cut through the noise

397
00:14:32,640 --> 00:14:34,880
去做你真正喜歡的事，而不是，

397
00:14:32,640 --> 00:14:34,880
and do things that you actually enjoy versus, you know

398
00:14:34,880 --> 00:14:38,140
那些被分配給你的事情。

398
00:14:34,880 --> 00:14:38,140
things that are kind of like, assigned to you.

399
00:14:38,140 --> 00:14:40,480
是的，絕對如此。

399
00:14:38,140 --> 00:14:40,480
Yes, yeah, definitely.

400
00:14:40,480 --> 00:14:44,340
我想對現在的高中生說，要保持警惕。

400
00:14:40,480 --> 00:14:44,340
I'd say for high school students right now, I'd say, be wary.

401
00:14:44,340 --> 00:14:48,400
我在這段時間裡注意到了這點。

401
00:14:44,340 --> 00:14:48,400
I've noticed that throughout my time.

402
00:14:48,400 --> 00:14:50,660
我的意思是，我是在高中開始接觸這些的，

402
00:14:48,400 --> 00:14:50,660
I mean, I started learning about in high school

403
00:14:50,660 --> 00:14:52,200
我確實看到了其中的危險。

403
00:14:50,660 --> 00:14:52,200
and I definitely saw the dangers of it.

404
00:14:52,200 --> 00:14:56,140
因為自滿和習慣了它，做了所有那些事情。

404
00:14:52,200 --> 00:14:56,140
With complacency and getting used to it, doing all the things.

405
00:14:56,140 --> 00:14:58,060
然後當你

405
00:14:56,140 --> 00:14:58,060
And then when you

406
00:14:58,060 --> 00:14:59,280
當你真正面對考試時

406
00:14:58,060 --> 00:14:59,280
when you actually get to the test

407
00:14:59,280 --> 00:15:01,360
當你真正挑戰自己所學的時候

407
00:14:59,280 --> 00:15:01,360
and when you actually get to challenge what you've learned

408
00:15:01,360 --> 00:15:02,900
你會發現自己掌握的知識變少了。

408
00:15:01,360 --> 00:15:02,900
you have less of that knowledge.

409
00:15:02,900 --> 00:15:05,800
但我注意到這幾個月來

409
00:15:02,900 --> 00:15:05,800
But I've noticed that these past few months

410
00:15:05,800 --> 00:15:08,300
我真的把 Chat GPT 推到了極限

410
00:15:05,800 --> 00:15:08,300
I've really kind of pushed Chat GPT to its limits

411
00:15:08,300 --> 00:15:09,760
也把自己逼到了極限。

411
00:15:08,300 --> 00:15:09,760
and pushed myself to my limits.

412
00:15:09,760 --> 00:15:13,380
我發現自己能完成的事情比以前多得多。

412
00:15:09,760 --> 00:15:13,380
And I've noticed that I can accomplish a lot more than I used to.

413
00:15:13,380 --> 00:15:16,740
我記得高中時我在做一個專案

413
00:15:13,380 --> 00:15:16,740
I remember in high school I was I was working on some project

414
00:15:16,740 --> 00:15:19,820
那是一個影像分類器專案。

414
00:15:16,740 --> 00:15:19,820
it was a image classifier project.

415
00:15:19,820 --> 00:15:25,200
我花了很長時間，閱讀大量文件才真正學會

415
00:15:19,820 --> 00:15:25,200
And it took me a long time, a lot of documents to to actually learn

416
00:15:25,200 --> 00:15:26,280
並了解如何完成這件事。

416
00:15:25,200 --> 00:15:26,280
and understand how to do this.

417
00:15:26,280 --> 00:15:27,020
但現在你可以

417
00:15:26,280 --> 00:15:27,020
But now you can

418
00:15:27,020 --> 00:15:29,940
利用 AI，實際上能更快完成這項工作。

418
00:15:27,020 --> 00:15:29,940
you can utilize AI and actually get this done much faster.

419
00:15:29,940 --> 00:15:33,060
還能改進並產出成果，

419
00:15:29,940 --> 00:15:33,060
And improve on it and produce it

420
00:15:33,060 --> 00:15:38,460
持續反覆優化，做出更好的產品。

420
00:15:33,060 --> 00:15:38,460
and continue and and and iterate on it to make a much better product.

421
00:15:38,460 --> 00:15:42,740
我會鼓勵高中生找到自己的熱情

421
00:15:38,460 --> 00:15:42,740
And I would encourage high schoolers to to find a passion

422
00:15:42,740 --> 00:15:44,580
並且真正投入其中

422
00:15:42,740 --> 00:15:44,580
and and really dive into that

423
00:15:44,580 --> 00:15:46,840
並且充分發揮它的價值

423
00:15:44,580 --> 00:15:46,840
and and get the most out of it.

424
00:15:46,840 --> 00:15:47,860
但同時

424
00:15:46,840 --> 00:15:47,860
But at the same time

425
00:15:47,860 --> 00:15:51,920
也要謹慎並堅持老師教導的概念

425
00:15:47,860 --> 00:15:51,920
be wary and stick to the concepts that teachers are teaching you.

426
00:15:51,920 --> 00:15:55,040
不要因為AI能幫你做就輕易否定它

426
00:15:51,920 --> 00:15:55,040
And and don't just automatically dismiss it

427
00:15:55,040 --> 00:15:56,440
因為AI可以替你完成

427
00:15:55,040 --> 00:15:56,440
because AI can do it for you.

428
00:15:56,440 --> 00:16:01,120
你希望老師現在怎麼教學？

428
00:15:56,440 --> 00:16:01,120
How would you like teachers to be teaching now?

429
00:16:01,120 --> 00:16:03,820
我認為，這是大膽的看法

429
00:16:01,120 --> 00:16:03,820
I think, and and this is a bold take.

430
00:16:03,820 --> 00:16:07,520
但我覺得最終我們會達到一個階段

430
00:16:03,820 --> 00:16:07,520
But I think that eventually we're gonna get to a point

431
00:16:07,520 --> 00:16:12,420
教育和講課完全由AI來完成

431
00:16:07,520 --> 00:16:12,420
where we're education and the lectures are fully done by AI.

432
00:16:12,420 --> 00:16:15,640
我認為所有的內容

432
00:16:12,420 --> 00:16:15,640
I think all all the content that

433
00:16:15,640 --> 00:16:17,740
我們自己學習的內容都會由AI提供

433
00:16:15,640 --> 00:16:17,740
we're consuming ourselves will be given by AI.

434
00:16:17,740 --> 00:16:20,360
因為我注意到，根據經驗

434
00:16:17,740 --> 00:16:20,360
Because I've noticed, anecdotally

435
00:16:20,360 --> 00:16:22,480
每個學生的學習方式都不同

435
00:16:20,360 --> 00:16:22,480
that each student learns differently.

436
00:16:22,480 --> 00:16:24,860
當人工智慧發展到某個階段時，

436
00:16:22,480 --> 00:16:24,860
And once AI gets to a point

437
00:16:24,860 --> 00:16:28,760
會有一種多模態系統，可以讓你視覺化事物。

437
00:16:24,860 --> 00:16:28,760
where there's like a multimodal system where you can visualize things.

438
00:16:28,760 --> 00:16:32,260
它還會提供你像是YouTube影片，

438
00:16:28,760 --> 00:16:32,260
And and it'll provide you, like YouTube videos

439
00:16:32,260 --> 00:16:33,840
這些影片都是它自己創造的。

439
00:16:32,260 --> 00:16:33,840
that it creates itself.

440
00:16:33,840 --> 00:16:35,540
我們將會到達一個階段，

440
00:16:33,840 --> 00:16:35,540
We're gonna get to a point

441
00:16:35,540 --> 00:16:37,280
學生能透過人工智慧學習得更好。

441
00:16:35,540 --> 00:16:37,280
where students learn better through AI.

442
00:16:37,280 --> 00:16:41,680
我認為老師屆時會更專注於社交方面，

442
00:16:37,280 --> 00:16:41,680
And I think teachers will then be focused more on like the social,

443
00:16:41,680 --> 00:16:42,620
社交技巧，

443
00:16:41,680 --> 00:16:42,620
social skills,

444
00:16:42,620 --> 00:16:47,300
社會資本、指導，以及如何使用人工智慧。

444
00:16:42,620 --> 00:16:47,300
social capital, part of it, and mentorship, and how to use AI.

445
00:16:47,300 --> 00:16:47,760
因為，

445
00:16:47,300 --> 00:16:47,760
Because.

446
00:16:48,580 --> 00:16:51,680
我想大家也都擔心工作問題。

446
00:16:48,580 --> 00:16:51,680
I think everyone's worried about jobs as well.

447
00:16:51,680 --> 00:16:57,400
但適應力是學生和

447
00:16:51,680 --> 00:16:57,400
But adaptation is the new job security for for students and

448
00:16:57,400 --> 00:17:02,840
想進入職場的新鮮人新的工作保障。

448
00:16:57,400 --> 00:17:02,840
for up-and-coming people that are trying to enter the job market.

449
00:17:02,840 --> 00:17:04,800
這讓我有點好奇，我認為

449
00:17:02,840 --> 00:17:04,800
That's where I kind of wonder those I think

450
00:17:04,800 --> 00:17:07,520
人工智慧在陪伴、輔導等方面有很棒的角色。

450
00:17:04,800 --> 00:17:07,520
that AI has this great place as a companion, as tutor and whatnot.

451
00:17:07,520 --> 00:17:09,540
很多教材在某種程度上取代了教科書。

451
00:17:07,520 --> 00:17:09,540
And a lot of material kind of replace the textbook.

452
00:17:09,540 --> 00:17:11,180
但你提到了「指導」這個詞。

452
00:17:09,540 --> 00:17:11,180
But you brought up the word mentorship.

453
00:17:11,180 --> 00:17:11,980
我只是覺得，

453
00:17:11,180 --> 00:17:11,980
I just see.

454
00:17:11,980 --> 00:17:13,180
就我個人而言，

454
00:17:11,980 --> 00:17:13,180
Personally speaking,

455
00:17:13,180 --> 00:17:15,220
能夠……對我來說非常有價值，

455
00:17:13,180 --> 00:17:15,220
I think there's so much value to me be able to

456
00:17:15,220 --> 00:17:17,020
去找一位電機工程師，

456
00:17:15,220 --> 00:17:17,020
go to an electrical engineer that

457
00:17:17,020 --> 00:17:18,580
他真的從事過電機工程的工作。

457
00:17:17,020 --> 00:17:18,580
actually had an electrical engineering job.

458
00:17:18,580 --> 00:17:18,860
你知道，

458
00:17:18,580 --> 00:17:18,860
You know,

459
00:17:18,860 --> 00:17:21,560
一位真正做過相關工作的電腦科學老師。

459
00:17:18,860 --> 00:17:21,560
a computer science teacher who actually has done the work of it, too.

460
00:17:21,560 --> 00:17:24,180
所以你覺得未來會是這樣嗎？

460
00:17:21,560 --> 00:17:24,180
And so do you see that kind of a sort of the future is?

461
00:17:24,180 --> 00:17:27,040
這個想法是，AI會是非常優秀的助教，

461
00:17:24,180 --> 00:17:27,040
The idea is that the AI is the really good TA

462
00:17:27,040 --> 00:17:28,020
也是教科書，

462
00:17:27,040 --> 00:17:28,020
it's the textbook

463
00:17:28,020 --> 00:17:29,360
它會做這類事情。

463
00:17:28,020 --> 00:17:29,360
it's doing this sort of stuff.

464
00:17:29,360 --> 00:17:33,540
但我想你還是希望人類能保持在核心位置。

464
00:17:29,360 --> 00:17:33,540
But then you want to still keep humans in the center, I guess.

465
00:17:33,540 --> 00:17:36,200
就教學和學習而言，

465
00:17:33,540 --> 00:17:36,200
Like in terms of like, teaching and learning

466
00:17:36,200 --> 00:17:38,540
我覺得教你的人顯然會影響你

466
00:17:36,200 --> 00:17:38,540
I feel like obviously who teaches you

467
00:17:38,540 --> 00:17:40,420
學到什麼以及你如何喜歡

467
00:17:38,540 --> 00:17:40,420
impacts what you learn and how you like

468
00:17:40,420 --> 00:17:41,860
在日後生活中應用它。

468
00:17:40,420 --> 00:17:41,860
apply it on later on in life.

469
00:17:41,860 --> 00:17:43,960
而且，我覺得你可以，你知道，

469
00:17:41,860 --> 00:17:43,960
And like, I feel like you can, you know,

470
00:17:43,960 --> 00:17:46,400
學習生物學中相同的概念，比如有絲分裂

470
00:17:43,960 --> 00:17:46,400
learn the same concept in biology, like mitosis

471
00:17:46,400 --> 00:17:49,260
概念是一樣的，但教你的人不同？

471
00:17:46,400 --> 00:17:49,260
it's the same concept, but like, who teaches you?

472
00:17:49,260 --> 00:17:51,540
這真的會改變很多，你知道嗎？

472
00:17:49,260 --> 00:17:51,540
Like, kind of really changes that, you know?

473
00:17:51,540 --> 00:17:53,480
所以我認為這就是學習中人性面向的所在，

473
00:17:51,540 --> 00:17:53,480
So I think that's where the human aspect of, like

474
00:17:53,480 --> 00:17:54,560
真正發揮作用的地方，

474
00:17:53,480 --> 00:17:54,560
learning really comes in

475
00:17:54,560 --> 00:17:56,060
也是讓學習變得重要的原因，

475
00:17:54,560 --> 00:17:56,060
and it's what makes it important

476
00:17:56,060 --> 00:17:57,320
我認為這就是人與人之間的連結。

476
00:17:56,060 --> 00:17:57,320
and I think that human connection.

477
00:17:57,320 --> 00:17:59,600
所以我覺得這有點像是，

477
00:17:57,320 --> 00:17:59,600
So I think it's it's somewhat like

478
00:17:59,600 --> 00:18:00,920
我想像的是，

478
00:17:59,600 --> 00:18:00,920
I think I envision, like,

479
00:18:00,920 --> 00:18:03,580
這兩者的某種混合體，

479
00:18:00,920 --> 00:18:03,580
somewhat of like a hybrid kind of those two things

480
00:18:04,060 --> 00:18:06,740
我們可以從人工智慧那裡獲得一個標準。

480
00:18:04,060 --> 00:18:06,740
where I think we can get a standard from AI.

481
00:18:06,740 --> 00:18:07,760
因為它很有幫助

481
00:18:06,740 --> 00:18:07,760
Because it's helpful

482
00:18:07,760 --> 00:18:08,680
因為它很簡單

482
00:18:07,760 --> 00:18:08,680
because it's easy

483
00:18:08,680 --> 00:18:11,620
因為它很容易接觸，然後有點這樣。

483
00:18:08,680 --> 00:18:11,620
because it's like, you know, accessible, and then kind of.

484
00:18:11,620 --> 00:18:13,700
那種指導的元素帶有個人觸感

484
00:18:11,620 --> 00:18:13,700
That mentorship element comes in with the touch

485
00:18:13,700 --> 00:18:15,220
還有你如何應用它

485
00:18:13,700 --> 00:18:15,220
and like how you apply it

486
00:18:15,220 --> 00:18:17,300
尤其是你如何從倫理角度思考它

486
00:18:15,220 --> 00:18:17,300
and especially how you think about it in terms of ethics.

487
00:18:17,300 --> 00:18:20,160
我想我們可能會開始轉變，遠離

487
00:18:17,300 --> 00:18:20,160
I think maybe we'll start shifting away from

488
00:18:20,160 --> 00:18:21,200
只是學習怎麼做，

488
00:18:20,160 --> 00:18:21,200
just like learning how to do,

489
00:18:21,200 --> 00:18:22,520
做、做，然後思考，像是

489
00:18:21,200 --> 00:18:22,520
do, do and thinking about, like

490
00:18:22,520 --> 00:18:23,900
這對人們有什麼影響？

490
00:18:22,520 --> 00:18:23,900
how does this impact people?

491
00:18:23,900 --> 00:18:26,580
你知道，我們如何透過這些技術幫助他人？

491
00:18:23,900 --> 00:18:26,580
You know, how can we help others through these technologies?

492
00:18:26,580 --> 00:18:29,700
我認為這種理解上的轉變

492
00:18:26,580 --> 00:18:29,700
And I think that shift in understanding is

493
00:18:29,700 --> 00:18:31,980
正是人性化觸感和指導會發揮作用的地方

493
00:18:29,700 --> 00:18:31,980
where the human touch and mentorship will also come in.

494
00:18:31,980 --> 00:18:33,340
所以我想，你知道

494
00:18:31,980 --> 00:18:33,340
So I think, you know

495
00:18:33,340 --> 00:18:35,220
擁有那種混合模式，真的

495
00:18:33,340 --> 00:18:35,220
having that hybrid model is, you know

496
00:18:35,220 --> 00:18:36,460
我對未來的某種想像

496
00:18:35,220 --> 00:18:36,460
something that I envision for the future

497
00:18:36,460 --> 00:18:37,920
這可能會非常成功。

497
00:18:36,460 --> 00:18:37,920
and that's something that could be really successful.

498
00:18:38,920 --> 00:18:41,040
你認為人工智慧的未來會走向何方？

498
00:18:38,920 --> 00:18:41,040
Where do you see the future headed with AI?

499
00:18:42,560 --> 00:18:44,980
當然，就像任何技術一樣，

499
00:18:42,560 --> 00:18:44,980
Definitely, just like how any technology works,

500
00:18:44,980 --> 00:18:47,980
明顯地會簡化許多工作。

500
00:18:44,980 --> 00:18:47,980
just like simplifying a lot of tasks, obviously.

501
00:18:47,980 --> 00:18:52,480
我想很多人可能會擔心

501
00:18:47,980 --> 00:18:52,480
And I think a lot of people maybe are worried about

502
00:18:52,480 --> 00:18:55,080
或是在思考人工智慧的意識問題

502
00:18:52,480 --> 00:18:55,080
or are thinking about consciousness in AI

503
00:18:55,080 --> 00:18:58,060
但我不認為

503
00:18:55,080 --> 00:18:58,060
and I don't think

504
00:18:58,060 --> 00:19:00,740
人工智慧在很長一段時間內會產生意識。

504
00:18:58,060 --> 00:19:00,740
that AI will will generate consciousness for a long time.

505
00:19:00,740 --> 00:19:05,520
但我認為未來，

505
00:19:00,740 --> 00:19:05,520
But I think that like in the future, like,

506
00:19:05,520 --> 00:19:08,140
對，就像部署，像是，

506
00:19:05,520 --> 00:19:08,140
right, like deploying, like, you know, like.

507
00:19:08,140 --> 00:19:09,640
某些公司已經開發出

507
00:19:08,140 --> 00:19:09,640
Certain companies have already developed,

508
00:19:09,640 --> 00:19:12,760
可部署的軟體工程師AI代理人，

508
00:19:09,640 --> 00:19:12,760
like deployable software engineers as an AI agent,

509
00:19:12,760 --> 00:19:17,080
但部署一個完整的單一代理人，

509
00:19:12,760 --> 00:19:17,080
but like deploying, like, like a full, just like a one agent.

510
00:19:17,080 --> 00:19:19,440
能夠協調軟體工程師的工作，

510
00:19:17,080 --> 00:19:19,440
That'll orchestrate, like a software engineer,

511
00:19:19,740 --> 00:19:22,840
行銷人員、設計師，還有很多角色。

511
00:19:19,740 --> 00:19:22,840
a marketer, a designer, so many things.

512
00:19:22,840 --> 00:19:24,900
我覺得未來就是朝那個方向發展。

512
00:19:22,840 --> 00:19:24,900
I think I think that's where the future is going.

513
00:19:25,720 --> 00:19:26,860
人類將會站在哪個位置。

513
00:19:25,720 --> 00:19:26,860
Where humans stand in.

514
00:19:26,860 --> 00:19:28,460
這點絕對是這樣。

514
00:19:26,860 --> 00:19:28,460
That definitely like.

515
00:19:29,360 --> 00:19:31,080
他們需要持續參與其中。

515
00:19:29,360 --> 00:19:31,080
They'll need to, like, stay in the loop.

516
00:19:31,080 --> 00:19:32,280
我認為單純放任

516
00:19:31,080 --> 00:19:32,280
I think just letting

517
00:19:32,280 --> 00:19:34,640
讓人工智慧自由發展是危險的，

517
00:19:32,280 --> 00:19:34,640
letting AI go is is dangerous

518
00:19:34,640 --> 00:19:37,840
而且在未來一段時間內也不切實際。

518
00:19:34,640 --> 00:19:37,840
and it isn't feasible for for the upcoming future..

519
00:19:38,700 --> 00:19:41,380
你覺得十年後你的工作會是什麼樣子？

519
00:19:38,700 --> 00:19:41,380
What do you think your job is gonna look like 10 years from now?

520
00:19:42,020 --> 00:19:43,040
我覺得會是，會是

520
00:19:42,020 --> 00:19:43,040
I think it'll, it'll be

521
00:19:43,040 --> 00:19:44,340
一個混合的狀態。

521
00:19:43,040 --> 00:19:44,340
it'll be a mixed bag.

522
00:19:44,340 --> 00:19:45,680
我想是因為我覺得

522
00:19:44,340 --> 00:19:45,680
I think because I think

523
00:19:45,680 --> 00:19:50,300
我的工作很大部分是對外、面對人的，

523
00:19:45,680 --> 00:19:50,300
because my job is pretty like kind of external facing people facing

524
00:19:50,300 --> 00:19:53,060
我覺得現在有種緊張感，

524
00:19:50,300 --> 00:19:53,060
I think right now, kind of the tension.

525
00:19:53,060 --> 00:19:55,180
或者說我看到的人工智慧使用者群體，

525
00:19:53,060 --> 00:19:55,180
Or the tribe I see with like AI usage

526
00:19:55,180 --> 00:19:57,480
在創意領域裡，大家常說，喔，像是

526
00:19:55,180 --> 00:19:57,480
and like creative fields is like, oh, like

527
00:19:57,480 --> 00:19:59,920
我不想讀那些像是由

527
00:19:57,480 --> 00:19:59,920
I don't want to read something made by like,

528
00:19:59,920 --> 00:20:02,040
ChatGBT 之類的東西寫的文章。

528
00:19:59,920 --> 00:20:02,040
ChatGBT or like, you know.

529
00:20:02,040 --> 00:20:05,060
而且很多人批評破折號的使用，

529
00:20:02,040 --> 00:20:05,060
And it's like a lot of critique on like the usage of em dashes,

530
00:20:05,060 --> 00:20:05,840
還有逗號的用法。

530
00:20:05,060 --> 00:20:05,840
like the comma.

531
00:20:06,320 --> 00:20:09,220
我也只想讀那些經過深入研究的文章。

531
00:20:06,320 --> 00:20:09,220
I want, I only want to read things by by deep research, too.

532
00:20:09,220 --> 00:20:12,460
我現在常用破折號，就是為了惹人生氣。

532
00:20:09,220 --> 00:20:12,460
I use em dashes all the time now, just to annoy people.

533
00:20:13,200 --> 00:20:14,860
是啊，所以，嗯，

533
00:20:13,200 --> 00:20:14,860
Yeah, and so like, yeah

534
00:20:14,860 --> 00:20:16,920
有很多人批評說，

534
00:20:14,860 --> 00:20:16,920
there's like a lot of like kind of critique about people,

535
00:20:16,920 --> 00:20:18,500
他們不想讀那些內容，

535
00:20:16,920 --> 00:20:18,500
like not wanting to read content.

536
00:20:18,500 --> 00:20:20,420
因為是 AI 生成的。

536
00:20:18,500 --> 00:20:20,420
That's like AI generated and like.

537
00:20:20,420 --> 00:20:21,900
我在社群媒體上看到這種情況，

537
00:20:20,420 --> 00:20:21,900
I see that like on social media

538
00:20:21,900 --> 00:20:22,880
我也在

538
00:20:21,900 --> 00:20:22,880
I see that in like

539
00:20:22,880 --> 00:20:24,760
比較小眾的社群裡看到，像是 Substack，

539
00:20:22,880 --> 00:20:24,760
more niche communities like substack

540
00:20:24,760 --> 00:20:27,760
我覺得我至少讀過十五篇 Substack 文章。

540
00:20:24,760 --> 00:20:27,760
I feel like I've read like at least 15 substack articles

541
00:20:27,760 --> 00:20:29,620
有點像是在批評「like」的用法，

541
00:20:27,760 --> 00:20:29,620
like kind of bashing the uses of like,

542
00:20:29,620 --> 00:20:30,200
還有破折號的使用。

542
00:20:29,620 --> 00:20:30,200
em dashes.

543
00:20:30,440 --> 00:20:31,920
是的，確實存在這種情況。

543
00:20:30,440 --> 00:20:31,920
Yeah, it's there is.

544
00:20:31,920 --> 00:20:34,540
反應很有趣，但對我來說，這讓人覺得

544
00:20:31,920 --> 00:20:34,540
Reactions are interesting, but like to me, it puts.

545
00:20:34,540 --> 00:20:37,440
我現在更重視自傳性質的內容。

545
00:20:34,540 --> 00:20:37,440
I put more value now on autobiographical stuff.

546
00:20:37,440 --> 00:20:40,200
如果有人寫的是那種任何人都能寫的普通東西，

546
00:20:37,440 --> 00:20:40,200
If somebody's writing a generic thing that anything could rate,

547
00:20:40,200 --> 00:20:40,700
你懂的，

547
00:20:40,200 --> 00:20:40,700
you know

548
00:20:40,700 --> 00:20:41,540
任何人都能創作出來，

548
00:20:40,700 --> 00:20:41,540
could anything could create

549
00:20:41,540 --> 00:20:42,980
那我就不會那麼在意了。

549
00:20:41,540 --> 00:20:42,980
then I don't really care as much.

550
00:20:42,980 --> 00:20:43,760
但如果有人是這樣的，

550
00:20:42,980 --> 00:20:43,760
But if somebody's like

551
00:20:43,760 --> 00:20:45,980
這是我做這件事的經驗，

551
00:20:43,760 --> 00:20:45,980
here's my experience doing this

552
00:20:45,980 --> 00:20:48,020
對我個人來說價值就高得多。

552
00:20:45,980 --> 00:20:48,020
that has way more value to me personally.

553
00:20:48,220 --> 00:20:49,820
這其實就是關於，

553
00:20:48,220 --> 00:20:49,820
It's just really about like

554
00:20:49,820 --> 00:20:50,580
我認為未來，

554
00:20:49,820 --> 00:20:50,580
I think in the future

555
00:20:50,580 --> 00:20:51,720
至少在行銷領域，

555
00:20:50,580 --> 00:20:51,720
at least in like marketing

556
00:20:51,720 --> 00:20:53,360
這大概就是在調和那種緊張感

556
00:20:51,720 --> 00:20:53,360
it's like probably just navigating that tension

557
00:20:53,360 --> 00:20:56,500
並觀察觀眾喜歡共鳴什麼。

557
00:20:53,360 --> 00:20:56,500
and seeing like what audiences like kind of resonate to.

558
00:20:56,500 --> 00:20:58,040
所以也許誰知道，五年後會怎樣。

558
00:20:56,500 --> 00:20:58,040
So maybe who knows, in like five years.

559
00:20:58,040 --> 00:20:59,980
我覺得人工智慧生成的內容可能會

559
00:20:58,040 --> 00:20:59,980
I feel like AI generated content might

560
00:20:59,980 --> 00:21:01,520
成為常態並被接受。

560
00:20:59,980 --> 00:21:01,520
be like the norm and like acceptable.

561
00:21:01,520 --> 00:21:03,000
而我們作為

561
00:21:01,520 --> 00:21:03,000
And like us as like

562
00:21:03,000 --> 00:21:05,020
行銷人員，正推動著

562
00:21:03,000 --> 00:21:05,020
marketers are kind of like pushing in that

563
00:21:05,020 --> 00:21:06,680
創意輸入，並且對此

563
00:21:05,020 --> 00:21:06,680
creative input and for that,

564
00:21:06,680 --> 00:21:08,080
也就是說，人工智慧生成的產出。

564
00:21:06,680 --> 00:21:08,080
like, you know, AI generated output.

565
00:21:08,080 --> 00:21:11,260
但我覺得那還是遙遠的未來。

565
00:21:08,080 --> 00:21:11,260
But I feel like that's that's still long years ahead.

566
00:21:11,260 --> 00:21:11,560
不過，

566
00:21:11,260 --> 00:21:11,560
But.

567
00:21:12,080 --> 00:21:15,340
你最大的恐懼是什麼？

567
00:21:12,080 --> 00:21:15,340
What is your biggest fear?

568
00:21:16,400 --> 00:21:17,980
我先說我的，好嗎？

568
00:21:16,400 --> 00:21:17,980
I'll give you mine, okay

569
00:21:17,980 --> 00:21:20,420
我認為這些是令人驚嘆的技術

569
00:21:17,980 --> 00:21:20,420
I think these are incredible technologies

570
00:21:20,420 --> 00:21:21,700
我覺得有些事情。

570
00:21:20,420 --> 00:21:21,700
I think there's.

571
00:21:21,700 --> 00:21:23,380
我認為每個人都有權利

571
00:21:21,700 --> 00:21:23,380
I think everybody has the right to

572
00:21:23,380 --> 00:21:24,860
以他們想要的方式來接近這件事。

572
00:21:23,380 --> 00:21:24,860
kind of approach this however they want.

573
00:21:24,860 --> 00:21:27,220
如果有人說我要花時間，花自己的時間，那也沒問題。

573
00:21:24,860 --> 00:21:27,220
And if somebody says I'm taking time, my time, that's fine.

574
00:21:27,220 --> 00:21:29,860
我擔心的是，我認為這裡有很多價值。

574
00:21:27,220 --> 00:21:29,860
My concern is, I think there's a lot of value.

575
00:21:29,860 --> 00:21:31,260
我覺得這是一個非常棒的學習工具，

575
00:21:29,860 --> 00:21:31,260
I think it's incredible learning tool

576
00:21:31,260 --> 00:21:34,580
而且我發現人們可以很快跟上進度。

576
00:21:31,260 --> 00:21:34,580
and and I find that people can get up to speed very quickly.

577
00:21:34,580 --> 00:21:36,960
花六個月時間學習這些東西，

577
00:21:34,580 --> 00:21:36,960
In six months, I think six months time spending this stuff.

578
00:21:36,960 --> 00:21:38,580
你就能進入一家公司，

578
00:21:36,960 --> 00:21:38,580
You can then go into a company

579
00:21:38,580 --> 00:21:39,800
並且能做很多事情，

579
00:21:38,580 --> 00:21:39,800
and you can do a lot of how

580
00:21:39,800 --> 00:21:41,420
像是創立公司和處理這些事。

580
00:21:39,800 --> 00:21:41,420
you can create a company and do this stuff.

581
00:21:41,420 --> 00:21:44,340
我擔心的是人們猶豫太多，

581
00:21:41,420 --> 00:21:44,340
And my concern is that people are kind of hesitating too much.

582
00:21:45,060 --> 00:21:46,620
不敢提出問題。

582
00:21:45,060 --> 00:21:46,620
And not asking a question.

583
00:21:46,620 --> 00:21:48,140
也許我們沒有做足夠的教學，

583
00:21:46,620 --> 00:21:48,140
And maybe we're not doing enough to teach

584
00:21:48,140 --> 00:21:50,000
或是沒有充分傳達我們所做的一切，

584
00:21:48,140 --> 00:21:50,000
or communicate everything we're doing there

585
00:21:50,000 --> 00:21:52,360
我擔心的是人們會錯失良機。

585
00:21:50,000 --> 00:21:52,360
and I fear my fear is people missing out.

586
00:21:53,260 --> 00:21:56,040
我有點持相反的看法。

586
00:21:53,260 --> 00:21:56,040
I kind of have like a like, an opposite, opposite view.

587
00:21:56,040 --> 00:21:58,640
我覺得也許特別是

587
00:21:56,040 --> 00:21:58,640
I think that maybe maybe specifically

588
00:21:58,640 --> 00:22:02,260
因為我正在加入這種教育，有些人

588
00:21:58,640 --> 00:22:02,260
because I'm I'm inserting this education that some

589
00:22:02,260 --> 00:22:04,100
會試著去

589
00:22:02,260 --> 00:22:04,100
people will try to like,

590
00:22:04,100 --> 00:22:04,960
找出繞過的方法。

590
00:22:04,100 --> 00:22:04,960
find a way around.

591
00:22:04,960 --> 00:22:08,280
像是所有傳統教育中關於 AI 的部分

591
00:22:04,960 --> 00:22:08,280
Like everything that's traditionally education with AI

592
00:22:08,280 --> 00:22:11,200
教育裡面有很多漏洞。

592
00:22:08,280 --> 00:22:11,200
and there are a lot of loopholes within education.

593
00:22:11,200 --> 00:22:14,020
我擔心的是

593
00:22:11,200 --> 00:22:14,020
And my fear is that

594
00:22:14,020 --> 00:22:16,040
你會進入職場，

594
00:22:14,020 --> 00:22:16,040
that you'll you'll get to the to the job market.

595
00:22:16,040 --> 00:22:20,780
而且基本上，面試你的人

595
00:22:16,040 --> 00:22:20,780
And and essentially, you, the the people interviewing.

596
00:22:20,780 --> 00:22:22,520
不會問你說，喔，

596
00:22:20,780 --> 00:22:22,520
You are not gonna ask like, oh, like

597
00:22:22,520 --> 00:22:23,300
如果你遇到這個問題，

597
00:22:22,520 --> 00:22:23,300
if you get this question

598
00:22:23,300 --> 00:22:25,020
你會怎麼用 AI 來解決？

598
00:22:23,300 --> 00:22:25,020
how are you gonna put into AI and solve it?

599
00:22:25,020 --> 00:22:26,780
因為 AI 可以解這個問題？

599
00:22:25,020 --> 00:22:26,780
because AI can solve this question?

600
00:22:26,780 --> 00:22:28,940
不管你有什麼傳統的程式設計題目。

600
00:22:26,780 --> 00:22:28,940
Whatever traditional coding question that you have.

601
00:22:28,940 --> 00:22:30,840
但他們真正尋找的是概念

601
00:22:28,940 --> 00:22:30,840
But they're really looking for the concepts

602
00:22:30,840 --> 00:22:33,760
以及你如何適應和解決問題

602
00:22:30,840 --> 00:22:33,760
and for how you how you adapt and solve problems

603
00:22:33,760 --> 00:22:35,680
因為他們也需要那種人性面向

603
00:22:33,760 --> 00:22:35,680
because they need that human aspect as well

604
00:22:35,680 --> 00:22:37,160
人性是不會消失的。

604
00:22:35,680 --> 00:22:37,160
like humanity is not going away.

605
00:22:37,160 --> 00:22:41,820
而我擔心的是，

605
00:22:37,160 --> 00:22:41,820
And my fear is that that,

606
00:22:41,820 --> 00:22:44,340
這種工具的使用會走得太遠。

606
00:22:41,820 --> 00:22:44,340
like the use of this tool, is gonna go too far.

607
00:22:44,340 --> 00:22:46,800
他們會意識到，傳統教育

607
00:22:44,340 --> 00:22:46,800
Where they realize that, like traditional education,

608
00:22:46,800 --> 00:22:47,700
仍然很重要，

608
00:22:46,800 --> 00:22:47,700
is still important,

609
00:22:47,700 --> 00:22:50,060
理解那些概念依然重要。

609
00:22:47,700 --> 00:22:50,060
and understanding those concepts is still important.

610
00:22:50,060 --> 00:22:53,900
之後想要回頭會很困難。

610
00:22:50,060 --> 00:22:53,900
And it'll be hard to, like, backtrack from there.

611
00:22:54,200 --> 00:22:54,840
對我來說，

611
00:22:54,200 --> 00:22:54,840
For me, like,

612
00:22:54,840 --> 00:22:58,500
我的擔憂來自於知識和真理

612
00:22:54,840 --> 00:22:58,500
my fear component comes from a place more of like having.

613
00:22:59,020 --> 00:23:02,580
集中在同一個地方。

613
00:22:59,020 --> 00:23:02,580
Kind of like knowledge and truth centralized in one place.

614
00:23:02,580 --> 00:23:04,600
我覺得這才是我更害怕的事，

614
00:23:02,580 --> 00:23:04,600
I think that's what's more fearful for me

615
00:23:04,600 --> 00:23:07,660
因為我認為現在我們擁有

615
00:23:04,600 --> 00:23:07,660
because I think right now we have

616
00:23:07,660 --> 00:23:10,220
沒有被承認，分散在許多不同的地方。

616
00:23:07,660 --> 00:23:10,220
not acknowledged fragmented in so many different places.

617
00:23:10,220 --> 00:23:13,200
我認為學習的美妙之處在於將每一塊拼圖

617
00:23:10,220 --> 00:23:13,200
And I think the beauty of learning is like taking each of those pieces

618
00:23:13,200 --> 00:23:14,940
拼湊成一個整體，像是

618
00:23:13,200 --> 00:23:14,940
and putting it together in one kind of like,

619
00:23:14,940 --> 00:23:16,400
為自己建立一個流暢的理解。

619
00:23:14,940 --> 00:23:16,400
streamlined understanding for yourself.

620
00:23:16,400 --> 00:23:18,680
但我認為，隨著你知道的，

620
00:23:16,400 --> 00:23:18,680
But I think with, you know,

621
00:23:18,680 --> 00:23:21,840
像是使用 ChatGBT 和某些

621
00:23:18,680 --> 00:23:21,840
kind of the usage of like ChatGBT and like certain,

622
00:23:21,840 --> 00:23:24,780
你知道的，像聊天機器人這類的工具。

622
00:23:21,840 --> 00:23:24,780
like, you know, like chatbots on its own.

623
00:23:24,780 --> 00:23:27,020
還有那種搜尋引擎的功能。

623
00:23:24,780 --> 00:23:27,020
And having that kind of like the search engine.

624
00:23:27,020 --> 00:23:28,600
或者說是資訊的集中化。

624
00:23:27,020 --> 00:23:28,600
Or like the centralization of truth.

625
00:23:28,600 --> 00:23:31,980
我覺得這有時候可能會形成一個惡性循環，

625
00:23:28,600 --> 00:23:31,980
I feel like could be like a bad feedback loop sometimes

626
00:23:31,980 --> 00:23:34,900
因為你可能一直引用相同的來源，

626
00:23:31,980 --> 00:23:34,900
where I think you're referring to the same sources

627
00:23:34,900 --> 00:23:36,940
而沒有真正去努力

627
00:23:34,900 --> 00:23:36,940
and not really doing kind of that work to like,

628
00:23:36,940 --> 00:23:38,700
接觸不同的知識來源。

628
00:23:36,940 --> 00:23:38,700
reach out different, like knowledge sources.

629
00:23:38,700 --> 00:23:40,580
所以我想這就是我擔心的原因，

629
00:23:38,700 --> 00:23:40,580
So I think that's where my fear comes from

630
00:23:40,580 --> 00:23:43,340
而且我覺得當你知道的時候，這種擔憂會被放大。

630
00:23:40,580 --> 00:23:43,340
and I think it becomes amplified when, like, you know

631
00:23:43,340 --> 00:23:45,720
隨著出現一些特定的事物。

631
00:23:43,340 --> 00:23:45,720
with emergence of things where they're like, specific.

632
00:23:45,720 --> 00:23:46,200
比如說。

632
00:23:45,720 --> 00:23:46,200
Like.

633
00:23:46,200 --> 00:23:49,400
像這樣的聊天機器人會有特定主題，可能是，

633
00:23:46,200 --> 00:23:49,400
Chatbots like that are themed so like, maybe,

634
00:23:49,400 --> 00:23:51,280
你知道的，做一個聊天機器人。

634
00:23:49,400 --> 00:23:51,280
like, you know, doing like a chatbot.

635
00:23:51,280 --> 00:23:52,360
只針對，

635
00:23:51,280 --> 00:23:52,360
Only for, you know,

636
00:23:52,360 --> 00:23:55,360
極右派活動家，或者是極少數的活動家。

636
00:23:52,360 --> 00:23:55,360
far far-right activists, or like far less activists.

637
00:23:55,360 --> 00:23:57,020
然後，當你，

637
00:23:55,360 --> 00:23:57,020
And like, kind of, when you, you know

638
00:23:57,020 --> 00:23:58,440
陷入那種反饋循環，

638
00:23:57,020 --> 00:23:58,440
go down those like feedback loops

639
00:23:58,440 --> 00:24:00,820
無法抽身並擁有，

639
00:23:58,440 --> 00:24:00,820
and you're unable to kind of extract yourself and have, like,

640
00:24:00,820 --> 00:24:03,840
更廣闊的視角，因為某種程度上能接觸到，

640
00:24:00,820 --> 00:24:03,840
a broader perspective because of kind of accessibility of,

641
00:24:03,840 --> 00:24:05,240
你知道的，生成式人工智慧。

641
00:24:03,840 --> 00:24:05,240
like, you know, generative AI.

642
00:24:05,240 --> 00:24:07,520
我想這就是我恐懼的來源，某種程度上

642
00:24:05,240 --> 00:24:07,520
I think is like where my fear comes from, so kind of

643
00:24:07,520 --> 00:24:08,460
就像我們的社群媒體一樣，

643
00:24:07,520 --> 00:24:08,460
like where our social media

644
00:24:08,460 --> 00:24:09,720
你的恐懼是回音室效應。

644
00:24:08,460 --> 00:24:09,720
your fear is kind of echo chambers.

645
00:24:09,720 --> 00:24:12,480
人們只會說我只用這些來源或那些來源。

645
00:24:09,720 --> 00:24:12,480
That people are gonna say I'm only use these sources or that sources.

646
00:24:12,480 --> 00:24:16,160
我會說我很樂觀

646
00:24:12,480 --> 00:24:16,160
I would say that I'm optimistic

647
00:24:16,160 --> 00:24:20,200
總體來說很樂觀，因為這變得容易多了。

647
00:24:16,160 --> 00:24:20,200
and overall optimistic because I it's a lot easier.

648
00:24:20,200 --> 00:24:22,160
就像你之前指出的，你說，嘿

648
00:24:20,200 --> 00:24:22,160
Like you pointed out before, as you said, hey

649
00:24:22,160 --> 00:24:23,460
我希望你使用這些資源

649
00:24:22,160 --> 00:24:23,460
I want you to use these sources

650
00:24:23,460 --> 00:24:26,240
我希望你接受不同的意見，我就是這麼想。

650
00:24:23,460 --> 00:24:26,240
and I want you to take different opinions and I want that.

651
00:24:26,240 --> 00:24:28,680
這是不是我們需要思考的事情

651
00:24:26,240 --> 00:24:28,680
And is that something we need to be thinking about

652
00:24:28,680 --> 00:24:31,100
將更多融入教育中，

652
00:24:28,680 --> 00:24:31,100
incorporating more into education and

653
00:24:31,100 --> 00:24:32,260
以及我們如何教導人們批判性思考？

653
00:24:31,100 --> 00:24:32,260
how we teach people critical thinking?

654
00:24:32,260 --> 00:24:34,740
如果你不挑戰自己這個想法，

654
00:24:32,260 --> 00:24:34,740
The idea that if you're not challenging yourself

655
00:24:34,740 --> 00:24:38,340
你肯定無法學習，我百分之百同意這點。

655
00:24:34,740 --> 00:24:38,340
you're not learning for sure, I 100% agree with that

656
00:24:38,340 --> 00:24:40,460
我認為這正是學習模式做得很好的地方，

656
00:24:38,340 --> 00:24:40,460
and I think that that's what the study mode does great

657
00:24:40,460 --> 00:24:42,960
它確實給你帶來挑戰的元素。

657
00:24:40,460 --> 00:24:42,960
like it does give you that challenging aspect.

658
00:24:42,960 --> 00:24:44,940
當你的大腦開始，像是，你知道，

658
00:24:42,960 --> 00:24:44,940
And when your brain starts, like, you know

659
00:24:44,940 --> 00:24:45,900
真正深入探究，

659
00:24:44,940 --> 00:24:45,900
really like diving deep

660
00:24:45,900 --> 00:24:48,140
並開始進行批判性思考，

660
00:24:45,900 --> 00:24:48,140
and you start thinking critically,

661
00:24:48,140 --> 00:24:50,740
思考可能的不同反例，

661
00:24:48,140 --> 00:24:50,740
thinking about maybe different counterexamples,

662
00:24:50,740 --> 00:24:51,700
不同的觀點。

662
00:24:50,740 --> 00:24:51,700
different perspectives.

663
00:24:51,700 --> 00:24:53,960
這樣你就能獲得完整的整體

663
00:24:51,700 --> 00:24:53,960
Then you can get a full overall

664
00:24:53,960 --> 00:24:55,860
評估，等待

664
00:24:53,960 --> 00:24:55,860
just assessment of wait

665
00:24:55,860 --> 00:24:57,100
無論你想思考什麼。

665
00:24:55,860 --> 00:24:57,100
whatever you want to be thinking about.

666
00:24:57,100 --> 00:24:59,000
每次我問這個問題，

666
00:24:57,100 --> 00:24:59,000
Every time I ask this question

667
00:24:59,000 --> 00:25:01,140
我通常會得到不錯的答案，有時是立刻的，

667
00:24:59,000 --> 00:25:01,140
I usually get a good answer, sometimes immediately

668
00:25:01,140 --> 00:25:03,340
有時人們需要更久的時間來思考。

668
00:25:01,140 --> 00:25:03,340
sometimes it takes longer for people to think about that.

669
00:25:03,340 --> 00:25:06,820
我想問你最喜歡的提示語是什麼，

669
00:25:03,340 --> 00:25:06,820
I want to ask about your favorite prompts

670
00:25:06,820 --> 00:25:08,740
那些你用過能帶來有趣結果的東西，

670
00:25:06,820 --> 00:25:08,740
things that you've used that give you fun results

671
00:25:08,740 --> 00:25:10,580
或是很酷的使用方式。

671
00:25:08,740 --> 00:25:10,580
or cool ways to use it.

672
00:25:10,580 --> 00:25:13,380
所以我注意到兩件事。

672
00:25:10,580 --> 00:25:13,380
So I've kind of noticed two things.

673
00:25:13,380 --> 00:25:15,380
我覺得最主要的是我真的可以，

673
00:25:13,380 --> 00:25:15,380
I think the main thing is I can really.

674
00:25:15,380 --> 00:25:18,460
我可以稍微個人化，

674
00:25:15,380 --> 00:25:18,460
I can kind of personalize

675
00:25:18,460 --> 00:25:20,700
並且告訴 ChatGPT 關於我的具體資訊，

675
00:25:18,460 --> 00:25:20,700
and give Chachi BT the specifics about me

676
00:25:20,700 --> 00:25:23,940
然後讓它根據那些給我一個答案。

676
00:25:20,700 --> 00:25:23,940
and and then have it give me an answer based on that.

677
00:25:23,940 --> 00:25:27,480
所以我告訴它我的飲食偏好，還有我

677
00:25:23,940 --> 00:25:27,480
So I tell it my diet preferences, what I

678
00:25:27,480 --> 00:25:29,360
對我吃的食物有什麼期望。

678
00:25:27,480 --> 00:25:29,360
what I want out of the food that I'm eating.

679
00:25:29,360 --> 00:25:30,820
當我去超市時，

679
00:25:29,360 --> 00:25:30,820
And when I go to the grocery store

680
00:25:30,820 --> 00:25:33,760
我會拍下收據，讓它幫我評分。

680
00:25:30,820 --> 00:25:33,760
I'll take a picture of the receipt and have it grade it.

681
00:25:33,760 --> 00:25:35,220
你知道，我做得如何？

681
00:25:33,760 --> 00:25:35,220
You know, how well did I do?

682
00:25:35,900 --> 00:25:37,500
根據這次購物行程，

682
00:25:35,900 --> 00:25:37,500
Based on this grocery trip

683
00:25:37,500 --> 00:25:39,080
因為我想確保

683
00:25:37,500 --> 00:25:39,080
because I want to make sure

684
00:25:39,080 --> 00:25:42,200
我有攝取到所有的維生素。

684
00:25:39,080 --> 00:25:42,200
that I highlight and I get all my vitamins.

685
00:25:42,200 --> 00:25:44,120
假設這是我特定的目標，

685
00:25:42,200 --> 00:25:44,120
And let's say that's my specific goal

686
00:25:44,120 --> 00:25:46,340
它會根據這點客觀地給我評分。

686
00:25:44,120 --> 00:25:46,340
and it'll grade me based on that objectively.

687
00:25:46,340 --> 00:25:48,500
而且它還會追蹤我的進展。

687
00:25:46,340 --> 00:25:48,500
And it also track progress over time.

688
00:25:48,500 --> 00:25:51,180
但我認為，提示的重點是

688
00:25:48,500 --> 00:25:51,180
But I think the the main thing with with prompts is

689
00:25:51,180 --> 00:25:53,280
你可以談論你自己，

689
00:25:51,180 --> 00:25:53,280
that you can talk about you yourself

690
00:25:53,280 --> 00:25:54,960
而且它會把這些存進記憶中。

690
00:25:53,280 --> 00:25:54,960
and it also store that in memory.

691
00:25:54,960 --> 00:25:58,100
我打籃球已經很久了，

691
00:25:54,960 --> 00:25:58,100
So I, I've been playing basketball for a long time,

692
00:25:58,100 --> 00:25:59,020
幾乎是我一生中大部分時間。

692
00:25:58,100 --> 00:25:59,020
almost all my life.

693
00:25:59,020 --> 00:26:02,140
我遇到了一些膝蓋的問題。

693
00:25:59,020 --> 00:26:02,140
And I have like some knee issues that I've run into.

694
00:26:02,140 --> 00:26:06,700
有時候傳統的、所謂最好的腿部鍛鍊方式，

694
00:26:02,140 --> 00:26:06,700
And sometimes the traditional, like, best way to, like

695
00:26:06,700 --> 00:26:08,300
對我來說並不適合。

695
00:26:06,700 --> 00:26:08,300
work out your legs isn't the best.

696
00:26:08,300 --> 00:26:09,040
當你膝蓋疼痛時，

696
00:26:08,300 --> 00:26:09,040
When you have knee pain

697
00:26:09,040 --> 00:26:13,640
每個人的疼痛類型和誘發方式都不同，

697
00:26:09,040 --> 00:26:13,640
and everyone has different types of pain and different ways

698
00:26:13,640 --> 00:26:14,620
會有不同的加劇原因。

698
00:26:13,640 --> 00:26:14,620
that that gets aggravated.

699
00:26:14,620 --> 00:26:16,600
所以當我問Chachi BT時，

699
00:26:14,620 --> 00:26:16,600
So when I ask Chachi BT, you know

700
00:26:16,600 --> 00:26:17,320
我有這個疼痛，

700
00:26:16,600 --> 00:26:17,320
I have this pain

701
00:26:17,320 --> 00:26:19,320
但做這個動作時感覺沒問題，

701
00:26:17,320 --> 00:26:19,320
but when I do this exercise, it feels fine

702
00:26:19,320 --> 00:26:21,140
做這個動作時卻很痛。

702
00:26:19,320 --> 00:26:21,140
but when I do this exercise, it really hurts.

703
00:26:21,140 --> 00:26:22,320
你能幫我嗎？

703
00:26:21,140 --> 00:26:22,320
So can you make me?

704
00:26:22,320 --> 00:26:24,460
你能幫我診斷問題嗎？

704
00:26:22,320 --> 00:26:24,460
Can you maybe diagnose the problem?

705
00:26:24,460 --> 00:26:25,660
也許它不太擅長這方面。

705
00:26:24,460 --> 00:26:25,660
Maybe it's not that good at that

706
00:26:25,660 --> 00:26:27,680
但你能幫我做嗎？

706
00:26:25,660 --> 00:26:27,680
but can you make me?

707
00:26:27,680 --> 00:26:32,460
只是一個圍繞我特定需求設計的運動計劃嗎？

707
00:26:27,680 --> 00:26:32,460
Just a exercise regimen that'll be wrapped around my specific needs.?

708
00:26:32,780 --> 00:26:33,680
那真的很酷

708
00:26:32,780 --> 00:26:33,680
That's really cool

709
00:26:34,380 --> 00:26:35,820
讓我有了一些想法。

709
00:26:34,380 --> 00:26:35,820
giving me some ideas.

710
00:26:36,880 --> 00:26:38,260
我覺得對我來說。

710
00:26:36,880 --> 00:26:38,260
I think for me.

711
00:26:38,260 --> 00:26:40,740
所以我也能想到兩個使用情境。

711
00:26:38,260 --> 00:26:40,740
So I also can think of like two use cases.

712
00:26:40,740 --> 00:26:42,580
但我個人認為

712
00:26:40,740 --> 00:26:42,580
But I think personally for me

713
00:26:42,580 --> 00:26:44,200
最棒的事情是，

713
00:26:42,580 --> 00:26:44,200
the best thing that you know.

714
00:26:44,200 --> 00:26:47,280
ChatGPT 和 AI 給我的，是時間，還有

714
00:26:44,200 --> 00:26:47,280
Chachi BT and like AI gives me is like, time, and like

715
00:26:47,280 --> 00:26:48,840
我覺得有更多時間會讓我快樂。

715
00:26:47,280 --> 00:26:48,840
I think having more time makes me happy.

716
00:26:48,840 --> 00:26:51,860
所以我覺得 ChatGPT 確實讓我這個人更快樂。

716
00:26:48,840 --> 00:26:51,860
So like, I feel like Chachi BT does make me happy as a person.

717
00:26:51,860 --> 00:26:55,160
但我認為這種快樂最好的展現是，

717
00:26:51,860 --> 00:26:55,160
But I think the best kind of like manifestation of that is like,

718
00:26:55,160 --> 00:26:56,740
像是日常生活中一些小事。

718
00:26:55,160 --> 00:26:56,740
kind of like small, day-to-day tasks.

719
00:26:56,740 --> 00:27:00,640
我記得有一次我用它是因為

719
00:26:56,740 --> 00:27:00,640
And I think one of the times I used it was so

720
00:27:00,640 --> 00:27:02,200
我想我有一個閱讀清單，

720
00:27:00,640 --> 00:27:02,200
I think I had like a reading list of

721
00:27:02,200 --> 00:27:03,980
我想去買大約15本書。

721
00:27:02,200 --> 00:27:03,980
like 15 books that I wanted to go get.

722
00:27:03,980 --> 00:27:08,920
我校園附近最近的書店是最後一本書店，

722
00:27:03,980 --> 00:27:08,920
And so the nearest bookstore from my campus is the last bookstore

723
00:27:08,920 --> 00:27:09,800
它位於洛杉磯市中心。

723
00:27:08,920 --> 00:27:09,800
which is in DTLA.

724
00:27:09,800 --> 00:27:13,240
對於不在洛杉磯或不熟悉這區域的人來說，

724
00:27:09,800 --> 00:27:13,240
And for those of you who aren't from LA or know the area

725
00:27:13,240 --> 00:27:14,740
停車非常困難。

725
00:27:13,240 --> 00:27:14,740
it's really tough on parking.

726
00:27:14,740 --> 00:27:17,560
這裡的停車情況很緊湊，進出都很快。

726
00:27:14,740 --> 00:27:17,560
It's kind of like a very in and out situation.

727
00:27:17,560 --> 00:27:20,160
所以我感受到時間上的壓力。

727
00:27:17,560 --> 00:27:20,160
So I think there's like a time pressure for me.

728
00:27:20,160 --> 00:27:22,880
我也不想付四十美元的停車費。

728
00:27:20,160 --> 00:27:22,880
And I also didn't want to pay like 40 bucks.

729
00:27:22,880 --> 00:27:23,840
到了某個時候，

729
00:27:22,880 --> 00:27:23,840
After a certain point.

730
00:27:23,840 --> 00:27:27,520
我想要制定一個最佳策略，讓我能夠進入，

730
00:27:23,840 --> 00:27:27,520
I wanted to put together, you know, the optimal strategy to get in

731
00:27:27,520 --> 00:27:29,280
並在15分鐘內離開書店。

731
00:27:27,520 --> 00:27:29,280
and out of the bookstore in like 15 minutes.

732
00:27:29,280 --> 00:27:32,400
所以我先把資料輸入，

732
00:27:29,280 --> 00:27:32,400
And so that started off with kind of feeding it.

733
00:27:32,400 --> 00:27:34,280
像是我心中想買的那15本書，

733
00:27:32,400 --> 00:27:34,280
Like the 15 books that I had in mind

734
00:27:34,280 --> 00:27:36,280
並根據，

734
00:27:34,280 --> 00:27:36,280
and organize it based off of,

735
00:27:36,280 --> 00:27:38,160
圖書館的分類系統來整理，

735
00:27:36,280 --> 00:27:38,160
like, the library's organization system,

736
00:27:38,160 --> 00:27:40,800
然後是該類型作者的姓氏。

736
00:27:38,160 --> 00:27:40,800
and then the last name of the authors in the genre.

737
00:27:40,800 --> 00:27:42,400
所以，大概就是這三個輸入。

737
00:27:40,800 --> 00:27:42,400
So, like, kind of, like those three inputs.

738
00:27:42,400 --> 00:27:45,480
它給了我最優化的策略，讓我能夠

738
00:27:42,400 --> 00:27:45,480
And so it gave me the most optimal strat to kind of like,

739
00:27:45,480 --> 00:27:47,680
盡快拿到那些書然後離開。

739
00:27:45,480 --> 00:27:47,680
grab those books and get out as fast as possible.

740
00:27:47,680 --> 00:27:49,860
我覺得這是一個很酷的應用案例。

740
00:27:47,680 --> 00:27:49,860
So I thought that was a really cool use case.

741
00:27:49,860 --> 00:27:51,080
而且我一直在使用，

741
00:27:49,860 --> 00:27:51,080
And I've been using, you know,

742
00:27:51,080 --> 00:27:53,580
很多類似的應用來優化我的時間。

742
00:27:51,080 --> 00:27:53,580
a lot of kind of use cases like that to optimize my time.

743
00:27:53,580 --> 00:27:56,600
然後還有另一個應用案例，

743
00:27:53,580 --> 00:27:56,600
And then just another kind of like use case.

744
00:27:56,600 --> 00:27:59,640
其實是我朋友教我的，

744
00:27:56,600 --> 00:27:59,640
That actually my friend taught me is to kind of

745
00:27:59,640 --> 00:28:03,260
利用語音模式來幫助節省時間。

745
00:27:59,640 --> 00:28:03,260
use voice mode to really also help with that time convenience.

746
00:28:03,260 --> 00:28:07,180
我看到他開車時使用它，

746
00:28:03,260 --> 00:28:07,180
And so I saw him use it while he was driving

747
00:28:07,180 --> 00:28:08,840
他開啟了語音模式。

747
00:28:07,180 --> 00:28:08,840
and he put it on voice mode.

748
00:28:08,840 --> 00:28:11,660
每次他看到舊金山的科技廣告牌，

748
00:28:08,840 --> 00:28:11,660
And every time he would see, like a tech billboard in San Francisco

749
00:28:11,660 --> 00:28:13,020
他就會問，喔，

749
00:28:11,660 --> 00:28:13,020
he would ask it, Oh

750
00:28:13,020 --> 00:28:15,300
這是什麼公司？這個廣告牌是什麼意思？

750
00:28:13,020 --> 00:28:15,300
what is this company? like? what does this billboard mean?

751
00:28:15,300 --> 00:28:17,200
我覺得他能夠這樣做很不錯，

751
00:28:15,300 --> 00:28:17,200
So I think it was nice how he's able

752
00:28:17,200 --> 00:28:19,040
邊做邊學，然後就像是

752
00:28:17,200 --> 00:28:19,040
to kind of learn on the go and just like

753
00:28:19,040 --> 00:28:21,780
做一些他感興趣的事情。

753
00:28:19,040 --> 00:28:21,780
do some things that he's like interested in with.

754
00:28:21,780 --> 00:28:23,580
像是開車這種平凡的任務。

754
00:28:21,780 --> 00:28:23,580
Kind of like mundane tasks like driving.

755
00:28:23,580 --> 00:28:26,340
所以我也開始嘗試這樣做。

755
00:28:23,580 --> 00:28:26,340
And then so I started kind of adopting that.

756
00:28:26,340 --> 00:28:28,380
我現在正在上暑期課程，

756
00:28:26,340 --> 00:28:28,380
I'm taking like a summer course right now

757
00:28:28,380 --> 00:28:30,740
所以沒有太多時間，

757
00:28:28,380 --> 00:28:30,740
and so I don't have a lot of time with,

758
00:28:30,740 --> 00:28:32,520
你知道的，工作、通勤和開車，

758
00:28:30,740 --> 00:28:32,520
you know, the work, commute while driving

759
00:28:32,520 --> 00:28:35,200
而且大部分時間都在工作。

759
00:28:32,520 --> 00:28:35,200
and then there's actual work for most of my day.

760
00:28:35,200 --> 00:28:36,900
所以為了真正有效利用時間，

760
00:28:35,200 --> 00:28:36,900
So to really optimize my time

761
00:28:36,900 --> 00:28:39,700
我一直用語音模式來進行，

761
00:28:36,900 --> 00:28:39,700
I've been just asking, using voice mode to kind of do like,

762
00:28:39,700 --> 00:28:42,940
穩定地反覆回顧某些概念。

762
00:28:39,700 --> 00:28:42,940
steady, like, recall and back and forth on certain concepts.

763
00:28:42,940 --> 00:28:44,420
這樣當我回到家時，

763
00:28:42,940 --> 00:28:44,420
So that when I get home

764
00:28:44,420 --> 00:28:47,200
我覺得我的大腦已經準備好並且活躍，

764
00:28:44,420 --> 00:28:47,200
I feel like my brain is already ready and active to like,

765
00:28:47,200 --> 00:28:47,860
可以開始做作業。

765
00:28:47,200 --> 00:28:47,860
do the assignments.

766
00:28:47,860 --> 00:28:50,440
我覺得這一點真的很酷，

766
00:28:47,860 --> 00:28:50,440
So I think like that aspect has been really cool,

767
00:28:50,440 --> 00:28:52,820
能夠使用語音功能也非常棒。

767
00:28:50,440 --> 00:28:52,820
and being able to use that voice feature is also really cool.

768
00:28:52,820 --> 00:28:55,440
因為它驚人地像真人一樣。

768
00:28:52,820 --> 00:28:55,440
Because it's scarily like, human like.

769
00:28:55,440 --> 00:28:57,040
所以我感覺自己真的在跟人對話。

769
00:28:55,440 --> 00:28:57,040
So I feel like I'm actually talking to someone.

770
00:28:57,040 --> 00:28:58,380
大家真的很棒。

770
00:28:57,040 --> 00:28:58,380
Guys has been great.

771
00:28:58,380 --> 00:29:00,420
感謝你們兩位的到來，

771
00:28:58,380 --> 00:29:00,420
I thank you both for being here

772
00:29:00,420 --> 00:29:03,280
期待未來能再度交談，

772
00:29:00,420 --> 00:29:03,280
and look forward to talking again further down

773
00:29:03,280 --> 00:29:05,020
看看你們各自的路走向何方。

773
00:29:03,280 --> 00:29:05,020
and seeing where your paths have taken you.

774
00:29:05,020 --> 00:29:07,500
不，真的非常感謝你邀請我們。

774
00:29:05,020 --> 00:29:07,500
Yeah, no, thank you so much for having us.

775
00:29:07,500 --> 00:29:10,360
我覺得這是一個很棒的平台，可以談論，

775
00:29:07,500 --> 00:29:10,360
I feel like it's a great platform to kind of like talk about,

776
00:29:10,360 --> 00:29:12,120
像是人工智慧，還有

776
00:29:10,360 --> 00:29:12,120
like AI and like

777
00:29:12,120 --> 00:29:13,560
特別是在教育領域，

777
00:29:12,120 --> 00:29:13,560
especially in the education space

778
00:29:13,560 --> 00:29:14,900
因為這是非常重要的議題。

778
00:29:13,560 --> 00:29:14,900
because something's so salient.

779
00:29:14,900 --> 00:29:15,920
所以我很高興，像是，

779
00:29:14,900 --> 00:29:15,920
So I'm glad, like, you know

780
00:29:15,920 --> 00:29:17,960
我們有這個平台讓我們聚在一起。

780
00:29:15,920 --> 00:29:17,960
we had this platform for having us.

781
00:29:17,960 --> 00:29:19,360
是的，可以用像這樣的詞語。

781
00:29:17,960 --> 00:29:19,360
Yeah, can use words like.

782
00:29:21,220 --> 00:29:23,980
突出，我贊同她的說法，不，這是。

782
00:29:21,220 --> 00:29:23,980
Salient, I echo what she said, No, this is.

783
00:29:23,980 --> 00:29:26,060
這真是一次很棒的對話。

783
00:29:23,980 --> 00:29:26,060
This has been great, great conversation.

784
00:29:26,060 --> 00:29:27,300
我很高興我們能談論

784
00:29:26,060 --> 00:29:27,300
I'm glad we got to talk about

785
00:29:27,300 --> 00:29:30,180
一些尖銳的議題，也有一些樂觀的話題。

785
00:29:27,300 --> 00:29:30,180
some hard-hitting topics and also some optimistic topics.

786
00:29:30,180 --> 00:29:31,380
是的，我非常樂觀。

786
00:29:30,180 --> 00:29:31,380
Yeah, I'm very optimistic.

787
00:29:31,380 --> 00:29:32,440
我的意思是，這將會是。

787
00:29:31,380 --> 00:29:32,440
I mean, it's gonna be.

788
00:29:32,440 --> 00:29:35,680
我們越能幫助朋友適應

788
00:29:32,440 --> 00:29:35,680
The more we can help our friends adapt

789
00:29:35,680 --> 00:29:37,700
並找到自己的定位，就越好。

789
00:29:35,680 --> 00:29:37,700
and find their own place with it, the better.
